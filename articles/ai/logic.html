<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/c9a5bc6a7c948fb0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/8de0e5cdd54a87d1.css" crossorigin="" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/1cb3813e458884a4.css" crossorigin="" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-75d50c1cec9253a4.js" crossorigin=""/><script src="/_next/static/chunks/fd9d1056-672a444f700a8e5a.js" async="" crossorigin=""></script><script src="/_next/static/chunks/472-7e5eafffd19dca44.js" async="" crossorigin=""></script><script src="/_next/static/chunks/main-app-d355f2bc91b5bf25.js" async="" crossorigin=""></script><script src="/_next/static/chunks/326-dcee1ff54fa4f70c.js" async=""></script><script src="/_next/static/chunks/app/layout-20881daac4d91a26.js" async=""></script><script src="/_next/static/chunks/d3ac728e-1e5d8b71e3d43fec.js" async=""></script><script src="/_next/static/chunks/app/articles/%5B...slug%5D/page-44f6caefb92fad34.js" async=""></script><title>Article</title><meta name="description" content="Personal site"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="256x256"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js" crossorigin="" noModule=""></script></head><body class="__className_aaf875 Layout_body__oXsmr"><header class="Layout_header__XC_Gv"><div class="NavBar_container__pKdC7"><a class="link" href="/">Home</a><a class="link" href="/academic">Academic Work</a><a class="link" href="/categories">Categories</a><a class="link" href="/articles">All</a><a class="link" href="/articles/about">About</a></div></header><main class="Layout_main__luTTh"><div class="article-page_container__5yaZl"><h1>Logic</h1><div class="Taxonomy_container__C2Vdb"><div class="Taxonomy_values__yvK16"><a href="/tags/Logic"><div class="Chip_container__q1AW2">Logic</div></a></div></div><div class="ArticleContent_md__SxPPI"><!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>
  <body>
    <h1 id="logic">Logic</h1>
    <p><strong>Logic</strong> is the field of knowledge representation and reasoning about this knowledge according to some <strong>Possible World</strong> also known as a <strong>Model</strong>. There are different types of logic formulizations such as <strong>Propositional Logic</strong> and <strong>First Order Logic</strong>, each of which can represent different information.</p>
    <p>Formally, each logic defines its <strong>Syntax</strong>, which is the specification of how we express <strong>Sentences</strong>. In addition to syntax, the logic also defines its <strong>Semantics</strong> which specify the meaning of those sentences - it assigns a <strong>Truth Value</strong> to each sentence according to the model.</p>
    <p>A set of sentences is known as a <strong>Knowledge Base</strong>. The set of sentences that are given to us and assumed true are known as <strong>Axioms</strong>.</p>
    <p>If a given sentence $\alpha$ is true in some model $m$ we say that <strong>$\alpha$ is satisfied by the model $m$</strong>. We will denote $M(\alpha)$ to be the set of all models that satisfy $\alpha$.</p>
    <p><strong>Reasoning</strong> is the process of understanding the entailment between sentences. Given the sentences $\alpha$ and $\beta$, we say that $\alpha$ entails the sentence $\beta$ if and only if $M(\alpha) \subseteq M(\beta)$, meaning that every model $m$ that satisfies $\alpha$ also satisfies $\beta$. We denote entailment by $\alpha \models \beta$.</p>
    <h2 id="logical-inference">Logical Inference</h2>
    <p>Given a knowledge base $KB$, we want to derive new sentences - such a process is called <strong>Logical Inference</strong>. The algorithm we choose to derive new sentences can be arbitrary, but what's the point? We say that an inference algorithm is <strong>Sound</strong> if and only if it derives entailed sentences.</p>
    <p>For example, consider the following axioms:</p>
    <ul>
      <li>Some dogs are black</li>
      <li>Every dog is an animal</li>
    </ul>
    <p>Using some inference algorithm, we can infer the sentence: "Every animal is black" - but we know this is wrong, our algorithm is unsound.</p>
    <p>Another algorithm can infer the sentence: "Some animals are black" which is sound.</p>
    <p>Lastly, consider an inference algorithm that did not produce any sentence. If we desire to logically infer new sentences from the KB, such an algorithm is undesirable. This leads us to the second categorization of inference algorithms:</p>
    <p>Inference algorithms that can derive any sentence that is entailed are called <strong>Complete</strong>.</p>
    <p>Ideal inference algorithms are both complete and sound. Such algorithms can derive all entailed sentences, and all derived sentences are entailed.</p>
    <h1 id="propositional-logic">Propositional Logic</h1>
    <p>The first logic we will explore is the <strong>Propositional Logic</strong>.</p>
    <p>In this logic's syntax, each sentence is either an <strong>Atomic Sentence</strong> or a <strong>Complex Sentence</strong>.</p>
    <p>An atomic sentence consists of a <strong>Proposition</strong> - a symbol that can take a boolean value.</p>
    <p>A complex sentence is constructed of multiple sentences using <strong>Parentheses</strong> and <strong>Logical Operators</strong>.</p>
    <p>Parentheses, as commonly used, specify the precedence of operations.</p>
    <p>Logical operators operate on sentences and return a boolean value. The logical operators are:</p>
    <ul>
      <li>The <strong>negation (not)</strong> unary operator $\neg$</li>
      <li>The <strong>conjunction (and)</strong> binary operator $\land$</li>
      <li>The <strong>disjunction (or)</strong> binary operator $\lor$</li>
      <li>The <strong>implication (implies)</strong> binary operator $\Rightarrow$</li>
      <li>The <strong>biconditional (if and only if)</strong> binary operator $\Leftrightarrow$</li>
    </ul>
    <p>They are (unsuprisingly) defined by the truth table:</p>
    <div class="table-container">
      <table>
        <thead>
          <tr>
            <th>$P$</th>
            <th>$Q$</th>
            <th>$\neg P$</th>
            <th>$P \land Q$</th>
            <th>$P \lor Q$</th>
            <th>$P \Rightarrow Q$</th>
            <th>$P \Leftrightarrow A$</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>F</td>
            <td>F</td>
            <td>T</td>
            <td>F</td>
            <td>F</td>
            <td>T</td>
            <td>T</td>
          </tr>
          <tr>
            <td>F</td>
            <td>T</td>
            <td>T</td>
            <td>F</td>
            <td>T</td>
            <td>T</td>
            <td>F</td>
          </tr>
          <tr>
            <td>T</td>
            <td>F</td>
            <td>F</td>
            <td>F</td>
            <td>T</td>
            <td>F</td>
            <td>F</td>
          </tr>
          <tr>
            <td>T</td>
            <td>T</td>
            <td>F</td>
            <td>T</td>
            <td>T</td>
            <td>T</td>
            <td>T</td>
          </tr>
        </tbody>
      </table>
    </div>
    <p>The following <strong>Logical Equivalences</strong> hold:</p>
    <blockquote>
      <p>Regardig the notation below, note that we use $\cdot$ and $+$ as placeholders for when equivalence holds both for $\land$ and $\lor$. i.e. read $\alpha \cdot \beta$ as $\alpha \land \beta$ and also $\alpha \lor \beta$. When we use both $\cdot$ and $+$, it means we can use both interchangebly.</p>
    </blockquote>
    <ul>
      <li><strong>Commutativity</strong>: $\alpha \cdot \beta \equiv \beta \cdot \alpha$</li>
      <li><strong>Associativity</strong> meaning that $((\alpha \cdot \beta) \cdot \gamma) \equiv (\alpha \cdot (\beta \cdot \gamma))$</li>
      <li><strong>Double Negation</strong>: $\neg (\neg \alpha) \equiv \alpha$</li>
      <li><strong>Contraposition</strong>: $\alpha \Rightarrow \beta \equiv \neg \beta \Rightarrow \neg \alpha$
        <ul>
          <li>This equivalence is frequently used in mathematical proofs and is the basis of "proof by contradiction"</li>
        </ul>
      </li>
      <li><strong>Implication Elimination</strong>: $\alpha \Rightarrow \beta \equiv \neg \alpha \lor \beta$</li>
      <li><strong>Bidirectional Elimination</strong>: $\alpha \Leftrightarrow \beta \equiv (\alpha \Rightarrow \beta) \land (\beta \Rightarrow \alpha))$</li>
      <li><strong>Demorgan</strong>: $\neg (\alpha \cdot \beta) \equiv \neg \alpha \cdot \neg \beta$</li>
      <li><strong>Distributivity</strong> $(\alpha \cdot (\beta + \gamma)) \equiv (\alpha \cdot \beta) + (\alpha \cdot \gamma)$</li>
    </ul>
    <h2 id="inference">Inference</h2>
    <p>A commonly, sound rule that is used to derive sentences is known as <strong>Modus Ponens</strong> rule:</p>
    <p>
      $$
      \frac{\alpha \Rightarrow \beta, \alpha}{\beta}
      $$
    </p>
    <p>Read it as: "If we know that $\alpha$ implies $\beta$ and we also know that $\alpha$, we derive that $\beta$".</p>
    <p>Another sound rule is quite intuitive and is known as <strong>And Elimination</strong>:</p>
    <p>
      $$
      \frac{\alpha \land \beta}{\alpha}
      $$
    </p>
    <p>As additional sound inference rules, we can use all of the known equivalences.</p>
    <p>The last sound rule we will show is the <strong>Resolution Rule</strong>. This is an important rule that is used by a complete and sound inference algorithm known as the <strong>Resultion Inference Algorithm</strong> which we will see later.</p>
    <p>The resolution rule states the following:</p>
    <p>
      $$
      \frac
      {l_1 \lor l_2 \lor ... \lor l_i \lor ... \lor l_n ~,~ A \lor \neg l_i}
      {l_1 \lor l_2 \lor ... \lor l_{i-1} \lor l_{i+1} \lor ... \lor l_n \lor A}
      $$
    </p>
    <p>Where $A$ is a disjunction of other literals.</p>
    <p>For example:</p>
    <p>
      $$
      \frac{P \lor Q ~,~ \neg P \lor R}{Q \lor R}
      $$
    </p>
    <p>Notice that this important rule operates only on disjunctions. However, our sentences can be in an arbitrary form. We find that every KB is equivalent to another KB that is in a very specific form known as "conjunctive normal form" - In this form, we can easily apply the resolution rule.</p>
    <h2 id="conjunctive-normal-form-cnf">Conjunctive Normal Form (CNF)</h2>
    <p>A <strong>Clause</strong> is a disjunction of literals. We say that a sentence that is expressed by a conjunction of clauses is in a <strong>Conjunctive Normal Form</strong>.</p>
    <p>We already know how to transform a KB to a CNF - using the known equivalences!</p>
    <ol>
      <li>Eliminate bidirectional sentences</li>
      <li>Eliminate implications sentences</li>
      <li>Move negations inwards to literals mostly using double negation and de-morgan rules</li>
      <li>Apply distribution rules</li>
    </ol>
    <p>Let's see an example and convert the following sentence to CNF:</p>
    <p>
      $$
      P \Leftrightarrow Q \lor R
      $$
    </p>
    <ol>
      <li>Bidirectional elimination: $(P \Rightarrow Q \lor R) \land (Q \lor R \Rightarrow P)$</li>
      <li>Implication elimination of the left hand conjugate: $(\neg P \lor Q \lor R) \land (Q \lor R \Rightarrow P)$</li>
      <li>Implication elimination of the right hand conjugate: $(\neg P \lor Q \lor R) \land (\neg (Q \lor R) \lor P)$</li>
      <li>Move negation inwards (apply de-morgan at the right-hand side): $(\neg P \lor Q \lor R) \land ((\neg Q \land \neg R) \lor P)$</li>
      <li>Distribute $\lor$ over $\land$ at the right-hand side: $(\neg P \lor Q \lor R) \land (\neg Q \lor P) \land (\neg R \lor P)$</li>
    </ol>
    <p>And that's it, we have a sentence in CNF form.</p>
    <h2 id="the-resolution-algorithm">The Resolution Algorithm</h2>
    <p>The <strong>Resolution Algorithm</strong> is sound and complete (will not show this here).</p>
    <p>Given a KB $K$, we show that $K \models \alpha$ by proving that $K \land \neg \alpha$ is a contradiction (proof by contradiction).</p>
    <ol>
      <li>We add $\neg \alpha$ to the KB</li>
      <li>Convert the KB to CNF form</li>
      <li>As long as we can apply the resolution rule, apply it
        <ul>
          <li>If the derived sentence is non-empty, add it to the KB if it is doesn't exist there already</li>
          <li>Else, because this sentence is a contradiction (empty) we have reached a contradiction in the KB because of $\neg \alpha$ - Answer: the KB entails $\alpha$</li>
        </ul>
      </li>
      <li>No contradiction occurred - Answer: The KB does not entail $\alpha$</li>
    </ol>
    <h3 id="example">Example</h3>
    <p>Assume the initial KB</p>
    <ul>
      <li>$P \Leftrightarrow Q \lor R$</li>
      <li>$\neg P$</li>
    </ul>
    <p>And let's say we want to prove that $\neg Q$.</p>
    <p><strong>Add the negation of $\neg Q$ to the KB</strong></p>
    <ul>
      <li>$P \Leftrightarrow Q \lor R$</li>
      <li>$\neg P$</li>
      <li>$Q$</li>
    </ul>
    <p><strong>Convert to CNF</strong></p>
    <ol>
      <li>$\neg P \lor Q \lor R$</li>
      <li>$\neg Q \lor P$</li>
      <li>$\neg R \lor P$</li>
      <li>$\neg P$</li>
      <li>$Q$</li>
    </ol>
    <p><strong>Repeatedly apply the Resolution Rule</strong></p>
    <ol start="6">
      <li>(5, 2): $P$</li>
      <li>(6, 4): $\phi$</li>
    </ol>
    <p>Because we reached a contradiction, we deduce that the KB entails $\neg Q$.</p>
    <h1 id="first-order-logic">First Order Logic</h1>
    <p>Think of it, how complex are the sentences that we could express via Propositional Logic? It turns out that we can formalize a logic language that is far more expressive than it.</p>
    <p><strong>First Order Logic (FOL)</strong> is a logic formulation that includes objects and their relations to one another.</p>
    <p>FOL is a bit closer to natural language. In natural language, we can speak about objects such as a "Chair" or a "Pen". We can also talk about the relations between objects for example: "Pen is on Chair". Lastly, we can map objects to other objects as in "Leg of the Chair", kind of like functions.</p>
    <p>FOL builds on top of those concepts. Its components are:</p>
    <ul>
      <li><strong>Truth Values (Booleans)</strong> False and True</li>
      <li><strong>Logical Operators</strong> are the same logical operators we saw in propositional logic including the <strong>Equal</strong> operator $=$</li>
      <li><strong>Terms (or Objects)</strong> are the constants</li>
      <li><strong>Variables</strong> are placeholders for Terms</li>
      <li><strong>Quantifiers</strong>
        <ul>
          <li><strong>Universal quantifier</strong> (Forall) $\forall x B$ whether the boolean expression $B$ holds for every possible instantiation of the variable $x$</li>
          <li><strong>Existential quantifier</strong> (Exists) $\exists x B$ whether the boolean expression $B$ holds for some possible instantiation of the variable $x$</li>
        </ul>
      </li>
      <li><strong>Relations (or Predicates)</strong> are mappings from tuples of Terms to a Boolean</li>
      <li><strong>Functions</strong> are mappings from Term to Term</li>
    </ul>
    <p>For clarity, we defined the booleans separately from the predicates. To be more accurate, $True$ and $False$ are also defined as predicates (that always return the same value).</p>
    <p>A sentence in FOL is either an <strong>Atomic Sentence</strong> or a <strong>Complex Sentence</strong>.</p>
    <p>An atomic sentence is either a Term or a Predicate.</p>
    <p>A complex sentence is either an application of multiple sentences using logical operators or a quantified variable followed by a sentence.</p>
    <h2 id="logical-inference-1">Logical Inference</h2>
    <p>We won't go over the details about what logical inference is since we already covered it in the section about propositional logic. We will focus on the techniques that are related to FOL.</p>
    <h1 id="examples-from-exercises">Examples from Exercises</h1>
    <h2 id="propositional-logic-1">Propositional Logic</h2>
    <h2 id="first-order-logic-1">First Order Logic</h2>
    <h1 id="hebrew-appendix">Hebrew Appendix</h1>
    <ul>
      <li>Entailement - נביעה לוגית</li>
      <li>Sound - נאות</li>
      <li>Complete - שלם</li>
      <li>Proposition - פסוק</li>
    </ul>
  </body>
</html>
</div></div></main><footer><div class="Footer_container__Z8cUU"><a href="https://il.linkedin.com/in/tal-glanzman"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="25" width="25" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a><a href="https://github.com/tglanz"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="25" width="25" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div></footer><script src="/_next/static/chunks/webpack-75d50c1cec9253a4.js" crossorigin="" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/c9a5bc6a7c948fb0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/8de0e5cdd54a87d1.css\",\"style\",{\"crossOrigin\":\"\"}]\n0:\"$L3\"\n"])</script><script>self.__next_f.push([1,"4:HL[\"/_next/static/css/1cb3813e458884a4.css\",\"style\",{\"crossOrigin\":\"\"}]\n"])</script><script>self.__next_f.push([1,"5:I[3728,[],\"\"]\n7:I[9928,[],\"\"]\n8:I[5420,[\"326\",\"static/chunks/326-dcee1ff54fa4f70c.js\",\"185\",\"static/chunks/app/layout-20881daac4d91a26.js\"],\"\"]\n9:I[6954,[],\"\"]\na:I[7264,[],\"\"]\nd:I[8326,[\"954\",\"static/chunks/d3ac728e-1e5d8b71e3d43fec.js\",\"326\",\"static/chunks/326-dcee1ff54fa4f70c.js\",\"496\",\"static/chunks/app/articles/%5B...slug%5D/page-44f6caefb92fad34.js\"],\"\"]\ne:T518,"])</script><script>self.__next_f.push([1,"M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"])</script><script>self.__next_f.push([1,"3:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/8de0e5cdd54a87d1.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],[\"$\",\"$L5\",null,{\"buildId\":\"PwLf35PnxKSKCh4Uv7Bnr\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/articles/ai/logic\",\"initialTree\":[\"\",{\"children\":[\"articles\",{\"children\":[[\"slug\",\"ai/logic\",\"c\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":[\\\"ai\\\",\\\"logic\\\"]}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialHead\":[false,\"$L6\"],\"globalErrorComponent\":\"$7\",\"children\":[null,[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_aaf875 Layout_body__oXsmr\",\"children\":[[\"$\",\"header\",null,{\"className\":\"Layout_header__XC_Gv\",\"children\":[\"$\",\"$L8\",null,{}]}],[\"$\",\"main\",null,{\"className\":\"Layout_main__luTTh\",\"children\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"initialChildNode\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"articles\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"initialChildNode\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"articles\",\"children\",[\"slug\",\"ai/logic\",\"c\"],\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"initialChildNode\":[\"$Lb\",\"$Lc\",null],\"childPropSegment\":\"__PAGE__?{\\\"slug\\\":[\\\"ai\\\",\\\"logic\\\"]}\",\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/1cb3813e458884a4.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]]}],\"childPropSegment\":[\"slug\",\"ai/logic\",\"c\"],\"styles\":null}],\"childPropSegment\":\"articles\",\"styles\":null}]}],[\"$\",\"footer\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"Footer_container__Z8cUU\",\"children\":[[\"$\",\"$Ld\",null,{\"href\":\"https://il.linkedin.com/in/tal-glanzman\",\"children\":[\"$\",\"svg\",null,{\"stroke\":\"currentColor\",\"fill\":\"currentColor\",\"strokeWidth\":\"0\",\"viewBox\":\"0 0 448 512\",\"children\":[\"$undefined\",[[\"$\",\"path\",\"0\",{\"d\":\"M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z\",\"children\":\"$undefined\"}]]],\"className\":\"$undefined\",\"style\":{\"color\":\"$undefined\"},\"height\":25,\"width\":25,\"xmlns\":\"http://www.w3.org/2000/svg\"}]}],[\"$\",\"$Ld\",null,{\"href\":\"https://github.com/tglanz\",\"children\":[\"$\",\"svg\",null,{\"stroke\":\"currentColor\",\"fill\":\"currentColor\",\"strokeWidth\":\"0\",\"viewBox\":\"0 0 496 512\",\"children\":[\"$undefined\",[[\"$\",\"path\",\"0\",{\"d\":\"$e\",\"children\":\"$undefined\"}]]],\"className\":\"$undefined\",\"style\":{\"color\":\"$undefined\"},\"height\":25,\"width\":25,\"xmlns\":\"http://www.w3.org/2000/svg\"}]}]]}]}]]}]}],null]}]]\n"])</script><script>self.__next_f.push([1,"6:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Article\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Personal site\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"256x256\"}],[\"$\",\"meta\",\"5\",{\"name\":\"next-size-adjust\"}]]\nb:null\n"])</script><script>self.__next_f.push([1,"f:I[6222,[\"954\",\"static/chunks/d3ac728e-1e5d8b71e3d43fec.js\",\"326\",\"static/chunks/326-dcee1ff54fa4f70c.js\",\"496\",\"static/chunks/app/articles/%5B...slug%5D/page-44f6caefb92fad34.js\"],\"ArticleContent\"]\n10:T291f,"])</script><script>self.__next_f.push([1,"\n# Logic \n\n**Logic** is the field of knowledge representation and reasoning about this knowledge according to some **Possible World** also known as a **Model**. There are different types of logic formulizations such as **Propositional Logic** and **First Order Logic**, each of which can represent different information.\n\nFormally, each logic defines its **Syntax**, which is the specification of how we express **Sentences**. In addition to syntax, the logic also defines its **Semantics** which specify the meaning of those sentences - it assigns a **Truth Value** to each sentence according to the model.\n\nA set of sentences is known as a **Knowledge Base**. The set of sentences that are given to us and assumed true are known as **Axioms**.\n\nIf a given sentence $\\alpha$ is true in some model $m$ we say that **$\\alpha$ is satisfied by the model $m$**. We will denote $M(\\alpha)$ to be the set of all models that satisfy $\\alpha$.\n\n**Reasoning** is the process of understanding the entailment between sentences. Given the sentences $\\alpha$ and $\\beta$, we say that $\\alpha$ entails the sentence $\\beta$ if and only if $M(\\alpha) \\subseteq M(\\beta)$, meaning that every model $m$ that satisfies $\\alpha$ also satisfies $\\beta$. We denote entailment by $\\alpha \\models \\beta$.\n\n## Logical Inference\n\nGiven a knowledge base $KB$, we want to derive new sentences - such a process is called **Logical Inference**. The algorithm we choose to derive new sentences can be arbitrary, but what's the point? We say that an inference algorithm is **Sound** if and only if it derives entailed sentences.\n\nFor example, consider the following axioms:\n\n- Some dogs are black\n- Every dog is an animal\n\nUsing some inference algorithm, we can infer the sentence: \"Every animal is black\" - but we know this is wrong, our algorithm is unsound.\n\nAnother algorithm can infer the sentence: \"Some animals are black\" which is sound.\n\nLastly, consider an inference algorithm that did not produce any sentence. If we desire to logically infer new sentences from the KB, such an algorithm is undesirable. This leads us to the second categorization of inference algorithms:\n\nInference algorithms that can derive any sentence that is entailed are called **Complete**.\n\nIdeal inference algorithms are both complete and sound. Such algorithms can derive all entailed sentences, and all derived sentences are entailed.\n\n# Propositional Logic\n\nThe first logic we will explore is the **Propositional Logic**.\n\nIn this logic's syntax, each sentence is either an **Atomic Sentence** or a **Complex Sentence**.\n\nAn atomic sentence consists of a **Proposition** - a symbol that can take a boolean value.\n\nA complex sentence is constructed of multiple sentences using **Parentheses** and **Logical Operators**.\n\nParentheses, as commonly used, specify the precedence of operations.\n\nLogical operators operate on sentences and return a boolean value. The logical operators are:\n\n- The **negation (not)** unary operator $\\neg$\n- The **conjunction (and)** binary operator $\\land$\n- The **disjunction (or)** binary operator $\\lor$\n- The **implication (implies)** binary operator $\\Rightarrow$\n- The **biconditional (if and only if)** binary operator $\\Leftrightarrow$\n\nThey are (unsuprisingly) defined by the truth table:\n\n| $P$ | $Q$ | $\\neg P$ | $P \\land Q$ | $P \\lor Q$ | $P \\Rightarrow Q$ | $P \\Leftrightarrow A$ |\n|-|-|-|-|-|-|-|\n|F|F|T|F|F|T|T|\n|F|T|T|F|T|T|F|\n|T|F|F|F|T|F|F|\n|T|T|F|T|T|T|T|\n\nThe following **Logical Equivalences** hold:\n\n\u003e Regardig the notation below, note that we use $\\cdot$ and $+$ as placeholders for when equivalence holds both for $\\land$ and $\\lor$. i.e. read $\\alpha \\cdot \\beta$ as $\\alpha \\land \\beta$ and also $\\alpha \\lor \\beta$. When we use both $\\cdot$ and $+$, it means we can use both interchangebly.\n\n- **Commutativity**: $\\alpha \\cdot \\beta \\equiv \\beta \\cdot \\alpha$ \n- **Associativity** meaning that $((\\alpha \\cdot \\beta)  \\cdot \\gamma) \\equiv (\\alpha \\cdot (\\beta \\cdot \\gamma))$\n- **Double Negation**: $\\neg (\\neg \\alpha) \\equiv \\alpha$\n- **Contraposition**: $\\alpha \\Rightarrow \\beta \\equiv \\neg \\beta \\Rightarrow \\neg \\alpha$\n  - This equivalence is frequently used in mathematical proofs and is the basis of \"proof by contradiction\"\n- **Implication Elimination**: $\\alpha \\Rightarrow \\beta \\equiv \\neg \\alpha \\lor \\beta$\n- **Bidirectional Elimination**: $\\alpha \\Leftrightarrow \\beta \\equiv (\\alpha \\Rightarrow \\beta) \\land (\\beta \\Rightarrow \\alpha))$\n- **Demorgan**: $\\neg (\\alpha \\cdot \\beta) \\equiv \\neg \\alpha \\cdot \\neg \\beta$\n- **Distributivity** $(\\alpha \\cdot (\\beta + \\gamma)) \\equiv (\\alpha \\cdot \\beta) + (\\alpha \\cdot \\gamma)$\n\n## Inference\n\nA commonly, sound rule that is used to derive sentences is known as **Modus Ponens** rule:\n\n$$\n    \\frac{\\alpha \\Rightarrow \\beta, \\alpha}{\\beta}\n$$\n\nRead it as: \"If we know that $\\alpha$ implies $\\beta$ and we also know that $\\alpha$, we derive that $\\beta$\".\n\nAnother sound rule is quite intuitive and is known as **And Elimination**:\n\n$$\n    \\frac{\\alpha \\land \\beta}{\\alpha}\n$$\n\nAs additional sound inference rules, we can use all of the known equivalences.\n\nThe last sound rule we will show is the **Resolution Rule**. This is an important rule that is used by a complete and sound inference algorithm known as the **Resultion Inference Algorithm** which we will see later.\n\nThe resolution rule states the following:\n\n$$\n    \\frac\n    {l_1 \\lor l_2 \\lor ... \\lor l_i \\lor ... \\lor l_n \\~,\\~ A \\lor \\neg l_i}\n    {l_1 \\lor l_2 \\lor ... \\lor l_{i-1} \\lor l_{i+1} \\lor ... \\lor l_n \\lor A}\n$$\n\nWhere $A$ is a disjunction of other literals.\n\nFor example:\n\n$$\n    \\frac{P \\lor Q \\~,\\~ \\neg P \\lor R}{Q \\lor R} \n$$\n\nNotice that this important rule operates only on disjunctions. However, our sentences can be in an arbitrary form. We find that every KB is equivalent to another KB that is in a very specific form known as \"conjunctive normal form\" - In this form, we can easily apply the resolution rule.\n\n## Conjunctive Normal Form (CNF)\n\nA **Clause** is a disjunction of literals. We say that a sentence that is expressed by a conjunction of clauses is in a **Conjunctive Normal Form**.\n\nWe already know how to transform a KB to a CNF - using the known equivalences!\n\n1. Eliminate bidirectional sentences\n2. Eliminate implications sentences\n3. Move negations inwards to literals mostly using double negation and de-morgan rules\n4. Apply distribution rules\n\nLet's see an example and convert the following sentence to CNF:\n\n$$\n    P \\Leftrightarrow Q \\lor R\n$$\n\n1. Bidirectional elimination: $(P \\Rightarrow Q \\lor R) \\land (Q \\lor R \\Rightarrow P)$\n2. Implication elimination of the left hand conjugate: $(\\neg P \\lor Q \\lor R) \\land (Q \\lor R \\Rightarrow P)$\n3. Implication elimination of the right hand conjugate: $(\\neg P \\lor Q \\lor R) \\land (\\neg (Q \\lor R) \\lor P)$\n4. Move negation inwards (apply de-morgan at the right-hand side): $(\\neg P \\lor Q \\lor R) \\land ((\\neg Q \\land \\neg R) \\lor P)$\n5. Distribute $\\lor$ over $\\land$ at the right-hand side: $(\\neg P \\lor Q \\lor R) \\land (\\neg Q \\lor P) \\land (\\neg R \\lor P)$\n\nAnd that's it, we have a sentence in CNF form.\n\n## The Resolution Algorithm\n\nThe **Resolution Algorithm** is sound and complete (will not show this here).\n\nGiven a KB $K$, we show that $K \\models \\alpha$ by proving that $K \\land \\neg \\alpha$ is a contradiction (proof by contradiction).\n\n1. We add $\\neg \\alpha$ to the KB\n2. Convert the KB to CNF form\n3. As long as we can apply the resolution rule, apply it\n    - If the derived sentence is non-empty, add it to the KB if it is doesn't exist there already\n    - Else, because this sentence is a contradiction (empty) we have reached a contradiction in the KB because of $\\neg \\alpha$ - Answer: the KB entails $\\alpha$\n4. No contradiction occurred - Answer: The KB does not entail $\\alpha$\n\n### Example\n\nAssume the initial KB \n\n- $P \\Leftrightarrow Q \\lor R$\n- $\\neg P$\n\nAnd let's say we want to prove that $\\neg Q$.\n\n**Add the negation of $\\neg Q$ to the KB**\n\n- $P \\Leftrightarrow Q \\lor R$\n- $\\neg P$\n- $Q$\n\n**Convert to CNF**\n\n1. $\\neg P \\lor Q \\lor R$\n2. $\\neg Q \\lor P$\n3. $\\neg R \\lor P$\n4. $\\neg P$\n5. $Q$\n\n**Repeatedly apply the Resolution Rule**\n\n6. (5, 2): $P$\n7. (6, 4): $\\phi$\n\nBecause we reached a contradiction, we deduce that the KB entails $\\neg Q$.\n\n# First Order Logic\n\nThink of it, how complex are the sentences that we could express via Propositional Logic? It turns out that we can formalize a logic language that is far more expressive than it.\n\n**First Order Logic (FOL)** is a logic formulation that includes objects and their relations to one another.\n\nFOL is a bit closer to natural language. In natural language, we can speak about objects such as a \"Chair\" or a \"Pen\". We can also talk about the relations between objects for example: \"Pen is on Chair\". Lastly, we can map objects to other objects as in \"Leg of the Chair\", kind of like functions.\n\nFOL builds on top of those concepts. Its components are:\n\n- **Truth Values (Booleans)** False and True\n- **Logical Operators** are the same logical operators we saw in propositional logic including the **Equal** operator $=$\n- **Terms (or Objects)** are the constants\n- **Variables** are placeholders for Terms\n- **Quantifiers**\n  - **Universal quantifier** (Forall) $\\forall x B$ whether the boolean expression $B$ holds for every possible instantiation of the variable $x$\n  - **Existential quantifier** (Exists) $\\exists x B$ whether the boolean expression $B$ holds for some possible instantiation of the variable $x$\n- **Relations (or Predicates)** are mappings from tuples of Terms to a Boolean\n- **Functions** are mappings from Term to Term\n\nFor clarity, we defined the booleans separately from the predicates. To be more accurate, $True$ and $False$ are also defined as predicates (that always return the same value).\n\nA sentence in FOL is either an **Atomic Sentence** or a **Complex Sentence**.\n\nAn atomic sentence is either a Term or a Predicate.\n\nA complex sentence is either an application of multiple sentences using logical operators or a quantified variable followed by a sentence.\n\n## Logical Inference\n\nWe won't go over the details about what logical inference is since we already covered it in the section about propositional logic. We will focus on the techniques that are related to FOL.\n\n# Examples from Exercises\n\n## Propositional Logic\n\n## First Order Logic\n\n# Hebrew Appendix\n\n- Entailement - נביעה לוגית\n- Sound - נאות\n- Complete - שלם\n- Proposition - פסוק\n"])</script><script>self.__next_f.push([1,"11:T3917,"])</script><script>self.__next_f.push([1,"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch1 id=\"logic\"\u003eLogic\u003c/h1\u003e\n    \u003cp\u003e\u003cstrong\u003eLogic\u003c/strong\u003e is the field of knowledge representation and reasoning about this knowledge according to some \u003cstrong\u003ePossible World\u003c/strong\u003e also known as a \u003cstrong\u003eModel\u003c/strong\u003e. There are different types of logic formulizations such as \u003cstrong\u003ePropositional Logic\u003c/strong\u003e and \u003cstrong\u003eFirst Order Logic\u003c/strong\u003e, each of which can represent different information.\u003c/p\u003e\n    \u003cp\u003eFormally, each logic defines its \u003cstrong\u003eSyntax\u003c/strong\u003e, which is the specification of how we express \u003cstrong\u003eSentences\u003c/strong\u003e. In addition to syntax, the logic also defines its \u003cstrong\u003eSemantics\u003c/strong\u003e which specify the meaning of those sentences - it assigns a \u003cstrong\u003eTruth Value\u003c/strong\u003e to each sentence according to the model.\u003c/p\u003e\n    \u003cp\u003eA set of sentences is known as a \u003cstrong\u003eKnowledge Base\u003c/strong\u003e. The set of sentences that are given to us and assumed true are known as \u003cstrong\u003eAxioms\u003c/strong\u003e.\u003c/p\u003e\n    \u003cp\u003eIf a given sentence $\\alpha$ is true in some model $m$ we say that \u003cstrong\u003e$\\alpha$ is satisfied by the model $m$\u003c/strong\u003e. We will denote $M(\\alpha)$ to be the set of all models that satisfy $\\alpha$.\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eReasoning\u003c/strong\u003e is the process of understanding the entailment between sentences. Given the sentences $\\alpha$ and $\\beta$, we say that $\\alpha$ entails the sentence $\\beta$ if and only if $M(\\alpha) \\subseteq M(\\beta)$, meaning that every model $m$ that satisfies $\\alpha$ also satisfies $\\beta$. We denote entailment by $\\alpha \\models \\beta$.\u003c/p\u003e\n    \u003ch2 id=\"logical-inference\"\u003eLogical Inference\u003c/h2\u003e\n    \u003cp\u003eGiven a knowledge base $KB$, we want to derive new sentences - such a process is called \u003cstrong\u003eLogical Inference\u003c/strong\u003e. The algorithm we choose to derive new sentences can be arbitrary, but what's the point? We say that an inference algorithm is \u003cstrong\u003eSound\u003c/strong\u003e if and only if it derives entailed sentences.\u003c/p\u003e\n    \u003cp\u003eFor example, consider the following axioms:\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eSome dogs are black\u003c/li\u003e\n      \u003cli\u003eEvery dog is an animal\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eUsing some inference algorithm, we can infer the sentence: \"Every animal is black\" - but we know this is wrong, our algorithm is unsound.\u003c/p\u003e\n    \u003cp\u003eAnother algorithm can infer the sentence: \"Some animals are black\" which is sound.\u003c/p\u003e\n    \u003cp\u003eLastly, consider an inference algorithm that did not produce any sentence. If we desire to logically infer new sentences from the KB, such an algorithm is undesirable. This leads us to the second categorization of inference algorithms:\u003c/p\u003e\n    \u003cp\u003eInference algorithms that can derive any sentence that is entailed are called \u003cstrong\u003eComplete\u003c/strong\u003e.\u003c/p\u003e\n    \u003cp\u003eIdeal inference algorithms are both complete and sound. Such algorithms can derive all entailed sentences, and all derived sentences are entailed.\u003c/p\u003e\n    \u003ch1 id=\"propositional-logic\"\u003ePropositional Logic\u003c/h1\u003e\n    \u003cp\u003eThe first logic we will explore is the \u003cstrong\u003ePropositional Logic\u003c/strong\u003e.\u003c/p\u003e\n    \u003cp\u003eIn this logic's syntax, each sentence is either an \u003cstrong\u003eAtomic Sentence\u003c/strong\u003e or a \u003cstrong\u003eComplex Sentence\u003c/strong\u003e.\u003c/p\u003e\n    \u003cp\u003eAn atomic sentence consists of a \u003cstrong\u003eProposition\u003c/strong\u003e - a symbol that can take a boolean value.\u003c/p\u003e\n    \u003cp\u003eA complex sentence is constructed of multiple sentences using \u003cstrong\u003eParentheses\u003c/strong\u003e and \u003cstrong\u003eLogical Operators\u003c/strong\u003e.\u003c/p\u003e\n    \u003cp\u003eParentheses, as commonly used, specify the precedence of operations.\u003c/p\u003e\n    \u003cp\u003eLogical operators operate on sentences and return a boolean value. The logical operators are:\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eThe \u003cstrong\u003enegation (not)\u003c/strong\u003e unary operator $\\neg$\u003c/li\u003e\n      \u003cli\u003eThe \u003cstrong\u003econjunction (and)\u003c/strong\u003e binary operator $\\land$\u003c/li\u003e\n      \u003cli\u003eThe \u003cstrong\u003edisjunction (or)\u003c/strong\u003e binary operator $\\lor$\u003c/li\u003e\n      \u003cli\u003eThe \u003cstrong\u003eimplication (implies)\u003c/strong\u003e binary operator $\\Rightarrow$\u003c/li\u003e\n      \u003cli\u003eThe \u003cstrong\u003ebiconditional (if and only if)\u003c/strong\u003e binary operator $\\Leftrightarrow$\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eThey are (unsuprisingly) defined by the truth table:\u003c/p\u003e\n    \u003cdiv class=\"table-container\"\u003e\n      \u003ctable\u003e\n        \u003cthead\u003e\n          \u003ctr\u003e\n            \u003cth\u003e$P$\u003c/th\u003e\n            \u003cth\u003e$Q$\u003c/th\u003e\n            \u003cth\u003e$\\neg P$\u003c/th\u003e\n            \u003cth\u003e$P \\land Q$\u003c/th\u003e\n            \u003cth\u003e$P \\lor Q$\u003c/th\u003e\n            \u003cth\u003e$P \\Rightarrow Q$\u003c/th\u003e\n            \u003cth\u003e$P \\Leftrightarrow A$\u003c/th\u003e\n          \u003c/tr\u003e\n        \u003c/thead\u003e\n        \u003ctbody\u003e\n          \u003ctr\u003e\n            \u003ctd\u003eF\u003c/td\u003e\n            \u003ctd\u003eF\u003c/td\u003e\n            \u003ctd\u003eT\u003c/td\u003e\n            \u003ctd\u003eF\u003c/td\u003e\n            \u003ctd\u003eF\u003c/td\u003e\n            \u003ctd\u003eT\u003c/td\u003e\n            \u003ctd\u003eT\u003c/td\u003e\n          \u003c/tr\u003e\n          \u003ctr\u003e\n            \u003ctd\u003eF\u003c/td\u003e\n            \u003ctd\u003eT\u003c/td\u003e\n            \u003ctd\u003eT\u003c/td\u003e\n            \u003ctd\u003eF\u003c/td\u003e\n            \u003ctd\u003eT\u003c/td\u003e\n            \u003ctd\u003eT\u003c/td\u003e\n            \u003ctd\u003eF\u003c/td\u003e\n          \u003c/tr\u003e\n          \u003ctr\u003e\n            \u003ctd\u003eT\u003c/td\u003e\n            \u003ctd\u003eF\u003c/td\u003e\n            \u003ctd\u003eF\u003c/td\u003e\n            \u003ctd\u003eF\u003c/td\u003e\n            \u003ctd\u003eT\u003c/td\u003e\n            \u003ctd\u003eF\u003c/td\u003e\n            \u003ctd\u003eF\u003c/td\u003e\n          \u003c/tr\u003e\n          \u003ctr\u003e\n            \u003ctd\u003eT\u003c/td\u003e\n            \u003ctd\u003eT\u003c/td\u003e\n            \u003ctd\u003eF\u003c/td\u003e\n            \u003ctd\u003eT\u003c/td\u003e\n            \u003ctd\u003eT\u003c/td\u003e\n            \u003ctd\u003eT\u003c/td\u003e\n            \u003ctd\u003eT\u003c/td\u003e\n          \u003c/tr\u003e\n        \u003c/tbody\u003e\n      \u003c/table\u003e\n    \u003c/div\u003e\n    \u003cp\u003eThe following \u003cstrong\u003eLogical Equivalences\u003c/strong\u003e hold:\u003c/p\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003eRegardig the notation below, note that we use $\\cdot$ and $+$ as placeholders for when equivalence holds both for $\\land$ and $\\lor$. i.e. read $\\alpha \\cdot \\beta$ as $\\alpha \\land \\beta$ and also $\\alpha \\lor \\beta$. When we use both $\\cdot$ and $+$, it means we can use both interchangebly.\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003cul\u003e\n      \u003cli\u003e\u003cstrong\u003eCommutativity\u003c/strong\u003e: $\\alpha \\cdot \\beta \\equiv \\beta \\cdot \\alpha$\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003eAssociativity\u003c/strong\u003e meaning that $((\\alpha \\cdot \\beta) \\cdot \\gamma) \\equiv (\\alpha \\cdot (\\beta \\cdot \\gamma))$\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003eDouble Negation\u003c/strong\u003e: $\\neg (\\neg \\alpha) \\equiv \\alpha$\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003eContraposition\u003c/strong\u003e: $\\alpha \\Rightarrow \\beta \\equiv \\neg \\beta \\Rightarrow \\neg \\alpha$\n        \u003cul\u003e\n          \u003cli\u003eThis equivalence is frequently used in mathematical proofs and is the basis of \"proof by contradiction\"\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003eImplication Elimination\u003c/strong\u003e: $\\alpha \\Rightarrow \\beta \\equiv \\neg \\alpha \\lor \\beta$\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003eBidirectional Elimination\u003c/strong\u003e: $\\alpha \\Leftrightarrow \\beta \\equiv (\\alpha \\Rightarrow \\beta) \\land (\\beta \\Rightarrow \\alpha))$\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003eDemorgan\u003c/strong\u003e: $\\neg (\\alpha \\cdot \\beta) \\equiv \\neg \\alpha \\cdot \\neg \\beta$\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003eDistributivity\u003c/strong\u003e $(\\alpha \\cdot (\\beta + \\gamma)) \\equiv (\\alpha \\cdot \\beta) + (\\alpha \\cdot \\gamma)$\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch2 id=\"inference\"\u003eInference\u003c/h2\u003e\n    \u003cp\u003eA commonly, sound rule that is used to derive sentences is known as \u003cstrong\u003eModus Ponens\u003c/strong\u003e rule:\u003c/p\u003e\n    \u003cp\u003e\n      $$\n      \\frac{\\alpha \\Rightarrow \\beta, \\alpha}{\\beta}\n      $$\n    \u003c/p\u003e\n    \u003cp\u003eRead it as: \"If we know that $\\alpha$ implies $\\beta$ and we also know that $\\alpha$, we derive that $\\beta$\".\u003c/p\u003e\n    \u003cp\u003eAnother sound rule is quite intuitive and is known as \u003cstrong\u003eAnd Elimination\u003c/strong\u003e:\u003c/p\u003e\n    \u003cp\u003e\n      $$\n      \\frac{\\alpha \\land \\beta}{\\alpha}\n      $$\n    \u003c/p\u003e\n    \u003cp\u003eAs additional sound inference rules, we can use all of the known equivalences.\u003c/p\u003e\n    \u003cp\u003eThe last sound rule we will show is the \u003cstrong\u003eResolution Rule\u003c/strong\u003e. This is an important rule that is used by a complete and sound inference algorithm known as the \u003cstrong\u003eResultion Inference Algorithm\u003c/strong\u003e which we will see later.\u003c/p\u003e\n    \u003cp\u003eThe resolution rule states the following:\u003c/p\u003e\n    \u003cp\u003e\n      $$\n      \\frac\n      {l_1 \\lor l_2 \\lor ... \\lor l_i \\lor ... \\lor l_n ~,~ A \\lor \\neg l_i}\n      {l_1 \\lor l_2 \\lor ... \\lor l_{i-1} \\lor l_{i+1} \\lor ... \\lor l_n \\lor A}\n      $$\n    \u003c/p\u003e\n    \u003cp\u003eWhere $A$ is a disjunction of other literals.\u003c/p\u003e\n    \u003cp\u003eFor example:\u003c/p\u003e\n    \u003cp\u003e\n      $$\n      \\frac{P \\lor Q ~,~ \\neg P \\lor R}{Q \\lor R}\n      $$\n    \u003c/p\u003e\n    \u003cp\u003eNotice that this important rule operates only on disjunctions. However, our sentences can be in an arbitrary form. We find that every KB is equivalent to another KB that is in a very specific form known as \"conjunctive normal form\" - In this form, we can easily apply the resolution rule.\u003c/p\u003e\n    \u003ch2 id=\"conjunctive-normal-form-cnf\"\u003eConjunctive Normal Form (CNF)\u003c/h2\u003e\n    \u003cp\u003eA \u003cstrong\u003eClause\u003c/strong\u003e is a disjunction of literals. We say that a sentence that is expressed by a conjunction of clauses is in a \u003cstrong\u003eConjunctive Normal Form\u003c/strong\u003e.\u003c/p\u003e\n    \u003cp\u003eWe already know how to transform a KB to a CNF - using the known equivalences!\u003c/p\u003e\n    \u003col\u003e\n      \u003cli\u003eEliminate bidirectional sentences\u003c/li\u003e\n      \u003cli\u003eEliminate implications sentences\u003c/li\u003e\n      \u003cli\u003eMove negations inwards to literals mostly using double negation and de-morgan rules\u003c/li\u003e\n      \u003cli\u003eApply distribution rules\u003c/li\u003e\n    \u003c/ol\u003e\n    \u003cp\u003eLet's see an example and convert the following sentence to CNF:\u003c/p\u003e\n    \u003cp\u003e\n      $$\n      P \\Leftrightarrow Q \\lor R\n      $$\n    \u003c/p\u003e\n    \u003col\u003e\n      \u003cli\u003eBidirectional elimination: $(P \\Rightarrow Q \\lor R) \\land (Q \\lor R \\Rightarrow P)$\u003c/li\u003e\n      \u003cli\u003eImplication elimination of the left hand conjugate: $(\\neg P \\lor Q \\lor R) \\land (Q \\lor R \\Rightarrow P)$\u003c/li\u003e\n      \u003cli\u003eImplication elimination of the right hand conjugate: $(\\neg P \\lor Q \\lor R) \\land (\\neg (Q \\lor R) \\lor P)$\u003c/li\u003e\n      \u003cli\u003eMove negation inwards (apply de-morgan at the right-hand side): $(\\neg P \\lor Q \\lor R) \\land ((\\neg Q \\land \\neg R) \\lor P)$\u003c/li\u003e\n      \u003cli\u003eDistribute $\\lor$ over $\\land$ at the right-hand side: $(\\neg P \\lor Q \\lor R) \\land (\\neg Q \\lor P) \\land (\\neg R \\lor P)$\u003c/li\u003e\n    \u003c/ol\u003e\n    \u003cp\u003eAnd that's it, we have a sentence in CNF form.\u003c/p\u003e\n    \u003ch2 id=\"the-resolution-algorithm\"\u003eThe Resolution Algorithm\u003c/h2\u003e\n    \u003cp\u003eThe \u003cstrong\u003eResolution Algorithm\u003c/strong\u003e is sound and complete (will not show this here).\u003c/p\u003e\n    \u003cp\u003eGiven a KB $K$, we show that $K \\models \\alpha$ by proving that $K \\land \\neg \\alpha$ is a contradiction (proof by contradiction).\u003c/p\u003e\n    \u003col\u003e\n      \u003cli\u003eWe add $\\neg \\alpha$ to the KB\u003c/li\u003e\n      \u003cli\u003eConvert the KB to CNF form\u003c/li\u003e\n      \u003cli\u003eAs long as we can apply the resolution rule, apply it\n        \u003cul\u003e\n          \u003cli\u003eIf the derived sentence is non-empty, add it to the KB if it is doesn't exist there already\u003c/li\u003e\n          \u003cli\u003eElse, because this sentence is a contradiction (empty) we have reached a contradiction in the KB because of $\\neg \\alpha$ - Answer: the KB entails $\\alpha$\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003eNo contradiction occurred - Answer: The KB does not entail $\\alpha$\u003c/li\u003e\n    \u003c/ol\u003e\n    \u003ch3 id=\"example\"\u003eExample\u003c/h3\u003e\n    \u003cp\u003eAssume the initial KB\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e$P \\Leftrightarrow Q \\lor R$\u003c/li\u003e\n      \u003cli\u003e$\\neg P$\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eAnd let's say we want to prove that $\\neg Q$.\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eAdd the negation of $\\neg Q$ to the KB\u003c/strong\u003e\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e$P \\Leftrightarrow Q \\lor R$\u003c/li\u003e\n      \u003cli\u003e$\\neg P$\u003c/li\u003e\n      \u003cli\u003e$Q$\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003e\u003cstrong\u003eConvert to CNF\u003c/strong\u003e\u003c/p\u003e\n    \u003col\u003e\n      \u003cli\u003e$\\neg P \\lor Q \\lor R$\u003c/li\u003e\n      \u003cli\u003e$\\neg Q \\lor P$\u003c/li\u003e\n      \u003cli\u003e$\\neg R \\lor P$\u003c/li\u003e\n      \u003cli\u003e$\\neg P$\u003c/li\u003e\n      \u003cli\u003e$Q$\u003c/li\u003e\n    \u003c/ol\u003e\n    \u003cp\u003e\u003cstrong\u003eRepeatedly apply the Resolution Rule\u003c/strong\u003e\u003c/p\u003e\n    \u003col start=\"6\"\u003e\n      \u003cli\u003e(5, 2): $P$\u003c/li\u003e\n      \u003cli\u003e(6, 4): $\\phi$\u003c/li\u003e\n    \u003c/ol\u003e\n    \u003cp\u003eBecause we reached a contradiction, we deduce that the KB entails $\\neg Q$.\u003c/p\u003e\n    \u003ch1 id=\"first-order-logic\"\u003eFirst Order Logic\u003c/h1\u003e\n    \u003cp\u003eThink of it, how complex are the sentences that we could express via Propositional Logic? It turns out that we can formalize a logic language that is far more expressive than it.\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eFirst Order Logic (FOL)\u003c/strong\u003e is a logic formulation that includes objects and their relations to one another.\u003c/p\u003e\n    \u003cp\u003eFOL is a bit closer to natural language. In natural language, we can speak about objects such as a \"Chair\" or a \"Pen\". We can also talk about the relations between objects for example: \"Pen is on Chair\". Lastly, we can map objects to other objects as in \"Leg of the Chair\", kind of like functions.\u003c/p\u003e\n    \u003cp\u003eFOL builds on top of those concepts. Its components are:\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\u003cstrong\u003eTruth Values (Booleans)\u003c/strong\u003e False and True\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003eLogical Operators\u003c/strong\u003e are the same logical operators we saw in propositional logic including the \u003cstrong\u003eEqual\u003c/strong\u003e operator $=$\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003eTerms (or Objects)\u003c/strong\u003e are the constants\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003eVariables\u003c/strong\u003e are placeholders for Terms\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003eQuantifiers\u003c/strong\u003e\n        \u003cul\u003e\n          \u003cli\u003e\u003cstrong\u003eUniversal quantifier\u003c/strong\u003e (Forall) $\\forall x B$ whether the boolean expression $B$ holds for every possible instantiation of the variable $x$\u003c/li\u003e\n          \u003cli\u003e\u003cstrong\u003eExistential quantifier\u003c/strong\u003e (Exists) $\\exists x B$ whether the boolean expression $B$ holds for some possible instantiation of the variable $x$\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003eRelations (or Predicates)\u003c/strong\u003e are mappings from tuples of Terms to a Boolean\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003eFunctions\u003c/strong\u003e are mappings from Term to Term\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eFor clarity, we defined the booleans separately from the predicates. To be more accurate, $True$ and $False$ are also defined as predicates (that always return the same value).\u003c/p\u003e\n    \u003cp\u003eA sentence in FOL is either an \u003cstrong\u003eAtomic Sentence\u003c/strong\u003e or a \u003cstrong\u003eComplex Sentence\u003c/strong\u003e.\u003c/p\u003e\n    \u003cp\u003eAn atomic sentence is either a Term or a Predicate.\u003c/p\u003e\n    \u003cp\u003eA complex sentence is either an application of multiple sentences using logical operators or a quantified variable followed by a sentence.\u003c/p\u003e\n    \u003ch2 id=\"logical-inference-1\"\u003eLogical Inference\u003c/h2\u003e\n    \u003cp\u003eWe won't go over the details about what logical inference is since we already covered it in the section about propositional logic. We will focus on the techniques that are related to FOL.\u003c/p\u003e\n    \u003ch1 id=\"examples-from-exercises\"\u003eExamples from Exercises\u003c/h1\u003e\n    \u003ch2 id=\"propositional-logic-1\"\u003ePropositional Logic\u003c/h2\u003e\n    \u003ch2 id=\"first-order-logic-1\"\u003eFirst Order Logic\u003c/h2\u003e\n    \u003ch1 id=\"hebrew-appendix\"\u003eHebrew Appendix\u003c/h1\u003e\n    \u003cul\u003e\n      \u003cli\u003eEntailement - נביעה לוגית\u003c/li\u003e\n      \u003cli\u003eSound - נאות\u003c/li\u003e\n      \u003cli\u003eComplete - שלם\u003c/li\u003e\n      \u003cli\u003eProposition - פסוק\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"])</script><script>self.__next_f.push([1,"c:[\"$\",\"div\",null,{\"className\":\"article-page_container__5yaZl\",\"children\":[[\"$\",\"h1\",null,{\"children\":\"Logic\"}],[\"$\",\"div\",null,{\"className\":\"Taxonomy_container__C2Vdb\",\"children\":[\"$\",\"div\",null,{\"className\":\"Taxonomy_values__yvK16\",\"children\":[[\"$\",\"$Ld\",\"0\",{\"href\":\"/tags/Logic\",\"children\":[\"$\",\"div\",null,{\"className\":\"Chip_container__q1AW2\",\"children\":\"Logic\"}]}]]}]}],[\"$\",\"$Lf\",null,{\"content\":{\"raw\":\"$10\",\"html\":\"$11\"}}]]}]\n"])</script><script>self.__next_f.push([1,""])</script></body></html>