{"pageProps":{"article":{"id":"probabilistic-method/k-wise-independence","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/probabilistic-method/k-wise-independence.md","metadata":{"title":"k-Wise independence","description":null,"priority":0,"tags":["Probabilistic Method"],"categories":["Probabilistic Method"],"toc":false},"content":{"raw":"\n# k-wise independence\n\n**Definition** The $n$ random variables $X_1, X_2, ..., X_n$ are said to be **k-wise independent** iff any subset of $k$ random variables $X_{i_1}, X_{i_2}, ..., X_{i_k}$ consists of randomly independent variables, i.e.:\n\n$$\n    Pr(X_{i_1}, X_{i_2}, ..., X_{i_k}) = \\prod_{j=1}^k Pr(X_{i_j})\n$$\n\nCombinatorically, there are $n \\choose k$ different constraints.\n\nGiven $n$, for every $k < n -1$ we can construct a set of $n$ random variables that are k-wise indpendent but not (k+1)-wise independent. Let's see a specific example:\n\n\n## Example, 2 but not 3 wise independence\n\nFor an example, we will show a group of 3 random variables that are 2-wise independent and are not 3-wise independent.\n\nLet $X, Y$ be uniformly random bits (i.e. they are random variables that can assume the values 0 or 1).\n\n\nThe truth table, with the probability of of the entry:\n\nPr(X, Y) | $X$ | $Y$\n:-:|:-:|:-:\n$\\frac{1}{4}$ |0|0\n$\\frac{1}{4}$ |0|1\n$\\frac{1}{4}$ |1|0\n$\\frac{1}{4}$ |1|1\n\nWe can see that $Pr(X, Y) = Pr(X)Pr(Y)$ so obsiously $X$ and $Y$ are independent.\n\nNow, let's define $Z := X \\oplus Y$ where $\\oplus$ is the XOR operation. We can write the truth table:\n\n$X$ | $Y$ | $Z$\n:-:|:-:|:-:\n0|0|0\n0|1|1\n1|0|1\n1|1|0\n\nLet's examine only $X$ and $Z$:\n\nPr(X, Z) | $X$ | $Z$\n:-:|:-:|:-:\n$\\frac{1}{4}$ |0|0\n$\\frac{1}{4}$ |0|1\n$\\frac{1}{4}$ |1|1\n$\\frac{1}{4}$ |1|0\n\nWe can see that $Pr(X, Z) = Pr(X)Pr(Z)$ so obsiously $X$ and $Z$ are independent.\n\nLet's examine only $Y$ and $Z$:\n\nPr(Y, Z) | $Y$ | $Z$\n:-:|:-:|:-:\n$\\frac{1}{4}$ |0|0\n$\\frac{1}{4}$ |1|1\n$\\frac{1}{4}$ |0|1\n$\\frac{1}{4}$ |1|0\n\nWe can see that $Pr(Y, Z) = Pr(Y)Pr(Z)$ so obsiously $Y$ and $Z$ are independent.\n\nAll in all, we saw that every pair from $X, Y$ and $Z$ are independent. But of course they are not 3-wise indpeendent because by the definition of $Z$ it is not random at all! $Z$ is defined as the XOR of $X$ and $Y$ and is not assuming random values. For example, $Pr(X=0, Y=0, Z=1) = 0$ rather than $\\frac{1}{2}^3$.\n\n## The construction of Vandermonde (VDM)\n\nThe construction of VDM is a construction of $N$ random variables that are uniformly distributed in $\\{ 0, 1, ..., N \\}$ that are k-wise independent.\n\nLet $\\mathbb{F}$ be a field such that $\\mathbb{|F|} = N$ and $N$ is a power of a primary number.\n\n> We limited the genrality a bit by limiting $N$\n\nLet $a_0, a_1, ..., a_k \\in \\mathbb{F}$ be $k$ independent uniformly random variables. We define a unfiormly random polynomail $p(x)$ of degree $k$ by:\n\n$$\n    p(x) = \\sum_{i = 0}^{k-1} a_i x^i\n$$\n\nFor every $\\alpha \\in \\mathbb{F}$ the polynomial $p$ implicitly defines a random variable $p(\\alpha)$. \n\nWe will show that for every $k$ different elements $\\alpha_0, \\alpha_2, ..., \\alpha_{k-1} \\in \\mathbb{F}$ the $k$ random variables $P =  \\{ p(\\alpha_0), p(\\alpha_1), ..., p(\\alpha_{k-1}) \\}$ are independent and are uniformly distributed.\n\nOnce we showed this, we showed that the $N$ random variables which are implicitly defined from all the elements in the field $\\mathbb{F}$, are k-wise independent.\n\nTo show that $P$ are independent we need to show that for every $v_0, v_1, ..., v_{k-1} \\in \\mathbb{F}$ it holds true that\n\n$$\n  Pr(p(\\alpha_0) = v_0, p(\\alpha_1) = v_1, ..., p(\\alpha_{k-1}) = v_{k-1}) = Pr(p(\\alpha_0) = v_0) \\cdot Pr(p(\\alpha_1) = v_1) \\cdot \\cdot \\cdot Pr(p(\\alpha_{k-1}) = v_{k-1})\n$$\n\nOr in vector form:\n\n$$\n    Pr(p(\\alpha) = v) = \\prod_{i=0}^{k-1} Pr(p(\\alpha_i) = v_i)\n$$\n\nAnd to show that $P$ are uniformly distributed we need to show that\n\n$$\n    Pr(p(\\alpha) = v) = \\prod_{i=0}^{k-1} Pr(p(\\alpha_i) = v_i) = N^{-k}\n$$\n\nLet's get to work then. We start by expanding the expressions:\n\n\\begin{align*}\n&p(\\alpha_0) = \\sum_{i=0}^{k-1} a_i \\alpha_0^i = v_0 \\\\\\\\\n&p(\\alpha_1) = \\sum_{i=0}^{k-1} a_i \\alpha_1^i = v_1 \\\\\\\\\n&... \\\\\\\\\n&p(\\alpha_{k-1}) = \\sum_{i=0}^{k-1} a_i \\alpha_{k-1}^i = v_{k-1} \\\\\\\\\n\\end{align*}\n\nLet $V$ be the matrix:\n\n$$\nV = \n\\begin{pmatrix}\n\\alpha_0^0 & \\alpha_0^1 & ... & \\alpha_0^{k-1} \\\\\\\\\n\\alpha_1^0 & \\alpha_1^1 & ... & \\alpha_1^{k-1} \\\\\\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\\\\\n\\alpha_{k-1}^0 & \\alpha_{k-1}^1 & ... & \\alpha_{k-1}^{k-1} \\\\\\\\\n\\end{pmatrix}\n$$\n\nThis kind of matrix is well known as the **VDM** matrix of $\\alpha$.\n\nUsing $V$ we can write the previous equations in matrix form:\n\n$$\n    V a = p(\\alpha) = v\n$$\n\nA simple calculation shows that $det(V) \\neq 0$. We conclude then that $V$ is inversible.\n\nSo,\n\n$$\n    Pr(p(\\alpha) = v) = Pr(Va = v) = Pr(a = V^{-1}v)\n$$\n\nWell, what is the probability $Pr(a = V^{-1}v)$? $V^{-1}v = u$ is just an arbitrary vector in $\\mathbb{F}^k$. The probability of $a$ to be this exact vector is\n\n$$\n    Pr \\biggr[ a_0 = u_0, a_1 = u_1, ..., a_{k-1} = u_{k-1} \\biggr]\n$$\n\nBy definition, $\\{ a_i \\}_{i=0}^{k-1}$ are uniform and independent variables so we get that\n\n\\begin{align*}\n    Pr(p(\\alpha) = v) =& Pr(Va = v) \\\\\\\\\n    =& Pr(a = V^{-1}v) = Pr(a = u) = \\prod_{i=0}^{k-1} \\frac{1}{N} \\\\\\\\\n    =& N^{k}\n\\end{align*}\n\nAs we wanted.","html":"<!doctype html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"utf-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n  </head>\n  <body>\n    <h1 id=\"k-wise-independence\">k-wise independence</h1>\n    <p><strong>Definition</strong> The $n$ random variables $X_1, X_2, ..., X_n$ are said to be <strong>k-wise independent</strong> iff any subset of $k$ random variables $X_{i_1}, X_{i_2}, ..., X_{i_k}$ consists of randomly independent variables, i.e.:</p>\n    <p>\n      $$\n      Pr(X_{i_1}, X_{i_2}, ..., X_{i_k}) = \\prod_{j=1}^k Pr(X_{i_j})\n      $$\n    </p>\n    <p>Combinatorically, there are $n \\choose k$ different constraints.</p>\n    <p>Given $n$, for every $k &#x3C; n -1$ we can construct a set of $n$ random variables that are k-wise indpendent but not (k+1)-wise independent. Let's see a specific example:</p>\n    <h2 id=\"example-2-but-not-3-wise-independence\">Example, 2 but not 3 wise independence</h2>\n    <p>For an example, we will show a group of 3 random variables that are 2-wise independent and are not 3-wise independent.</p>\n    <p>Let $X, Y$ be uniformly random bits (i.e. they are random variables that can assume the values 0 or 1).</p>\n    <p>The truth table, with the probability of of the entry:</p>\n    <div class=\"table-container\">\n      <table>\n        <thead>\n          <tr>\n            <th align=\"center\">Pr(X, Y)</th>\n            <th align=\"center\">$X$</th>\n            <th align=\"center\">$Y$</th>\n          </tr>\n        </thead>\n        <tbody>\n          <tr>\n            <td align=\"center\">$\\frac{1}{4}$</td>\n            <td align=\"center\">0</td>\n            <td align=\"center\">0</td>\n          </tr>\n          <tr>\n            <td align=\"center\">$\\frac{1}{4}$</td>\n            <td align=\"center\">0</td>\n            <td align=\"center\">1</td>\n          </tr>\n          <tr>\n            <td align=\"center\">$\\frac{1}{4}$</td>\n            <td align=\"center\">1</td>\n            <td align=\"center\">0</td>\n          </tr>\n          <tr>\n            <td align=\"center\">$\\frac{1}{4}$</td>\n            <td align=\"center\">1</td>\n            <td align=\"center\">1</td>\n          </tr>\n        </tbody>\n      </table>\n    </div>\n    <p>We can see that $Pr(X, Y) = Pr(X)Pr(Y)$ so obsiously $X$ and $Y$ are independent.</p>\n    <p>Now, let's define $Z := X \\oplus Y$ where $\\oplus$ is the XOR operation. We can write the truth table:</p>\n    <div class=\"table-container\">\n      <table>\n        <thead>\n          <tr>\n            <th align=\"center\">$X$</th>\n            <th align=\"center\">$Y$</th>\n            <th align=\"center\">$Z$</th>\n          </tr>\n        </thead>\n        <tbody>\n          <tr>\n            <td align=\"center\">0</td>\n            <td align=\"center\">0</td>\n            <td align=\"center\">0</td>\n          </tr>\n          <tr>\n            <td align=\"center\">0</td>\n            <td align=\"center\">1</td>\n            <td align=\"center\">1</td>\n          </tr>\n          <tr>\n            <td align=\"center\">1</td>\n            <td align=\"center\">0</td>\n            <td align=\"center\">1</td>\n          </tr>\n          <tr>\n            <td align=\"center\">1</td>\n            <td align=\"center\">1</td>\n            <td align=\"center\">0</td>\n          </tr>\n        </tbody>\n      </table>\n    </div>\n    <p>Let's examine only $X$ and $Z$:</p>\n    <div class=\"table-container\">\n      <table>\n        <thead>\n          <tr>\n            <th align=\"center\">Pr(X, Z)</th>\n            <th align=\"center\">$X$</th>\n            <th align=\"center\">$Z$</th>\n          </tr>\n        </thead>\n        <tbody>\n          <tr>\n            <td align=\"center\">$\\frac{1}{4}$</td>\n            <td align=\"center\">0</td>\n            <td align=\"center\">0</td>\n          </tr>\n          <tr>\n            <td align=\"center\">$\\frac{1}{4}$</td>\n            <td align=\"center\">0</td>\n            <td align=\"center\">1</td>\n          </tr>\n          <tr>\n            <td align=\"center\">$\\frac{1}{4}$</td>\n            <td align=\"center\">1</td>\n            <td align=\"center\">1</td>\n          </tr>\n          <tr>\n            <td align=\"center\">$\\frac{1}{4}$</td>\n            <td align=\"center\">1</td>\n            <td align=\"center\">0</td>\n          </tr>\n        </tbody>\n      </table>\n    </div>\n    <p>We can see that $Pr(X, Z) = Pr(X)Pr(Z)$ so obsiously $X$ and $Z$ are independent.</p>\n    <p>Let's examine only $Y$ and $Z$:</p>\n    <div class=\"table-container\">\n      <table>\n        <thead>\n          <tr>\n            <th align=\"center\">Pr(Y, Z)</th>\n            <th align=\"center\">$Y$</th>\n            <th align=\"center\">$Z$</th>\n          </tr>\n        </thead>\n        <tbody>\n          <tr>\n            <td align=\"center\">$\\frac{1}{4}$</td>\n            <td align=\"center\">0</td>\n            <td align=\"center\">0</td>\n          </tr>\n          <tr>\n            <td align=\"center\">$\\frac{1}{4}$</td>\n            <td align=\"center\">1</td>\n            <td align=\"center\">1</td>\n          </tr>\n          <tr>\n            <td align=\"center\">$\\frac{1}{4}$</td>\n            <td align=\"center\">0</td>\n            <td align=\"center\">1</td>\n          </tr>\n          <tr>\n            <td align=\"center\">$\\frac{1}{4}$</td>\n            <td align=\"center\">1</td>\n            <td align=\"center\">0</td>\n          </tr>\n        </tbody>\n      </table>\n    </div>\n    <p>We can see that $Pr(Y, Z) = Pr(Y)Pr(Z)$ so obsiously $Y$ and $Z$ are independent.</p>\n    <p>All in all, we saw that every pair from $X, Y$ and $Z$ are independent. But of course they are not 3-wise indpeendent because by the definition of $Z$ it is not random at all! $Z$ is defined as the XOR of $X$ and $Y$ and is not assuming random values. For example, $Pr(X=0, Y=0, Z=1) = 0$ rather than $\\frac{1}{2}^3$.</p>\n    <h2 id=\"the-construction-of-vandermonde-vdm\">The construction of Vandermonde (VDM)</h2>\n    <p>The construction of VDM is a construction of $N$ random variables that are uniformly distributed in ${ 0, 1, ..., N }$ that are k-wise independent.</p>\n    <p>Let $\\mathbb{F}$ be a field such that $\\mathbb{|F|} = N$ and $N$ is a power of a primary number.</p>\n    <blockquote>\n      <p>We limited the genrality a bit by limiting $N$</p>\n    </blockquote>\n    <p>Let $a_0, a_1, ..., a_k \\in \\mathbb{F}$ be $k$ independent uniformly random variables. We define a unfiormly random polynomail $p(x)$ of degree $k$ by:</p>\n    <p>\n      $$\n      p(x) = \\sum_{i = 0}^{k-1} a_i x^i\n      $$\n    </p>\n    <p>For every $\\alpha \\in \\mathbb{F}$ the polynomial $p$ implicitly defines a random variable $p(\\alpha)$.</p>\n    <p>We will show that for every $k$ different elements $\\alpha_0, \\alpha_2, ..., \\alpha_{k-1} \\in \\mathbb{F}$ the $k$ random variables $P = { p(\\alpha_0), p(\\alpha_1), ..., p(\\alpha_{k-1}) }$ are independent and are uniformly distributed.</p>\n    <p>Once we showed this, we showed that the $N$ random variables which are implicitly defined from all the elements in the field $\\mathbb{F}$, are k-wise independent.</p>\n    <p>To show that $P$ are independent we need to show that for every $v_0, v_1, ..., v_{k-1} \\in \\mathbb{F}$ it holds true that</p>\n    <p>\n      $$\n      Pr(p(\\alpha_0) = v_0, p(\\alpha_1) = v_1, ..., p(\\alpha_{k-1}) = v_{k-1}) = Pr(p(\\alpha_0) = v_0) \\cdot Pr(p(\\alpha_1) = v_1) \\cdot \\cdot \\cdot Pr(p(\\alpha_{k-1}) = v_{k-1})\n      $$\n    </p>\n    <p>Or in vector form:</p>\n    <p>\n      $$\n      Pr(p(\\alpha) = v) = \\prod_{i=0}^{k-1} Pr(p(\\alpha_i) = v_i)\n      $$\n    </p>\n    <p>And to show that $P$ are uniformly distributed we need to show that</p>\n    <p>\n      $$\n      Pr(p(\\alpha) = v) = \\prod_{i=0}^{k-1} Pr(p(\\alpha_i) = v_i) = N^{-k}\n      $$\n    </p>\n    <p>Let's get to work then. We start by expanding the expressions:</p>\n    <p>\n      \\begin{align*}\n      &#x26;p(\\alpha_0) = \\sum_{i=0}^{k-1} a_i \\alpha_0^i = v_0 \\\\\n      &#x26;p(\\alpha_1) = \\sum_{i=0}^{k-1} a_i \\alpha_1^i = v_1 \\\\\n      &#x26;... \\\\\n      &#x26;p(\\alpha_{k-1}) = \\sum_{i=0}^{k-1} a_i \\alpha_{k-1}^i = v_{k-1} \\\\\n      \\end{align*}\n    </p>\n    <p>Let $V$ be the matrix:</p>\n    <p>\n      $$\n      V =\n      \\begin{pmatrix}\n      \\alpha_0^0 &#x26; \\alpha_0^1 &#x26; ... &#x26; \\alpha_0^{k-1} \\\\\n      \\alpha_1^0 &#x26; \\alpha_1^1 &#x26; ... &#x26; \\alpha_1^{k-1} \\\\\n      \\vdots &#x26; \\vdots &#x26; \\vdots &#x26; \\vdots \\\\\n      \\alpha_{k-1}^0 &#x26; \\alpha_{k-1}^1 &#x26; ... &#x26; \\alpha_{k-1}^{k-1} \\\\\n      \\end{pmatrix}\n      $$\n    </p>\n    <p>This kind of matrix is well known as the <strong>VDM</strong> matrix of $\\alpha$.</p>\n    <p>Using $V$ we can write the previous equations in matrix form:</p>\n    <p>\n      $$\n      V a = p(\\alpha) = v\n      $$\n    </p>\n    <p>A simple calculation shows that $det(V) \\neq 0$. We conclude then that $V$ is inversible.</p>\n    <p>So,</p>\n    <p>\n      $$\n      Pr(p(\\alpha) = v) = Pr(Va = v) = Pr(a = V^{-1}v)\n      $$\n    </p>\n    <p>Well, what is the probability $Pr(a = V^{-1}v)$? $V^{-1}v = u$ is just an arbitrary vector in $\\mathbb{F}^k$. The probability of $a$ to be this exact vector is</p>\n    <p>\n      $$\n      Pr \\biggr[ a_0 = u_0, a_1 = u_1, ..., a_{k-1} = u_{k-1} \\biggr]\n      $$\n    </p>\n    <p>By definition, ${ a_i }_{i=0}^{k-1}$ are uniform and independent variables so we get that</p>\n    <p>\n      \\begin{align*}\n      Pr(p(\\alpha) = v) =&#x26; Pr(Va = v) \\\\\n      =&#x26; Pr(a = V^{-1}v) = Pr(a = u) = \\prod_{i=0}^{k-1} \\frac{1}{N} \\\\\n      =&#x26; N^{k}\n      \\end{align*}\n    </p>\n    <p>As we wanted.</p>\n  </body>\n</html>\n"}}},"__N_SSG":true}