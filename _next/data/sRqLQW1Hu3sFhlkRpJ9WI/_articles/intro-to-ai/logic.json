{"pageProps":{"article":{"id":"intro-to-ai/logic","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/intro-to-ai/logic.md","metadata":{"title":"Logic","description":null,"priority":200,"tags":["Logic"],"categories":["Intro to AI"],"toc":false},"content":{"raw":"\n# Logic \n\n**Logic** is the field of knowledge representation and reasoning about this knowledge according to some **Possible World** also known as a **Model**. There are different types of logic formulizations such as **Propositional Logic** and **First Order Logic**, each of which can represent different information.\n\nFormally, each logic defines its **Syntax**, which is the specification of how we express **Sentences**. In addition to syntax, the logic also defines its **Semantics** which specify the meaning of those sentences - it assigns a **Truth Value** to each sentence according to the model.\n\nA set of sentences is known as a **Knowledge Base**. The set of sentences that are given to us and assumed true are known as **Axioms**.\n\nIf a given sentence $\\alpha$ is true in some model $m$ we say that **$\\alpha$ is satisfied by the model $m$**. We will denote $M(\\alpha)$ to be the set of all models that satisfy $\\alpha$.\n\n**Reasoning** is the process of understanding the entailment between sentences. Given the sentences $\\alpha$ and $\\beta$, we say that $\\alpha$ entails the sentence $\\beta$ if and only if $M(\\alpha) \\subseteq M(\\beta)$, meaning that every model $m$ that satisfies $\\alpha$ also satisfies $\\beta$. We denote entailment by $\\alpha \\models \\beta$.\n\n## Logical Inference\n\nGiven a knowledge base $KB$, we want to derive new sentences - such a process is called **Logical Inference**. The algorithm we choose to derive new sentences can be arbitrary, but what's the point? We say that an inference algorithm is **Sound** if and only if it derives entailed sentences.\n\nFor example, consider the following axioms:\n\n- Some dogs are black\n- Every dog is an animal\n\nUsing some inference algorithm, we can infer the sentence: \"Every animal is black\" - but we know this is wrong, our algorithm is unsound.\n\nAnother algorithm can infer the sentence: \"Some animals are black\" which is sound.\n\nLastly, consider an inference algorithm that did not produce any sentence. If we desire to logically infer new sentences from the KB, such an algorithm is undesirable. This leads us to the second categorization of inference algorithms:\n\nInference algorithms that can derive any sentence that is entailed are called **Complete**.\n\nIdeal inference algorithms are both complete and sound. Such algorithms can derive all entailed sentences, and all derived sentences are entailed.\n\n# Propositional Logic\n\nThe first logic we will explore is the **Propositional Logic**.\n\nIn this logic's syntax, each sentence is either an **Atomic Sentence** or a **Complex Sentence**.\n\nAn atomic sentence consists of a **Proposition** - a symbol that can take a boolean value.\n\nA complex sentence is constructed of multiple sentences using **Parentheses** and **Logical Operators**.\n\nParentheses, as commonly used, specify the precedence of operations.\n\nLogical operators operate on sentences and return a boolean value. The logical operators are:\n\n- The **negation (not)** unary operator $\\neg$\n- The **conjunction (and)** binary operator $\\land$\n- The **disjunction (or)** binary operator $\\lor$\n- The **implication (implies)** binary operator $\\Rightarrow$\n- The **biconditional (if and only if)** binary operator $\\Leftrightarrow$\n\nThey are (unsuprisingly) defined by the truth table:\n\n| $P$ | $Q$ | $\\neg P$ | $P \\land Q$ | $P \\lor Q$ | $P \\Rightarrow Q$ | $P \\Leftrightarrow A$ |\n|-|-|-|-|-|-|-|\n|F|F|T|F|F|T|T|\n|F|T|T|F|T|T|F|\n|T|F|F|F|T|F|F|\n|T|T|F|T|T|T|T|\n\nThe following **Logical Equivalences** hold:\n\n> Notation note: that we use $\\cdot$ and $+$ as placeholders for when equivalence holds both for $\\land$ and $\\lor$. i.e. read $\\alpha \\cdot \\beta$ as $\\alpha \\land \\beta$ and also $\\alpha \\lor \\beta$. When we use both $\\cdot$ and $+$, it means we can use both interchangebly.\n\n- **Commutativity**: $\\alpha \\cdot \\beta \\equiv \\beta \\cdot \\alpha$ \n- **Associativity** operators, meaning that $((\\alpha \\cdot \\beta)  \\cdot \\gamma) \\equiv (\\alpha \\cdot (\\beta \\cdot \\gamma))$\n- **Double Negation**: $\\neg (\\neg \\alpha) \\equiv \\alpha$\n- **Contraposition**: $\\alpha \\Rightarrow \\beta \\equiv \\neg \\beta \\Rightarrow \\neg \\alpha$\n  - This equivalence is frequently used in mathematical proofs and is the basis of \"proof by contradiction\"\n- **Implication Elimination**: $\\alpha \\Rightarrow \\beta \\equiv \\neg \\alpha \\lor \\beta$\n- **Bidirectional Elimination**: $\\alpha \\Leftrightarrow \\beta \\equiv (\\alpha \\Rightarrow \\beta) \\land (\\beta \\Rightarrow \\alpha))$\n- **Demorgan**: $\\neg (\\alpha \\cdot \\beta) \\equiv \\neg \\alpha \\cdot \\neg \\beta$\n- **Distributivity** $(\\alpha \\cdot (\\beta + \\gamma)) \\equiv (\\alpha \\cdot \\beta) + (\\alpha \\cdot \\gamma)$\n\n## Inference\n\nA commonly, sound rule that is used to derive sentences is known as **Modus \\alphaonens** rule:\n\n$$\n    \\frac{\\alpha \\Rightarrow \\beta, \\alpha}{\\beta}\n$$\n\nRead it as: \"If we know that $\\alpha$ implies $\\beta$ and we also know that $\\alpha$, we derive that $\\beta$\".\n\nAnother sound rule is quite intuitive and is known as **And Elimination**:\n\n$$\n    \\frac{\\alpha \\land \\beta}{\\alpha}\n$$\n\nAs additional sound inference rules, we can use all of the known equivalences.\n\nThe last sound rule we will show is the **Resolution Rule**. This is an important rule that is used by a complete and sound inference algorithm known as the **Resultion Inference Algorithm** which we will see later.\n\nThe resolution rule states the following:\n\n$$\n    \\frac\n    {l_1 \\lor l_2 \\lor ... \\lor l_i \\lor ... \\lor l_n ~~,~~ A \\lor \\neg l_i}\n    {l_1 \\lor l_2 \\lor ... \\lor l_{i-1} \\lor l_{i+1} \\lor ... \\lor l_n \\lor A }\n$$\n\nWhere $A$ is a disjunction of other literals.\n\nFor example:\n\n$$\n    \\frac{P \\lor Q ~~,~~ \\neg P \\lor R}{Q \\lor R} \n$$\n\nNotice that this important rule operates only on disjunctions. However, our sentences can be in an arbitrary form. We find that every KB is equivalent to another KB that is in a very specific form known as \"conjunctive normal form\" - In this form, we can easily apply the resolution rule.\n\n## Conjunctive Normal Form (CNF)\n\nA **Clause** is a disjunction of literals. We say that a sentence that is expressed by a conjunction of clauses is in a **Conjunctive Normal Form**.\n\nWe already know how to transform a KB to a CNF - using the known equivalences!\n\n1. Eliminate bidirectional sentences\n2. Eliminate implications sentences\n3. Move negations inwards to literals mostly using double negation and de-morgan rules\n4. Apply distribution rules\n\nLet's see an example and convert the following sentence to CNF:\n\n$$\n    P \\Leftrightarrow Q \\lor R\n$$\n\n1. Bidirectional elimination: $(P \\Rightarrow Q \\lor R) \\land (Q \\lor R\\\n \\Rightarrow P)$\n2. Implication elimination of the left hand conjugate: $(\\neg P \\lor Q \\lor R) \\land (Q \\lor R \\Rightarrow P)$\n3. Implication elimination of the right hand conjugate: $(\\neg P \\lor Q \\lor R) \\land (\\neg (Q \\lor R) \\lor P)$\n4. Move negation inwards (apply de-morgan at the right-hand side): $(\\neg P \\lor Q \\lor R) \\land ((\\neg Q \\land \\neg R) \\lor P)$\n5. Distribute $\\lor$ over $\\land$ at the right-hand side: $(\\neg P \\lor Q \\lor R) \\land (\\neg Q \\lor P) \\land (\\neg R \\lor P)$\n\nAnd that's it, we have a sentence in CNF form.\n\n## The Resolution Algorithm\n\nThe **Resolution Algorithm** is sound and complete (will not show this here).\n\nGiven a KB $K$, we show that $K \\models \\alpha$ by proving that $K \\land \\neg \\alpha$ is a contradiction (proof by contradiction).\n\n1. We add $\\neg \\alpha$ to the KB\n2. Convert the KB to CNF form\n3. As long as we can apply the resolution rule, apply it\n    - If the derived sentence is non-empty, add it to the KB if it is doesn't exist there already\n    - Else, because this sentence is a contradiction (empty) we have reached a contradiction in the KB because of $\\neg \\alpha$ - Answer: the KB entails $\\alpha$\n4. No contradiction occurred - Answer: The KB does not entail $\\alpha$\n\n### Example\n\nAssume the initial KB \n\n- $P \\Leftrightarrow Q \\lor R$\n- $\\neg P$\n\nAnd let's say we want to prove that $\\neg Q$.\n\n**Add the negation of $\\neg Q$ to the KB**\n\n- $P \\Leftrightarrow Q \\lor R$\n- $\\neg P$\n- $Q$\n\n**Convert to CNF**\n\n1. $\\neg P \\lor Q \\lor R$\n2. $\\neg Q \\lor P$\n3. $\\neg R \\lor P$\n4. $\\neg P$\n5. $Q$\n\n**Repeatedly apply the Resolution Rule**\n\n6. (5, 2): $P$\n7. (6, 4): $\\phi$\n\nBecause we reached a contradiction, we deduce that the KB entails $\\neg Q$.\n\n# First Order Logic\n\nThink of it, how complex are the sentences that we could express via Propositional Logic? It turns out that we can formalize a logic language that is far more expressive than it.\n\n**First Order Logic** is a \n\n# Examples from Exercises\n\n## Propositional Logic\n\n## First Order Logic\n\n# Hebrew Appendix\n\n- Entailement - נביעה לוגית\n- Sound - נאות\n- Complete - שלם\n- Proposition - פסוק","html":"<!doctype html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"utf-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n  </head>\n  <body>\n    <h1 id=\"logic\">Logic</h1>\n    <p><strong>Logic</strong> is the field of knowledge representation and reasoning about this knowledge according to some <strong>Possible World</strong> also known as a <strong>Model</strong>. There are different types of logic formulizations such as <strong>Propositional Logic</strong> and <strong>First Order Logic</strong>, each of which can represent different information.</p>\n    <p>Formally, each logic defines its <strong>Syntax</strong>, which is the specification of how we express <strong>Sentences</strong>. In addition to syntax, the logic also defines its <strong>Semantics</strong> which specify the meaning of those sentences - it assigns a <strong>Truth Value</strong> to each sentence according to the model.</p>\n    <p>A set of sentences is known as a <strong>Knowledge Base</strong>. The set of sentences that are given to us and assumed true are known as <strong>Axioms</strong>.</p>\n    <p>If a given sentence $\\alpha$ is true in some model $m$ we say that <strong>$\\alpha$ is satisfied by the model $m$</strong>. We will denote $M(\\alpha)$ to be the set of all models that satisfy $\\alpha$.</p>\n    <p><strong>Reasoning</strong> is the process of understanding the entailment between sentences. Given the sentences $\\alpha$ and $\\beta$, we say that $\\alpha$ entails the sentence $\\beta$ if and only if $M(\\alpha) \\subseteq M(\\beta)$, meaning that every model $m$ that satisfies $\\alpha$ also satisfies $\\beta$. We denote entailment by $\\alpha \\models \\beta$.</p>\n    <h2 id=\"logical-inference\">Logical Inference</h2>\n    <p>Given a knowledge base $KB$, we want to derive new sentences - such a process is called <strong>Logical Inference</strong>. The algorithm we choose to derive new sentences can be arbitrary, but what's the point? We say that an inference algorithm is <strong>Sound</strong> if and only if it derives entailed sentences.</p>\n    <p>For example, consider the following axioms:</p>\n    <ul>\n      <li>Some dogs are black</li>\n      <li>Every dog is an animal</li>\n    </ul>\n    <p>Using some inference algorithm, we can infer the sentence: \"Every animal is black\" - but we know this is wrong, our algorithm is unsound.</p>\n    <p>Another algorithm can infer the sentence: \"Some animals are black\" which is sound.</p>\n    <p>Lastly, consider an inference algorithm that did not produce any sentence. If we desire to logically infer new sentences from the KB, such an algorithm is undesirable. This leads us to the second categorization of inference algorithms:</p>\n    <p>Inference algorithms that can derive any sentence that is entailed are called <strong>Complete</strong>.</p>\n    <p>Ideal inference algorithms are both complete and sound. Such algorithms can derive all entailed sentences, and all derived sentences are entailed.</p>\n    <h1 id=\"propositional-logic\">Propositional Logic</h1>\n    <p>The first logic we will explore is the <strong>Propositional Logic</strong>.</p>\n    <p>In this logic's syntax, each sentence is either an <strong>Atomic Sentence</strong> or a <strong>Complex Sentence</strong>.</p>\n    <p>An atomic sentence consists of a <strong>Proposition</strong> - a symbol that can take a boolean value.</p>\n    <p>A complex sentence is constructed of multiple sentences using <strong>Parentheses</strong> and <strong>Logical Operators</strong>.</p>\n    <p>Parentheses, as commonly used, specify the precedence of operations.</p>\n    <p>Logical operators operate on sentences and return a boolean value. The logical operators are:</p>\n    <ul>\n      <li>The <strong>negation (not)</strong> unary operator $\\neg$</li>\n      <li>The <strong>conjunction (and)</strong> binary operator $\\land$</li>\n      <li>The <strong>disjunction (or)</strong> binary operator $\\lor$</li>\n      <li>The <strong>implication (implies)</strong> binary operator $\\Rightarrow$</li>\n      <li>The <strong>biconditional (if and only if)</strong> binary operator $\\Leftrightarrow$</li>\n    </ul>\n    <p>They are (unsuprisingly) defined by the truth table:</p>\n    <div class=\"table-container\">\n      <table>\n        <thead>\n          <tr>\n            <th>$P$</th>\n            <th>$Q$</th>\n            <th>$\\neg P$</th>\n            <th>$P \\land Q$</th>\n            <th>$P \\lor Q$</th>\n            <th>$P \\Rightarrow Q$</th>\n            <th>$P \\Leftrightarrow A$</th>\n          </tr>\n        </thead>\n        <tbody>\n          <tr>\n            <td>F</td>\n            <td>F</td>\n            <td>T</td>\n            <td>F</td>\n            <td>F</td>\n            <td>T</td>\n            <td>T</td>\n          </tr>\n          <tr>\n            <td>F</td>\n            <td>T</td>\n            <td>T</td>\n            <td>F</td>\n            <td>T</td>\n            <td>T</td>\n            <td>F</td>\n          </tr>\n          <tr>\n            <td>T</td>\n            <td>F</td>\n            <td>F</td>\n            <td>F</td>\n            <td>T</td>\n            <td>F</td>\n            <td>F</td>\n          </tr>\n          <tr>\n            <td>T</td>\n            <td>T</td>\n            <td>F</td>\n            <td>T</td>\n            <td>T</td>\n            <td>T</td>\n            <td>T</td>\n          </tr>\n        </tbody>\n      </table>\n    </div>\n    <p>The following <strong>Logical Equivalences</strong> hold:</p>\n    <blockquote>\n      <p>Notation note: that we use $\\cdot$ and $+$ as placeholders for when equivalence holds both for $\\land$ and $\\lor$. i.e. read $\\alpha \\cdot \\beta$ as $\\alpha \\land \\beta$ and also $\\alpha \\lor \\beta$. When we use both $\\cdot$ and $+$, it means we can use both interchangebly.</p>\n    </blockquote>\n    <ul>\n      <li><strong>Commutativity</strong>: $\\alpha \\cdot \\beta \\equiv \\beta \\cdot \\alpha$</li>\n      <li><strong>Associativity</strong> operators, meaning that $((\\alpha \\cdot \\beta) \\cdot \\gamma) \\equiv (\\alpha \\cdot (\\beta \\cdot \\gamma))$</li>\n      <li><strong>Double Negation</strong>: $\\neg (\\neg \\alpha) \\equiv \\alpha$</li>\n      <li><strong>Contraposition</strong>: $\\alpha \\Rightarrow \\beta \\equiv \\neg \\beta \\Rightarrow \\neg \\alpha$\n        <ul>\n          <li>This equivalence is frequently used in mathematical proofs and is the basis of \"proof by contradiction\"</li>\n        </ul>\n      </li>\n      <li><strong>Implication Elimination</strong>: $\\alpha \\Rightarrow \\beta \\equiv \\neg \\alpha \\lor \\beta$</li>\n      <li><strong>Bidirectional Elimination</strong>: $\\alpha \\Leftrightarrow \\beta \\equiv (\\alpha \\Rightarrow \\beta) \\land (\\beta \\Rightarrow \\alpha))$</li>\n      <li><strong>Demorgan</strong>: $\\neg (\\alpha \\cdot \\beta) \\equiv \\neg \\alpha \\cdot \\neg \\beta$</li>\n      <li><strong>Distributivity</strong> $(\\alpha \\cdot (\\beta + \\gamma)) \\equiv (\\alpha \\cdot \\beta) + (\\alpha \\cdot \\gamma)$</li>\n    </ul>\n    <h2 id=\"inference\">Inference</h2>\n    <p>A commonly, sound rule that is used to derive sentences is known as <strong>Modus \\alphaonens</strong> rule:</p>\n    <p>\n      $$\n      \\frac{\\alpha \\Rightarrow \\beta, \\alpha}{\\beta}\n      $$\n    </p>\n    <p>Read it as: \"If we know that $\\alpha$ implies $\\beta$ and we also know that $\\alpha$, we derive that $\\beta$\".</p>\n    <p>Another sound rule is quite intuitive and is known as <strong>And Elimination</strong>:</p>\n    <p>\n      $$\n      \\frac{\\alpha \\land \\beta}{\\alpha}\n      $$\n    </p>\n    <p>As additional sound inference rules, we can use all of the known equivalences.</p>\n    <p>The last sound rule we will show is the <strong>Resolution Rule</strong>. This is an important rule that is used by a complete and sound inference algorithm known as the <strong>Resultion Inference Algorithm</strong> which we will see later.</p>\n    <p>The resolution rule states the following:</p>\n    <p>\n      $$\n      \\frac\n      {l_1 \\lor l_2 \\lor ... \\lor l_i \\lor ... \\lor l_n <del>,</del> A \\lor \\neg l_i}\n      {l_1 \\lor l_2 \\lor ... \\lor l_{i-1} \\lor l_{i+1} \\lor ... \\lor l_n \\lor A }\n      $$\n    </p>\n    <p>Where $A$ is a disjunction of other literals.</p>\n    <p>For example:</p>\n    <p>\n      $$\n      \\frac{P \\lor Q <del>,</del> \\neg P \\lor R}{Q \\lor R}\n      $$\n    </p>\n    <p>Notice that this important rule operates only on disjunctions. However, our sentences can be in an arbitrary form. We find that every KB is equivalent to another KB that is in a very specific form known as \"conjunctive normal form\" - In this form, we can easily apply the resolution rule.</p>\n    <h2 id=\"conjunctive-normal-form-cnf\">Conjunctive Normal Form (CNF)</h2>\n    <p>A <strong>Clause</strong> is a disjunction of literals. We say that a sentence that is expressed by a conjunction of clauses is in a <strong>Conjunctive Normal Form</strong>.</p>\n    <p>We already know how to transform a KB to a CNF - using the known equivalences!</p>\n    <ol>\n      <li>Eliminate bidirectional sentences</li>\n      <li>Eliminate implications sentences</li>\n      <li>Move negations inwards to literals mostly using double negation and de-morgan rules</li>\n      <li>Apply distribution rules</li>\n    </ol>\n    <p>Let's see an example and convert the following sentence to CNF:</p>\n    <p>\n      $$\n      P \\Leftrightarrow Q \\lor R\n      $$\n    </p>\n    <ol>\n      <li>Bidirectional elimination: $(P \\Rightarrow Q \\lor R) \\land (Q \\lor R<br>\\Rightarrow P)$</li>\n      <li>Implication elimination of the left hand conjugate: $(\\neg P \\lor Q \\lor R) \\land (Q \\lor R \\Rightarrow P)$</li>\n      <li>Implication elimination of the right hand conjugate: $(\\neg P \\lor Q \\lor R) \\land (\\neg (Q \\lor R) \\lor P)$</li>\n      <li>Move negation inwards (apply de-morgan at the right-hand side): $(\\neg P \\lor Q \\lor R) \\land ((\\neg Q \\land \\neg R) \\lor P)$</li>\n      <li>Distribute $\\lor$ over $\\land$ at the right-hand side: $(\\neg P \\lor Q \\lor R) \\land (\\neg Q \\lor P) \\land (\\neg R \\lor P)$</li>\n    </ol>\n    <p>And that's it, we have a sentence in CNF form.</p>\n    <h2 id=\"the-resolution-algorithm\">The Resolution Algorithm</h2>\n    <p>The <strong>Resolution Algorithm</strong> is sound and complete (will not show this here).</p>\n    <p>Given a KB $K$, we show that $K \\models \\alpha$ by proving that $K \\land \\neg \\alpha$ is a contradiction (proof by contradiction).</p>\n    <ol>\n      <li>We add $\\neg \\alpha$ to the KB</li>\n      <li>Convert the KB to CNF form</li>\n      <li>As long as we can apply the resolution rule, apply it\n        <ul>\n          <li>If the derived sentence is non-empty, add it to the KB if it is doesn't exist there already</li>\n          <li>Else, because this sentence is a contradiction (empty) we have reached a contradiction in the KB because of $\\neg \\alpha$ - Answer: the KB entails $\\alpha$</li>\n        </ul>\n      </li>\n      <li>No contradiction occurred - Answer: The KB does not entail $\\alpha$</li>\n    </ol>\n    <h3 id=\"example\">Example</h3>\n    <p>Assume the initial KB</p>\n    <ul>\n      <li>$P \\Leftrightarrow Q \\lor R$</li>\n      <li>$\\neg P$</li>\n    </ul>\n    <p>And let's say we want to prove that $\\neg Q$.</p>\n    <p><strong>Add the negation of $\\neg Q$ to the KB</strong></p>\n    <ul>\n      <li>$P \\Leftrightarrow Q \\lor R$</li>\n      <li>$\\neg P$</li>\n      <li>$Q$</li>\n    </ul>\n    <p><strong>Convert to CNF</strong></p>\n    <ol>\n      <li>$\\neg P \\lor Q \\lor R$</li>\n      <li>$\\neg Q \\lor P$</li>\n      <li>$\\neg R \\lor P$</li>\n      <li>$\\neg P$</li>\n      <li>$Q$</li>\n    </ol>\n    <p><strong>Repeatedly apply the Resolution Rule</strong></p>\n    <ol start=\"6\">\n      <li>(5, 2): $P$</li>\n      <li>(6, 4): $\\phi$</li>\n    </ol>\n    <p>Because we reached a contradiction, we deduce that the KB entails $\\neg Q$.</p>\n    <h1 id=\"first-order-logic\">First Order Logic</h1>\n    <p>Think of it, how complex are the sentences that we could express via Propositional Logic? It turns out that we can formalize a logic language that is far more expressive than it.</p>\n    <p><strong>First Order Logic</strong> is a</p>\n    <h1 id=\"examples-from-exercises\">Examples from Exercises</h1>\n    <h2 id=\"propositional-logic-1\">Propositional Logic</h2>\n    <h2 id=\"first-order-logic-1\">First Order Logic</h2>\n    <h1 id=\"hebrew-appendix\">Hebrew Appendix</h1>\n    <ul>\n      <li>Entailement - נביעה לוגית</li>\n      <li>Sound - נאות</li>\n      <li>Complete - שלם</li>\n      <li>Proposition - פסוק</li>\n    </ul>\n  </body>\n</html>\n"}}},"__N_SSG":true}