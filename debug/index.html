<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="./_next/static/css/8336ab73ef3a5f12.css" as="style"/><link rel="stylesheet" href="./_next/static/css/8336ab73ef3a5f12.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="./_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="./_next/static/chunks/webpack-e50e9853db18b759.js" defer=""></script><script src="./_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="./_next/static/chunks/main-97927b8cd08b2c1f.js" defer=""></script><script src="./_next/static/chunks/pages/_app-d80a98ef9a52cbba.js" defer=""></script><script src="./_next/static/chunks/pages/debug-1f1b02972aabbd0c.js" defer=""></script><script src="./_next/static/yGlP4Z_vvfL6c7wzC5t24/_buildManifest.js" defer=""></script><script src="./_next/static/yGlP4Z_vvfL6c7wzC5t24/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="Layout_container__K9hpm"><nav class="p-6 border border-r-2"><div class="flex flex-col items-center"><img src="/logo.png" width="100" height="100"/><span class="text-3xl font-semibold">Tal Glanzman</span><span class="text-lg">v0</span></div><div class="mt-6"><ul><li><a href="/">Home</a></li><details><summary>Advanced CS</summary><ul class="ml-4"><li><a href="/_articles/advanced-cs/linear-programming/">Linear Programming</a></li><li><a href="/_articles/advanced-cs/expanders/">Expanders</a></li></ul></details><li><a href="/_articles/about/">About</a></li></ul></div></nav><div><h1>Debug</h1><pre>{
    &quot;articles&quot;: [
        {
            &quot;id&quot;: &quot;about&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/about.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: null,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [],
                &quot;categories&quot;: []
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;# About\n\nBla bla\n\nAdditional Blabla&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;About&lt;/h1&gt;\n    &lt;p&gt;Bla bla&lt;/p&gt;\n    &lt;p&gt;Additional Blabla&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;algorithms/bloom&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/algorithms/bloom.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Bloom Filter&quot;,
                &quot;description&quot;: &quot;Illustrate the Bloomfilter data structure&quot;,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [],
                &quot;categories&quot;: [
                    &quot;Algorithms&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\n## Summary\n\nA _Bloom Filter_ is an data structure that given a set of elements $S$ and a subset $X \\subset S$, answers the question whether some element $s \\in S$ is in $X$.\n\nThe main difference between this and standard data structures using sets and lists is in the fact that this is a probabilistic data structure. Specifically, it can return False Positives but never False Negatives.\n\n|           | Positive      | Negative\n|-----------|---------------|---------\n| **True**  | Always        | Always\n| **False** | Probabilistic | Never\n\nA standard implementation of _Bloom Filters_ support adding elements to $X$ but does not support removal requiring the application to reconstruct the data structure accordingly.\n\n_Bloom Filter_ provides a more efficient memory usage than most other constructs such as lists and sets in the cost of a chance for False Positives.\n\nTo illustrate, consider the following interface\n\n```c#\ninterface BloomFilter&lt;S&gt; {\n    // Add {element} to the container\n    void add(S element);\n\n    // Determines whether {element} is in the container\n    bool contains(S element);\n}\n```\n\n## Implementation\n\n- Set __A__ to be an $m$ bits bit array.\n- Set __H__ to be a set of $k$ functions mapping $S$ onto $\\\\{1, 2, ..., m \\\\}$\n\n__add(x)__\n\nApply all hash functions of $H$ on $x$ and set the corresponding bit in $A$ to 1.\n\n- $\\forall h \\in H$  \n  - $A[h(x)] \\leftarrow 1$\n\n__contains(x)__\n\nApply all hash functions of $H$ on $x$ and return true if and only if all of the corresponding bits in $A$ are set to 1.\n\n- $\\forall h \\in H$\n  - $A[h(x)] = 1 \\Rightarrow True$\n\nNow we can easily understand where the False Positives comes from.\n\n## False Positives\n\nAssume \n\n- $S = \\\\{x_1, x_2, x_3\\\\}$\n- $m=5$\n- $k=2$\n\nWith the hash functions  \n\n- $h_1(x_1)=1 ~~ h_1(x_2)=3 ~~ h_1(x_3)=3$\n- $h_2(x_1)=4 ~~ h_2(x_2)=1 ~~ h_2(x_3)=4$\n\nAnd the scenario of $X = \\\\{x_1, x_2\\\\}$ as shown below\n\n| 1 | 2 | 3 | 4 | 5 |\n|:-:|:-:|:-:|:-:|:-:|\n|$x_1, x_2$||$x_2$|$x_1$||\n\nApplying __contains($x_3$)__ will yield a False Positive.\n\nWhat was the chance of that happening?\n\nAssuming equal distribution of the hash functions, the probability that a specific bit is unset after applying $k$ functions on an element is $(1 - \\frac {1}{m})^k \\approx e^{-\\frac{k}{m}}$ for large $m$.\n\nNow we can conclude that after the addition of $n$ elements (using __add(x)__) to the data structure the probability for a specific bit to be unset is $e^{-\\frac{kn}{m}}$ and for it to be set is $1 - e^{-\\frac{kn}{m}}$.\n\nFor the containment check to return true it is enough for $k$ specific bits to be set. From the above, the probability for this is $(1 - e^{-\\frac{kn}{m}})^k$.\n\n__To summarize, the probability for a _False Positive_ is $(1 - e^{-\\frac{kn}{m}})^k$.__\n\n## Picking the hash functions\n\n[This paper](https://www.eecs.harvard.edu/~michaelm/postscripts/tr-02-05.pdf) describes a technique of constructing a set of $k$ hash functions based upon exactly 2 hash functions $h_1$ and $h_2$.\n\nThe usefulness of this approach relies in the fact that not all scenarios require the same number of hash functions $k$. Thus, allowing us to provide only 2 base functions $h_1$ and $h_2$ and construct a set of hash functions for any arbitrary $k$.\n\nThere they describe a partitioning of the bit array, each partition of a primary size $p$, such that each hash function maps to $\\\\{ 1, 2, ..., p \\\\}$ within it&#x27;s own unique partition. (Note that this is just a restatement of the original view).\n\nNow, forall $i$ we can have\n$$\n    g_i(x) = h_1(x) + ih_2(x) \\mod p\n$$\n\n&gt; The paper discusses a lot more and more in-depth&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h2&gt;Summary&lt;/h2&gt;\n    &lt;p&gt;A &lt;em&gt;Bloom Filter&lt;/em&gt; is an data structure that given a set of elements $S$ and a subset $X \\subset S$, answers the question whether some element $s \\in S$ is in $X$.&lt;/p&gt;\n    &lt;p&gt;The main difference between this and standard data structures using sets and lists is in the fact that this is a probabilistic data structure. Specifically, it can return False Positives but never False Negatives.&lt;/p&gt;\n    &lt;table&gt;\n      &lt;thead&gt;\n        &lt;tr&gt;\n          &lt;th&gt;&lt;/th&gt;\n          &lt;th&gt;Positive&lt;/th&gt;\n          &lt;th&gt;Negative&lt;/th&gt;\n        &lt;/tr&gt;\n      &lt;/thead&gt;\n      &lt;tbody&gt;\n        &lt;tr&gt;\n          &lt;td&gt;&lt;strong&gt;True&lt;/strong&gt;&lt;/td&gt;\n          &lt;td&gt;Always&lt;/td&gt;\n          &lt;td&gt;Always&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td&gt;&lt;strong&gt;False&lt;/strong&gt;&lt;/td&gt;\n          &lt;td&gt;Probabilistic&lt;/td&gt;\n          &lt;td&gt;Never&lt;/td&gt;\n        &lt;/tr&gt;\n      &lt;/tbody&gt;\n    &lt;/table&gt;\n    &lt;p&gt;A standard implementation of &lt;em&gt;Bloom Filters&lt;/em&gt; support adding elements to $X$ but does not support removal requiring the application to reconstruct the data structure accordingly.&lt;/p&gt;\n    &lt;p&gt;&lt;em&gt;Bloom Filter&lt;/em&gt; provides a more efficient memory usage than most other constructs such as lists and sets in the cost of a chance for False Positives.&lt;/p&gt;\n    &lt;p&gt;To illustrate, consider the following interface&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-c#\&quot;&gt;&lt;span class=\&quot;hljs-keyword\&quot;&gt;interface&lt;/span&gt; &lt;span class=\&quot;hljs-title\&quot;&gt;BloomFilter&lt;/span&gt;&amp;#x3C;&lt;span class=\&quot;hljs-title\&quot;&gt;S&lt;/span&gt;&gt; {\n    &lt;span class=\&quot;hljs-comment\&quot;&gt;// Add {element} to the container&lt;/span&gt;\n    &lt;span class=\&quot;hljs-function\&quot;&gt;&lt;span class=\&quot;hljs-keyword\&quot;&gt;void&lt;/span&gt; &lt;span class=\&quot;hljs-title\&quot;&gt;add&lt;/span&gt;(&lt;span class=\&quot;hljs-params\&quot;&gt;S element&lt;/span&gt;)&lt;/span&gt;;\n\n    &lt;span class=\&quot;hljs-comment\&quot;&gt;// Determines whether {element} is in the container&lt;/span&gt;\n    &lt;span class=\&quot;hljs-function\&quot;&gt;&lt;span class=\&quot;hljs-built_in\&quot;&gt;bool&lt;/span&gt; &lt;span class=\&quot;hljs-title\&quot;&gt;contains&lt;/span&gt;(&lt;span class=\&quot;hljs-params\&quot;&gt;S element&lt;/span&gt;)&lt;/span&gt;;\n}\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;h2&gt;Implementation&lt;/h2&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Set &lt;strong&gt;A&lt;/strong&gt; to be an $m$ bits bit array.&lt;/li&gt;\n      &lt;li&gt;Set &lt;strong&gt;H&lt;/strong&gt; to be a set of $k$ functions mapping $S$ onto $\\{1, 2, ..., m \\}$&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;&lt;strong&gt;add(x)&lt;/strong&gt;&lt;/p&gt;\n    &lt;p&gt;Apply all hash functions of $H$ on $x$ and set the corresponding bit in $A$ to 1.&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;$\\forall h \\in H$\n        &lt;ul&gt;\n          &lt;li&gt;$A[h(x)] \\leftarrow 1$&lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;&lt;strong&gt;contains(x)&lt;/strong&gt;&lt;/p&gt;\n    &lt;p&gt;Apply all hash functions of $H$ on $x$ and return true if and only if all of the corresponding bits in $A$ are set to 1.&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;$\\forall h \\in H$\n        &lt;ul&gt;\n          &lt;li&gt;$A[h(x)] = 1 \\Rightarrow True$&lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;Now we can easily understand where the False Positives comes from.&lt;/p&gt;\n    &lt;h2&gt;False Positives&lt;/h2&gt;\n    &lt;p&gt;Assume&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;$S = \\{x_1, x_2, x_3\\}$&lt;/li&gt;\n      &lt;li&gt;$m=5$&lt;/li&gt;\n      &lt;li&gt;$k=2$&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;With the hash functions&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;$h_1(x_1)=1 ~~ h_1(x_2)=3 ~~ h_1(x_3)=3$&lt;/li&gt;\n      &lt;li&gt;$h_2(x_1)=4 ~~ h_2(x_2)=1 ~~ h_2(x_3)=4$&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;And the scenario of $X = \\{x_1, x_2\\}$ as shown below&lt;/p&gt;\n    &lt;table&gt;\n      &lt;thead&gt;\n        &lt;tr&gt;\n          &lt;th align=\&quot;center\&quot;&gt;1&lt;/th&gt;\n          &lt;th align=\&quot;center\&quot;&gt;2&lt;/th&gt;\n          &lt;th align=\&quot;center\&quot;&gt;3&lt;/th&gt;\n          &lt;th align=\&quot;center\&quot;&gt;4&lt;/th&gt;\n          &lt;th align=\&quot;center\&quot;&gt;5&lt;/th&gt;\n        &lt;/tr&gt;\n      &lt;/thead&gt;\n      &lt;tbody&gt;\n        &lt;tr&gt;\n          &lt;td align=\&quot;center\&quot;&gt;$x_1, x_2$&lt;/td&gt;\n          &lt;td align=\&quot;center\&quot;&gt;&lt;/td&gt;\n          &lt;td align=\&quot;center\&quot;&gt;$x_2$&lt;/td&gt;\n          &lt;td align=\&quot;center\&quot;&gt;$x_1$&lt;/td&gt;\n          &lt;td align=\&quot;center\&quot;&gt;&lt;/td&gt;\n        &lt;/tr&gt;\n      &lt;/tbody&gt;\n    &lt;/table&gt;\n    &lt;p&gt;Applying &lt;strong&gt;contains($x_3$)&lt;/strong&gt; will yield a False Positive.&lt;/p&gt;\n    &lt;p&gt;What was the chance of that happening?&lt;/p&gt;\n    &lt;p&gt;Assuming equal distribution of the hash functions, the probability that a specific bit is unset after applying $k$ functions on an element is $(1 - \\frac {1}{m})^k \\approx e^{-\\frac{k}{m}}$ for large $m$.&lt;/p&gt;\n    &lt;p&gt;Now we can conclude that after the addition of $n$ elements (using &lt;strong&gt;add(x)&lt;/strong&gt;) to the data structure the probability for a specific bit to be unset is $e^{-\\frac{kn}{m}}$ and for it to be set is $1 - e^{-\\frac{kn}{m}}$.&lt;/p&gt;\n    &lt;p&gt;For the containment check to return true it is enough for $k$ specific bits to be set. From the above, the probability for this is $(1 - e^{-\\frac{kn}{m}})^k$.&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;To summarize, the probability for a &lt;em&gt;False Positive&lt;/em&gt; is $(1 - e^{-\\frac{kn}{m}})^k$.&lt;/strong&gt;&lt;/p&gt;\n    &lt;h2&gt;Picking the hash functions&lt;/h2&gt;\n    &lt;p&gt;&lt;a href=\&quot;https://www.eecs.harvard.edu/~michaelm/postscripts/tr-02-05.pdf\&quot;&gt;This paper&lt;/a&gt; describes a technique of constructing a set of $k$ hash functions based upon exactly 2 hash functions $h_1$ and $h_2$.&lt;/p&gt;\n    &lt;p&gt;The usefulness of this approach relies in the fact that not all scenarios require the same number of hash functions $k$. Thus, allowing us to provide only 2 base functions $h_1$ and $h_2$ and construct a set of hash functions for any arbitrary $k$.&lt;/p&gt;\n    &lt;p&gt;There they describe a partitioning of the bit array, each partition of a primary size $p$, such that each hash function maps to $\\{ 1, 2, ..., p \\}$ within it&#x27;s own unique partition. (Note that this is just a restatement of the original view).&lt;/p&gt;\n    &lt;p&gt;\n      Now, forall $i$ we can have\n      $$\n      g_i(x) = h_1(x) + ih_2(x) \\mod p\n      $$\n    &lt;/p&gt;\n    &lt;blockquote&gt;\n      &lt;p&gt;The paper discusses a lot more and more in-depth&lt;/p&gt;\n    &lt;/blockquote&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;algorithms/dynamic-programming/longest-increasing-subsequence&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/algorithms/dynamic-programming/longest-increasing-subsequence.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Longest increasing subsequence&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [
                    &quot;Dynamic Programming&quot;
                ],
                &quot;categories&quot;: [
                    &quot;Algorithms&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\n\n**The problem**\n\nGiven a sequence of numbers $N$, find the length of longest subsequence of numbers that is increasing\n\n**Illustration**\n\n- [1, 2, 3, 4] $\\rightarrow$ [1, 2, 3, 4]\n- [1, 4, 2, 6, 3, 7] $\\rightarrow$ [1, 4, 6, 7]\n\n**Solution**\n\nlet $(a_i)_{i=1}^n = N$.\n\n$a_i$ can contribute to a subsequence of previous elements only if it is greater than all those elements.\n\nWe can also realize the the longest increasing subsequence that ends with $a_i$ is the previous longest subsequence containing elements less that $a_i$, adding $a_i$ to it.\n\nWe shall define opt(i) to indicate the length of the longest increasing subsequence that ends with $a_i$.\n\nTrivially,\n$$\nopt(0) = 0\n$$\n\nRecursively,\n$$\n\\forall i &gt; 0; ~ opt(i) = 1 + \\max \\\\{ \\\\{0\\\\} \\cup \\\\{ opt(j) | j &lt; i \\land a_j &lt; a_i \\\\} \\\\}\n$$\n\n**Psuedo**\n\n- Initialize opt\n  - $opt \\leftarrow array(n+1)$\n  - $opt[0] \\leftarrow 0$\n\n- Build opt in a bottom up fashion\n  - $for ~ i = 1, 2, ... n$\n    - $opt[i] = 1 + \\max_{j &lt; i \\land a_j &lt; a_i}{opt[j]}$\n- Return the final answer\n  - $ans \\leftarrow 0$\n  - $for ~ i = 1, 2, ... n$\n    - $ans \\leftarrow max \\\\{ ans, opt[i] \\\\}$\n  - Return $ans$\n\n\n**Time Complexity**\n\n- Initialization of opt is $O(1)$ (single access to opt)\n  - We exclude the actual array creation\n- Building opt takes $O(n)$ iterations\n  - Each iteration takes $O(n)$ accesses at the worst case\n\nSo in total, the time complexity is $O(n^2)$\n\n**Finding the subsequence using a Journal**\n\nWe will keep another data structure $S$ that will act as the journal.\n\nFor each $i$, $S$ will contain the index $j$ that contains the previous element in the subsequence.\n\nWe can fill it during the algorithm by modifying \&quot;Build opt in a bottom up fashion\&quot; to be\n\n- Build opt in a bottom up fashion\n  - $for ~ i = 1, 2, ... n$\n    - $opt[i] = 1 + \\max_{j &lt; i \\land a_j &lt; a_i}{opt[j]}$\n    - $S[i] = $ the $j$ that achieved the max\n\nFinally, to print we shall go back from the position of the result using the indices at the journal. \n\n**Finding the subsequence using Traceback**\n\nLets review an example.\n\nAssume $N=[3, 4, 2, 7, 5]$\n\nThe final tabulation of opt will be\n\ni|0|1|2|3|4|5\n-|-|-|-|-|-|-\n$a_i$| |3|4|2|7|5\nopt(i)|0|1|2|1|3|2\n\nHere, we need to find $7 \\leftarrow 4 \\leftarrow 3$. Specifically\n- $opt(\&quot;7\&quot;) = 3, opt(\&quot;4\&quot;) = 2, opt(\&quot;3\&quot;) = 1$\n\nGenerally, to traceback, we shall find the index i that contains the maximum of opt as we did finding the answer. We notice that the previous element in the relevant subsequence is ending a subsequence of length $opt(i) - 1$. So we traverse back in the opt array until we find an element $a_j$ such that $a_j &lt; a_i$ and $opt(j) = opt(i) - 1$. This $j$ is the index of the previous element in the answer. We will keep going in this manner until index 0. \n\n**Actual Code**\n\n{{&lt;codepen RwVdYyO&gt;}}&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;&lt;/p&gt;\n    &lt;p&gt;Given a sequence of numbers $N$, find the length of longest subsequence of numbers that is increasing&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Illustration&lt;/strong&gt;&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;[1, 2, 3, 4] $\\rightarrow$ [1, 2, 3, 4]&lt;/li&gt;\n      &lt;li&gt;[1, 4, 2, 6, 3, 7] $\\rightarrow$ [1, 4, 6, 7]&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;/p&gt;\n    &lt;p&gt;let $(a_i)_{i=1}^n = N$.&lt;/p&gt;\n    &lt;p&gt;$a_i$ can contribute to a subsequence of previous elements only if it is greater than all those elements.&lt;/p&gt;\n    &lt;p&gt;We can also realize the the longest increasing subsequence that ends with $a_i$ is the previous longest subsequence containing elements less that $a_i$, adding $a_i$ to it.&lt;/p&gt;\n    &lt;p&gt;We shall define opt(i) to indicate the length of the longest increasing subsequence that ends with $a_i$.&lt;/p&gt;\n    &lt;p&gt;\n      Trivially,\n      $$\n      opt(0) = 0\n      $$\n    &lt;/p&gt;\n    &lt;p&gt;\n      Recursively,\n      $$\n      \\forall i &gt; 0; ~ opt(i) = 1 + \\max \\{ \\{0\\} \\cup \\{ opt(j) | j &amp;#x3C; i \\land a_j &amp;#x3C; a_i \\} \\}\n      $$\n    &lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Psuedo&lt;/strong&gt;&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;\n        &lt;p&gt;Initialize opt&lt;/p&gt;\n        &lt;ul&gt;\n          &lt;li&gt;$opt \\leftarrow array(n+1)$&lt;/li&gt;\n          &lt;li&gt;$opt[0] \\leftarrow 0$&lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n      &lt;li&gt;\n        &lt;p&gt;Build opt in a bottom up fashion&lt;/p&gt;\n        &lt;ul&gt;\n          &lt;li&gt;$for ~ i = 1, 2, ... n$\n            &lt;ul&gt;\n              &lt;li&gt;$opt[i] = 1 + \\max_{j &amp;#x3C; i \\land a_j &amp;#x3C; a_i}{opt[j]}$&lt;/li&gt;\n            &lt;/ul&gt;\n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n      &lt;li&gt;\n        &lt;p&gt;Return the final answer&lt;/p&gt;\n        &lt;ul&gt;\n          &lt;li&gt;$ans \\leftarrow 0$&lt;/li&gt;\n          &lt;li&gt;$for ~ i = 1, 2, ... n$\n            &lt;ul&gt;\n              &lt;li&gt;$ans \\leftarrow max \\{ ans, opt[i] \\}$&lt;/li&gt;\n            &lt;/ul&gt;\n          &lt;/li&gt;\n          &lt;li&gt;Return $ans$&lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;&lt;strong&gt;Time Complexity&lt;/strong&gt;&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Initialization of opt is $O(1)$ (single access to opt)\n        &lt;ul&gt;\n          &lt;li&gt;We exclude the actual array creation&lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n      &lt;li&gt;Building opt takes $O(n)$ iterations\n        &lt;ul&gt;\n          &lt;li&gt;Each iteration takes $O(n)$ accesses at the worst case&lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;So in total, the time complexity is $O(n^2)$&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Finding the subsequence using a Journal&lt;/strong&gt;&lt;/p&gt;\n    &lt;p&gt;We will keep another data structure $S$ that will act as the journal.&lt;/p&gt;\n    &lt;p&gt;For each $i$, $S$ will contain the index $j$ that contains the previous element in the subsequence.&lt;/p&gt;\n    &lt;p&gt;We can fill it during the algorithm by modifying \&quot;Build opt in a bottom up fashion\&quot; to be&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Build opt in a bottom up fashion\n        &lt;ul&gt;\n          &lt;li&gt;$for ~ i = 1, 2, ... n$\n            &lt;ul&gt;\n              &lt;li&gt;$opt[i] = 1 + \\max_{j &amp;#x3C; i \\land a_j &amp;#x3C; a_i}{opt[j]}$&lt;/li&gt;\n              &lt;li&gt;$S[i] = $ the $j$ that achieved the max&lt;/li&gt;\n            &lt;/ul&gt;\n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;Finally, to print we shall go back from the position of the result using the indices at the journal.&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Finding the subsequence using Traceback&lt;/strong&gt;&lt;/p&gt;\n    &lt;p&gt;Lets review an example.&lt;/p&gt;\n    &lt;p&gt;Assume $N=[3, 4, 2, 7, 5]$&lt;/p&gt;\n    &lt;p&gt;The final tabulation of opt will be&lt;/p&gt;\n    &lt;table&gt;\n      &lt;thead&gt;\n        &lt;tr&gt;\n          &lt;th&gt;i&lt;/th&gt;\n          &lt;th&gt;0&lt;/th&gt;\n          &lt;th&gt;1&lt;/th&gt;\n          &lt;th&gt;2&lt;/th&gt;\n          &lt;th&gt;3&lt;/th&gt;\n          &lt;th&gt;4&lt;/th&gt;\n          &lt;th&gt;5&lt;/th&gt;\n        &lt;/tr&gt;\n      &lt;/thead&gt;\n      &lt;tbody&gt;\n        &lt;tr&gt;\n          &lt;td&gt;$a_i$&lt;/td&gt;\n          &lt;td&gt;&lt;/td&gt;\n          &lt;td&gt;3&lt;/td&gt;\n          &lt;td&gt;4&lt;/td&gt;\n          &lt;td&gt;2&lt;/td&gt;\n          &lt;td&gt;7&lt;/td&gt;\n          &lt;td&gt;5&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td&gt;opt(i)&lt;/td&gt;\n          &lt;td&gt;0&lt;/td&gt;\n          &lt;td&gt;1&lt;/td&gt;\n          &lt;td&gt;2&lt;/td&gt;\n          &lt;td&gt;1&lt;/td&gt;\n          &lt;td&gt;3&lt;/td&gt;\n          &lt;td&gt;2&lt;/td&gt;\n        &lt;/tr&gt;\n      &lt;/tbody&gt;\n    &lt;/table&gt;\n    &lt;p&gt;Here, we need to find $7 \\leftarrow 4 \\leftarrow 3$. Specifically&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;$opt(\&quot;7\&quot;) = 3, opt(\&quot;4\&quot;) = 2, opt(\&quot;3\&quot;) = 1$&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;Generally, to traceback, we shall find the index i that contains the maximum of opt as we did finding the answer. We notice that the previous element in the relevant subsequence is ending a subsequence of length $opt(i) - 1$. So we traverse back in the opt array until we find an element $a_j$ such that $a_j &amp;#x3C; a_i$ and $opt(j) = opt(i) - 1$. This $j$ is the index of the previous element in the answer. We will keep going in this manner until index 0.&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Actual Code&lt;/strong&gt;&lt;/p&gt;\n    &lt;p&gt;{{}}&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;algorithms/dynamic-programming/longest-path-in-ordered-graph&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/algorithms/dynamic-programming/longest-path-in-ordered-graph.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Longest Path in Ordered Graph&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [
                    &quot;Dynamic Programming&quot;
                ],
                &quot;categories&quot;: [
                    &quot;Algorithms&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\nA directed graph $G=(V, E)$ is **ordererd**  \n\nif\n$$\n  \\forall (v_i, v_j) \\in E \\Rightarrow i &lt; j\n$$\n\nand\n$$\n  \\forall v_i \\in V / \\\\{ v_n \\\\} ~;~ \\exists j&gt;i, e=(v_i, v_j) \\in E\n$$\n\n**The problem**\n\nGiven such graph, find the length of the longest path from $v_1$ to $v_n$ where $n = |V|$.\n\n**Illustration**\n\n{{&lt; mermaid &gt;}}\n  graph LR\n    v1 --&gt; v2\n    v3 --&gt; v4\n    v4 --&gt; v5\n    v1 --&gt; v4\n    v2 --&gt; v4\n    v2 --&gt; v5\n{{&lt;/ mermaid &gt;}}\n\nFor this example, the longest path is $(v_1, v_2) \\rightarrow (v_2, v_4) \\rightarrow (v_4, v_5)$.\n\n**Solution**\n\nWe shall define $opt(i)$ to indicate the length of the longest path from $v_1$ to $v_i$ by\n\n$$\n  opt(0) = 0\n$$\n\n$$\n  opt(i)_{1 &gt; 0} = 1 + \\max \\\\{ opt(k) | (v_k, v_i) \\in E \\\\}\n$$\n\nThe answer we are looking for is given by $opt(n)$.\n\nThe intuition here is breaking the problem into smaller subproblems. The most straight forward way of doing so is by starting from $v_1$ and incrementally adding more vertices into consideration.\n\n**Psuedo**\n\nAs always, pseudo is very important in dynamic programming algorithms in order to illustrate pattenr of building the data structure (bottom up).\n\n- Initialize opt\n  - $opt \\leftarrow array(n)$\n  - $opt[0] \\leftarrow 0$\n  - $\\forall i \\in \\\\{ 1, 2, ..., n \\\\}$\n    - $opt[i] \\leftarrow nil$\n- Build opt in a bottom up fashion\n  - $for ~ i \\leftarrow 1 ~ to ~ n$\n    - $opt[i] \\leftarrow 1 + \\max \\\\{ opt(k) | (v_k, v_i) \\in E \\\\}$\n- Return $opt[n]$\n\n**Time Complexity**\n\n- Initialization of opt is $O(n)$\n- Building opt takes $O(n)$ iterations\n  - Each iteration takes $O(n)$ accesses to opt\n\nSo in total, the time complexity is $O(n^2)$&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;p&gt;A directed graph $G=(V, E)$ is &lt;strong&gt;ordererd&lt;/strong&gt;&lt;/p&gt;\n    &lt;p&gt;\n      if\n      $$\n      \\forall (v_i, v_j) \\in E \\Rightarrow i &amp;#x3C; j\n      $$\n    &lt;/p&gt;\n    &lt;p&gt;\n      and\n      $$\n      \\forall v_i \\in V / \\{ v_n \\} &lt;del&gt;;&lt;/del&gt; \\exists j&gt;i, e=(v_i, v_j) \\in E\n      $$\n    &lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;&lt;/p&gt;\n    &lt;p&gt;Given such graph, find the length of the longest path from $v_1$ to $v_n$ where $n = |V|$.&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Illustration&lt;/strong&gt;&lt;/p&gt;\n    &lt;p&gt;\n      {{&amp;#x3C; mermaid &gt;}}\n      graph LR\n      v1 --&gt; v2\n      v3 --&gt; v4\n      v4 --&gt; v5\n      v1 --&gt; v4\n      v2 --&gt; v4\n      v2 --&gt; v5\n      {{&amp;#x3C;/ mermaid &gt;}}\n    &lt;/p&gt;\n    &lt;p&gt;For this example, the longest path is $(v_1, v_2) \\rightarrow (v_2, v_4) \\rightarrow (v_4, v_5)$.&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;/p&gt;\n    &lt;p&gt;We shall define $opt(i)$ to indicate the length of the longest path from $v_1$ to $v_i$ by&lt;/p&gt;\n    &lt;p&gt;\n      $$\n      opt(0) = 0\n      $$\n    &lt;/p&gt;\n    &lt;p&gt;\n      $$\n      opt(i)_{1 &gt; 0} = 1 + \\max \\{ opt(k) | (v_k, v_i) \\in E \\}\n      $$\n    &lt;/p&gt;\n    &lt;p&gt;The answer we are looking for is given by $opt(n)$.&lt;/p&gt;\n    &lt;p&gt;The intuition here is breaking the problem into smaller subproblems. The most straight forward way of doing so is by starting from $v_1$ and incrementally adding more vertices into consideration.&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Psuedo&lt;/strong&gt;&lt;/p&gt;\n    &lt;p&gt;As always, pseudo is very important in dynamic programming algorithms in order to illustrate pattenr of building the data structure (bottom up).&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Initialize opt\n        &lt;ul&gt;\n          &lt;li&gt;$opt \\leftarrow array(n)$&lt;/li&gt;\n          &lt;li&gt;$opt[0] \\leftarrow 0$&lt;/li&gt;\n          &lt;li&gt;$\\forall i \\in \\{ 1, 2, ..., n \\}$\n            &lt;ul&gt;\n              &lt;li&gt;$opt[i] \\leftarrow nil$&lt;/li&gt;\n            &lt;/ul&gt;\n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n      &lt;li&gt;Build opt in a bottom up fashion\n        &lt;ul&gt;\n          &lt;li&gt;$for ~ i \\leftarrow 1 ~ to ~ n$\n            &lt;ul&gt;\n              &lt;li&gt;$opt[i] \\leftarrow 1 + \\max \\{ opt(k) | (v_k, v_i) \\in E \\}$&lt;/li&gt;\n            &lt;/ul&gt;\n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n      &lt;li&gt;Return $opt[n]$&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;&lt;strong&gt;Time Complexity&lt;/strong&gt;&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Initialization of opt is $O(n)$&lt;/li&gt;\n      &lt;li&gt;Building opt takes $O(n)$ iterations\n        &lt;ul&gt;\n          &lt;li&gt;Each iteration takes $O(n)$ accesses to opt&lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;So in total, the time complexity is $O(n^2)$&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;algorithms/dynamic-programming/longest-substring-that-is-a-palindrom&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/algorithms/dynamic-programming/longest-substring-that-is-a-palindrom.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Longest substring that is a Palindrom&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [
                    &quot;Dynamic Programming&quot;,
                    &quot;Palindrom&quot;
                ],
                &quot;categories&quot;: [
                    &quot;Algorithms&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\n# Overview\n\nA string $S$ is a palindrom iff $S = reverse(S)$\n\n**The Problem**\n\nGiven a string S, find the longest *substring* of S that is also a palindrom\n\n&gt; Remember that substrings are consequtive\n\n**To illustrate**\n\n- a**bcb**ea $\\rightarrow$ bcb (not abcba)\n- **abbcbba**dad $\\rightarrow$ abbcbba\n\n# Solution\n\nWe shall define $opt(i, j)$ to indicate the length of the longest palindrom substring between the i&#x27;th character to the j&#x27;th character by the recurrence relation\n\nfor all i, opt(i, i) = true since a single character is a palindrom of itself.\n\nfor all i and $j &gt; i$, opt(i, j) true if S[i] = S[j] and also opt(i + 1, j - 1) = true.\n\nFinally, to figure out the length of the longest substring that is a palindrom we need to find i and j that maximizes $j - i$ and such that opt(i, j) = true.\n\n**Psuedo**\n\n- Initialize opt\n  - $opt \\leftarrow matrix(n, n)$\n    - initialize all values false by default\n    - $\\forall i$\n      - $opt(i, i) = true$\n      - $opt(i, i + 1) \\leftarrow S[i]=S[i+1]$\n\n- Build opt in a bottom up fashion\n  - $for ~ l = 2, 3, ... n - 1$\n    - $for ~ i = 1, ..., n - l$\n      - $j \\leftarrow i +  - 1$\n      - $opt(i, j) \\leftarrow S[i] = S[j] \\land opt(i + 1, j - 1)$\n- Find the length of the longest substring that is a palindrom\n  - $for~ l = n-1, n-2, ..., 1$\n    - $for~ i = 1, 2, ..., n - l$\n      - $if~ opt(i, i + l)$\n        - return l\n\n**Time Complexity**\n\n- Initialization of opt is $O(n^2)$\n- Building opt takes $O(n^2)$ iterations\n  - Each iteration takes $O(1)$ accesses to opt\n\nSo in total, the time complexity is $O(n^2)$&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Overview&lt;/h1&gt;\n    &lt;p&gt;A string $S$ is a palindrom iff $S = reverse(S)$&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;The Problem&lt;/strong&gt;&lt;/p&gt;\n    &lt;p&gt;Given a string S, find the longest &lt;em&gt;substring&lt;/em&gt; of S that is also a palindrom&lt;/p&gt;\n    &lt;blockquote&gt;\n      &lt;p&gt;Remember that substrings are consequtive&lt;/p&gt;\n    &lt;/blockquote&gt;\n    &lt;p&gt;&lt;strong&gt;To illustrate&lt;/strong&gt;&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;a&lt;strong&gt;bcb&lt;/strong&gt;ea $\\rightarrow$ bcb (not abcba)&lt;/li&gt;\n      &lt;li&gt;&lt;strong&gt;abbcbba&lt;/strong&gt;dad $\\rightarrow$ abbcbba&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;h1&gt;Solution&lt;/h1&gt;\n    &lt;p&gt;We shall define $opt(i, j)$ to indicate the length of the longest palindrom substring between the i&#x27;th character to the j&#x27;th character by the recurrence relation&lt;/p&gt;\n    &lt;p&gt;for all i, opt(i, i) = true since a single character is a palindrom of itself.&lt;/p&gt;\n    &lt;p&gt;for all i and $j &gt; i$, opt(i, j) true if S[i] = S[j] and also opt(i + 1, j - 1) = true.&lt;/p&gt;\n    &lt;p&gt;Finally, to figure out the length of the longest substring that is a palindrom we need to find i and j that maximizes $j - i$ and such that opt(i, j) = true.&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Psuedo&lt;/strong&gt;&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;\n        &lt;p&gt;Initialize opt&lt;/p&gt;\n        &lt;ul&gt;\n          &lt;li&gt;$opt \\leftarrow matrix(n, n)$\n            &lt;ul&gt;\n              &lt;li&gt;initialize all values false by default&lt;/li&gt;\n              &lt;li&gt;$\\forall i$\n                &lt;ul&gt;\n                  &lt;li&gt;$opt(i, i) = true$&lt;/li&gt;\n                  &lt;li&gt;$opt(i, i + 1) \\leftarrow S[i]=S[i+1]$&lt;/li&gt;\n                &lt;/ul&gt;\n              &lt;/li&gt;\n            &lt;/ul&gt;\n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n      &lt;li&gt;\n        &lt;p&gt;Build opt in a bottom up fashion&lt;/p&gt;\n        &lt;ul&gt;\n          &lt;li&gt;$for ~ l = 2, 3, ... n - 1$\n            &lt;ul&gt;\n              &lt;li&gt;$for ~ i = 1, ..., n - l$\n                &lt;ul&gt;\n                  &lt;li&gt;$j \\leftarrow i + - 1$&lt;/li&gt;\n                  &lt;li&gt;$opt(i, j) \\leftarrow S[i] = S[j] \\land opt(i + 1, j - 1)$&lt;/li&gt;\n                &lt;/ul&gt;\n              &lt;/li&gt;\n            &lt;/ul&gt;\n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n      &lt;li&gt;\n        &lt;p&gt;Find the length of the longest substring that is a palindrom&lt;/p&gt;\n        &lt;ul&gt;\n          &lt;li&gt;$for~ l = n-1, n-2, ..., 1$\n            &lt;ul&gt;\n              &lt;li&gt;$for~ i = 1, 2, ..., n - l$\n                &lt;ul&gt;\n                  &lt;li&gt;$if~ opt(i, i + l)$\n                    &lt;ul&gt;\n                      &lt;li&gt;return l&lt;/li&gt;\n                    &lt;/ul&gt;\n                  &lt;/li&gt;\n                &lt;/ul&gt;\n              &lt;/li&gt;\n            &lt;/ul&gt;\n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;&lt;strong&gt;Time Complexity&lt;/strong&gt;&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Initialization of opt is $O(n^2)$&lt;/li&gt;\n      &lt;li&gt;Building opt takes $O(n^2)$ iterations\n        &lt;ul&gt;\n          &lt;li&gt;Each iteration takes $O(1)$ accesses to opt&lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;So in total, the time complexity is $O(n^2)$&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;aws/best-practices&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/aws/best-practices.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Aws, Best practices&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [],
                &quot;categories&quot;: [
                    &quot;Aws&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\n### Do not use the root acount for any task that is not required to be performed by a root user\n\nWhat else can we do then? Create a different user and specifically control permissions.\n\nEven ceating an Administrator user is good - The point is, the use the account owner.&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h3&gt;Do not use the root acount for any task that is not required to be performed by a root user&lt;/h3&gt;\n    &lt;p&gt;What else can we do then? Create a different user and specifically control permissions.&lt;/p&gt;\n    &lt;p&gt;Even ceating an Administrator user is good - The point is, the use the account owner.&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;aws/bullets&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/aws/bullets.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Aws, Bullets&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [],
                &quot;categories&quot;: [
                    &quot;Aws&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\n# Available Storage Types\n\n- Block\n    - EBS\n- File\n    - EFS\n    - FSx Lustre\n    - FSx Windows\n- Block\n    - S3\n    - Glacier\n\n# EBS Volume Types\n\n**EBS** is a block storage.\n\nIt provides one of the following volume types, with the following categories\n\n- Solid state (SSD)\n    - General Purpose SSD - Provides a sane cost/performance balance\n    - Provisioned IOPS SSD - High performance, mission-critical, low latency and high-throughput\n- Hard disk drives (HDD)\n    - Throughput Optimized HDD - Low cost HDD for throughput intensive workloads\n    - Cold HDD - Lowest cost HDD for less frequently accessed workloads\n- Previous generation\n\n# ELB\n\n- Application Load Balancer (ALB) - Works at application layer (layer 7 osi) HTTP/HTTPS\n    - Host based routing - Balance using the host and port portion of the url (scheme://host:port/path)\n    - Path based routing - Balance using the path portion of the url (scheme://host:port/path)\n\n- Network Load Balancer (NLB) - Works at the network layer (layer 4 osi) IP\n\n# Route53 routing policies\n\n- Simple Round Robin - Route the user in a round robin fashion accross servers\n- Weighted - Weight precentage of routes for each server\n- Geolocation - Route the user to the geographically nearest server\n\n# EC2 Instance Types\n\n- General Purpose (m4)\n- Compute Optimized\n- Memory Optimized\n- Accelerated Computing - Using hardware accelerators\n- Storage Optimized\n- High Memory - Acquired only using special request\n&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Available Storage Types&lt;/h1&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Block\n        &lt;ul&gt;\n          &lt;li&gt;EBS&lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n      &lt;li&gt;File\n        &lt;ul&gt;\n          &lt;li&gt;EFS&lt;/li&gt;\n          &lt;li&gt;FSx Lustre&lt;/li&gt;\n          &lt;li&gt;FSx Windows&lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n      &lt;li&gt;Block\n        &lt;ul&gt;\n          &lt;li&gt;S3&lt;/li&gt;\n          &lt;li&gt;Glacier&lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;h1&gt;EBS Volume Types&lt;/h1&gt;\n    &lt;p&gt;&lt;strong&gt;EBS&lt;/strong&gt; is a block storage.&lt;/p&gt;\n    &lt;p&gt;It provides one of the following volume types, with the following categories&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Solid state (SSD)\n        &lt;ul&gt;\n          &lt;li&gt;General Purpose SSD - Provides a sane cost/performance balance&lt;/li&gt;\n          &lt;li&gt;Provisioned IOPS SSD - High performance, mission-critical, low latency and high-throughput&lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n      &lt;li&gt;Hard disk drives (HDD)\n        &lt;ul&gt;\n          &lt;li&gt;Throughput Optimized HDD - Low cost HDD for throughput intensive workloads&lt;/li&gt;\n          &lt;li&gt;Cold HDD - Lowest cost HDD for less frequently accessed workloads&lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n      &lt;li&gt;Previous generation&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;h1&gt;ELB&lt;/h1&gt;\n    &lt;ul&gt;\n      &lt;li&gt;\n        &lt;p&gt;Application Load Balancer (ALB) - Works at application layer (layer 7 osi) HTTP/HTTPS&lt;/p&gt;\n        &lt;ul&gt;\n          &lt;li&gt;Host based routing - Balance using the host and port portion of the url (scheme://host:port/path)&lt;/li&gt;\n          &lt;li&gt;Path based routing - Balance using the path portion of the url (scheme://host:port/path)&lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n      &lt;li&gt;\n        &lt;p&gt;Network Load Balancer (NLB) - Works at the network layer (layer 4 osi) IP&lt;/p&gt;\n      &lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;h1&gt;Route53 routing policies&lt;/h1&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Simple Round Robin - Route the user in a round robin fashion accross servers&lt;/li&gt;\n      &lt;li&gt;Weighted - Weight precentage of routes for each server&lt;/li&gt;\n      &lt;li&gt;Geolocation - Route the user to the geographically nearest server&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;h1&gt;EC2 Instance Types&lt;/h1&gt;\n    &lt;ul&gt;\n      &lt;li&gt;General Purpose (m4)&lt;/li&gt;\n      &lt;li&gt;Compute Optimized&lt;/li&gt;\n      &lt;li&gt;Memory Optimized&lt;/li&gt;\n      &lt;li&gt;Accelerated Computing - Using hardware accelerators&lt;/li&gt;\n      &lt;li&gt;Storage Optimized&lt;/li&gt;\n      &lt;li&gt;High Memory - Acquired only using special request&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;aws/cli&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/aws/cli.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Aws, The cli&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [],
                &quot;categories&quot;: [
                    &quot;Aws&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\nThe _aws cli_ is a utility that captures all of the administration capabilities with aws.\n\n## Configuration\n\nWe can apply initial configuration using the `configure` command which will create a default configuration. This configuration is called a __profile__. For any action, if no profile is specified, the default one will be used.\n\n\nAlternatively, we can create named profiles and use each separately and specifically. The selected profile is determined using a cli argument or by an environment variable (AWS_PROFILE).\n\n{{&lt; alert \&quot;Perhaps providing a test environment as the default is the safest option!\&quot; &gt;}}\n\n## Cli vs Console\n\nThis is probably subjective, but I highly advocate the use of the cli.&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;p&gt;The &lt;em&gt;aws cli&lt;/em&gt; is a utility that captures all of the administration capabilities with aws.&lt;/p&gt;\n    &lt;h2&gt;Configuration&lt;/h2&gt;\n    &lt;p&gt;We can apply initial configuration using the &lt;code&gt;configure&lt;/code&gt; command which will create a default configuration. This configuration is called a &lt;strong&gt;profile&lt;/strong&gt;. For any action, if no profile is specified, the default one will be used.&lt;/p&gt;\n    &lt;p&gt;Alternatively, we can create named profiles and use each separately and specifically. The selected profile is determined using a cli argument or by an environment variable (AWS_PROFILE).&lt;/p&gt;\n    &lt;p&gt;{{&amp;#x3C; alert \&quot;Perhaps providing a test environment as the default is the safest option!\&quot; &gt;}}&lt;/p&gt;\n    &lt;h2&gt;Cli vs Console&lt;/h2&gt;\n    &lt;p&gt;This is probably subjective, but I highly advocate the use of the cli.&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;aws/kinesis&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/aws/kinesis.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: null,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [],
                &quot;categories&quot;: [
                    &quot;Aws&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\nList streams\n\n    aws kinesis list-streams\n\nDescribe a _Stream_, listing it&#x27;s _Shards_, _Stream&#x27;s_ ARN, current _SequenceNumbers_ etc...\n\n    aws kinesis list-streams --stream-name {stream-name}\n\nTo list the consumers/producers of a given _Stream_\n\n    aws kinesis list-stream-consumers --stream-arn {stream-arn}\n\n## Getting records\n\n_ShardIterator_ is an object used to iterate _Records_ within a specific _Shard_. So, in order to get a _Shard&#x27;s_ _Records_ we need to acquire a reference to a specific _ShardIterator_.\n\n    aws kines get-shard-iterator --stream-name {stream-name} --shard-id {shard-id} --shard-iterator-type {shard-iterator-type}\n\nThe ```shard-iterator-type``` has multiple choices, advise the documentation for those.\n\nThe ```get-shard-iterator``` command provided us with an identifier of the _ShardIterator_, we can use it to get _Records_ using\n\n    aws kinesis get-records --shard-iterator {shard-iterator}\n&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;p&gt;List streams&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-dsconfig\&quot;&gt;&lt;span class=\&quot;hljs-string\&quot;&gt;aws&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;kinesis&lt;/span&gt; &lt;span class=\&quot;hljs-built_in\&quot;&gt;list-streams&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;Describe a &lt;em&gt;Stream&lt;/em&gt;, listing it&#x27;s &lt;em&gt;Shards&lt;/em&gt;, &lt;em&gt;Stream&#x27;s&lt;/em&gt; ARN, current &lt;em&gt;SequenceNumbers&lt;/em&gt; etc...&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-dsconfig\&quot;&gt;&lt;span class=\&quot;hljs-string\&quot;&gt;aws&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;kinesis&lt;/span&gt; &lt;span class=\&quot;hljs-built_in\&quot;&gt;list-streams&lt;/span&gt; &lt;span class=\&quot;hljs-built_in\&quot;&gt;--stream-name&lt;/span&gt; {&lt;span class=\&quot;hljs-string\&quot;&gt;stream-name&lt;/span&gt;}\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;To list the consumers/producers of a given &lt;em&gt;Stream&lt;/em&gt;&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-dsconfig\&quot;&gt;&lt;span class=\&quot;hljs-string\&quot;&gt;aws&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;kinesis&lt;/span&gt; &lt;span class=\&quot;hljs-built_in\&quot;&gt;list-stream-consumers&lt;/span&gt; &lt;span class=\&quot;hljs-built_in\&quot;&gt;--stream-arn&lt;/span&gt; {&lt;span class=\&quot;hljs-string\&quot;&gt;stream-arn&lt;/span&gt;}\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;h2&gt;Getting records&lt;/h2&gt;\n    &lt;p&gt;&lt;em&gt;ShardIterator&lt;/em&gt; is an object used to iterate &lt;em&gt;Records&lt;/em&gt; within a specific &lt;em&gt;Shard&lt;/em&gt;. So, in order to get a &lt;em&gt;Shard&#x27;s&lt;/em&gt; &lt;em&gt;Records&lt;/em&gt; we need to acquire a reference to a specific &lt;em&gt;ShardIterator&lt;/em&gt;.&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-dsconfig\&quot;&gt;&lt;span class=\&quot;hljs-string\&quot;&gt;aws&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;kines&lt;/span&gt; &lt;span class=\&quot;hljs-built_in\&quot;&gt;get-shard-iterator&lt;/span&gt; &lt;span class=\&quot;hljs-built_in\&quot;&gt;--stream-name&lt;/span&gt; {&lt;span class=\&quot;hljs-string\&quot;&gt;stream-name&lt;/span&gt;} &lt;span class=\&quot;hljs-built_in\&quot;&gt;--shard-id&lt;/span&gt; {&lt;span class=\&quot;hljs-string\&quot;&gt;shard-id&lt;/span&gt;} &lt;span class=\&quot;hljs-built_in\&quot;&gt;--shard-iterator-type&lt;/span&gt; {&lt;span class=\&quot;hljs-string\&quot;&gt;shard-iterator-type&lt;/span&gt;}\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;The &lt;code&gt;shard-iterator-type&lt;/code&gt; has multiple choices, advise the documentation for those.&lt;/p&gt;\n    &lt;p&gt;The &lt;code&gt;get-shard-iterator&lt;/code&gt; command provided us with an identifier of the &lt;em&gt;ShardIterator&lt;/em&gt;, we can use it to get &lt;em&gt;Records&lt;/em&gt; using&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-dsconfig\&quot;&gt;&lt;span class=\&quot;hljs-string\&quot;&gt;aws&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;kinesis&lt;/span&gt; &lt;span class=\&quot;hljs-built_in\&quot;&gt;get-records&lt;/span&gt; &lt;span class=\&quot;hljs-built_in\&quot;&gt;--shard-iterator&lt;/span&gt; {&lt;span class=\&quot;hljs-string\&quot;&gt;shard-iterator&lt;/span&gt;}\n&lt;/code&gt;&lt;/pre&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;aws/services&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/aws/services.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Services&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [],
                &quot;categories&quot;: [
                    &quot;Aws&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\n# S3\n\n- Object-Based, Serverless, Unlimited storage service\n- Data is replicated across at least 3 AZs which ensures 99.99% __Availability__ and 11&#x27; 9s of __Durability__\n- Objects contain data and can have size for 0 Byts to 5 Terabytes\n- Buckets names are unique globally and contain either Objects or Folders (which in turn contain Objects)\n\n- __Lifecycle Management__ is a mechanism to delete/move objects between __Storage Classes__ based on schedule or some criteria\n\n[Exampro Cheatsheet](https://youtu.be/Ia-UEYYR44s?t=3524)\n\nTypes of Replication\n- Cross-Region Replication (CRR) - Bucket is **asynchronously** replicated to another region\n- Same-Region Replication (SRR) - Bucket is **asynchronously** replicated to the same region\n\n# Snowball\n\n- Snowball\n- Snoball Edge\n- Snowmobile\n\n# VPC\n\n- VPC Peering\n- Route Tables\n- Internet Gateway\n- Bastion / Jumpbox\n- Direct Connect\n\n## VPC Endpoints\n\n- Interface Endpoints\n- Gateway Endpoints\n\n## VPC Flow Logs\n\n# NACL\n\n# Security Groups \n\n# NAT\n\n# IAM\n\n# COGNITO\n\n# DNS\n\n# Route 53\n\n# EC2\n\n## EC2 Pricing\n\n## AMI\n\n## Auto Scaling Groups\n\n## ELB\n\n# EFS\n\n# EBS\n\n# Cloud Front\n\n# Aurora\n\n# Redshift\n\n# DynamoDB\n\n# CloudFormation\n\n# CloudWatch\n\n# CloudTrail\n\n# Lambda\n\n# SQS\n\n# SNS\n\n# ElasticCache\n\n# High Availability\n\n# Elastic Beanstalk\n\n# Kinesis\n\nRealtime processing platform.\n\n# Storage Gateway\n\nProvides on-premise storage access to cloud storage.\n\nPractically you install a VM on the on-premise host which will can be connected as NFS/SMB.\n\nStorage Types\n- S3 File Gateway\n- FSx File Gateway\n- Tape Gateway\n- Volume Gateway\n\nModes\n- Gateway Stored - Access data in the VM and synchronously get data from remote\n- Cached Stored - Frequently accessed data is cached at the VM and data is invalidated asynchronously\n&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;S3&lt;/h1&gt;\n    &lt;ul&gt;\n      &lt;li&gt;\n        &lt;p&gt;Object-Based, Serverless, Unlimited storage service&lt;/p&gt;\n      &lt;/li&gt;\n      &lt;li&gt;\n        &lt;p&gt;Data is replicated across at least 3 AZs which ensures 99.99% &lt;strong&gt;Availability&lt;/strong&gt; and 11&#x27; 9s of &lt;strong&gt;Durability&lt;/strong&gt;&lt;/p&gt;\n      &lt;/li&gt;\n      &lt;li&gt;\n        &lt;p&gt;Objects contain data and can have size for 0 Byts to 5 Terabytes&lt;/p&gt;\n      &lt;/li&gt;\n      &lt;li&gt;\n        &lt;p&gt;Buckets names are unique globally and contain either Objects or Folders (which in turn contain Objects)&lt;/p&gt;\n      &lt;/li&gt;\n      &lt;li&gt;\n        &lt;p&gt;&lt;strong&gt;Lifecycle Management&lt;/strong&gt; is a mechanism to delete/move objects between &lt;strong&gt;Storage Classes&lt;/strong&gt; based on schedule or some criteria&lt;/p&gt;\n      &lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;&lt;a href=\&quot;https://youtu.be/Ia-UEYYR44s?t=3524\&quot;&gt;Exampro Cheatsheet&lt;/a&gt;&lt;/p&gt;\n    &lt;p&gt;Types of Replication&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Cross-Region Replication (CRR) - Bucket is &lt;strong&gt;asynchronously&lt;/strong&gt; replicated to another region&lt;/li&gt;\n      &lt;li&gt;Same-Region Replication (SRR) - Bucket is &lt;strong&gt;asynchronously&lt;/strong&gt; replicated to the same region&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;h1&gt;Snowball&lt;/h1&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Snowball&lt;/li&gt;\n      &lt;li&gt;Snoball Edge&lt;/li&gt;\n      &lt;li&gt;Snowmobile&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;h1&gt;VPC&lt;/h1&gt;\n    &lt;ul&gt;\n      &lt;li&gt;VPC Peering&lt;/li&gt;\n      &lt;li&gt;Route Tables&lt;/li&gt;\n      &lt;li&gt;Internet Gateway&lt;/li&gt;\n      &lt;li&gt;Bastion / Jumpbox&lt;/li&gt;\n      &lt;li&gt;Direct Connect&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;h2&gt;VPC Endpoints&lt;/h2&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Interface Endpoints&lt;/li&gt;\n      &lt;li&gt;Gateway Endpoints&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;h2&gt;VPC Flow Logs&lt;/h2&gt;\n    &lt;h1&gt;NACL&lt;/h1&gt;\n    &lt;h1&gt;Security Groups&lt;/h1&gt;\n    &lt;h1&gt;NAT&lt;/h1&gt;\n    &lt;h1&gt;IAM&lt;/h1&gt;\n    &lt;h1&gt;COGNITO&lt;/h1&gt;\n    &lt;h1&gt;DNS&lt;/h1&gt;\n    &lt;h1&gt;Route 53&lt;/h1&gt;\n    &lt;h1&gt;EC2&lt;/h1&gt;\n    &lt;h2&gt;EC2 Pricing&lt;/h2&gt;\n    &lt;h2&gt;AMI&lt;/h2&gt;\n    &lt;h2&gt;Auto Scaling Groups&lt;/h2&gt;\n    &lt;h2&gt;ELB&lt;/h2&gt;\n    &lt;h1&gt;EFS&lt;/h1&gt;\n    &lt;h1&gt;EBS&lt;/h1&gt;\n    &lt;h1&gt;Cloud Front&lt;/h1&gt;\n    &lt;h1&gt;Aurora&lt;/h1&gt;\n    &lt;h1&gt;Redshift&lt;/h1&gt;\n    &lt;h1&gt;DynamoDB&lt;/h1&gt;\n    &lt;h1&gt;CloudFormation&lt;/h1&gt;\n    &lt;h1&gt;CloudWatch&lt;/h1&gt;\n    &lt;h1&gt;CloudTrail&lt;/h1&gt;\n    &lt;h1&gt;Lambda&lt;/h1&gt;\n    &lt;h1&gt;SQS&lt;/h1&gt;\n    &lt;h1&gt;SNS&lt;/h1&gt;\n    &lt;h1&gt;ElasticCache&lt;/h1&gt;\n    &lt;h1&gt;High Availability&lt;/h1&gt;\n    &lt;h1&gt;Elastic Beanstalk&lt;/h1&gt;\n    &lt;h1&gt;Kinesis&lt;/h1&gt;\n    &lt;p&gt;Realtime processing platform.&lt;/p&gt;\n    &lt;h1&gt;Storage Gateway&lt;/h1&gt;\n    &lt;p&gt;Provides on-premise storage access to cloud storage.&lt;/p&gt;\n    &lt;p&gt;Practically you install a VM on the on-premise host which will can be connected as NFS/SMB.&lt;/p&gt;\n    &lt;p&gt;Storage Types&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;S3 File Gateway&lt;/li&gt;\n      &lt;li&gt;FSx File Gateway&lt;/li&gt;\n      &lt;li&gt;Tape Gateway&lt;/li&gt;\n      &lt;li&gt;Volume Gateway&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;Modes&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Gateway Stored - Access data in the VM and synchronously get data from remote&lt;/li&gt;\n      &lt;li&gt;Cached Stored - Frequently accessed data is cached at the VM and data is invalidated asynchronously&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;ddd/applying&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/ddd/applying.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Applying&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [],
                &quot;categories&quot;: [
                    &quot;Domain Driven Design&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\n## Identify the possible Subdomains\n\n1. Core\n2. Support\n3. Generic\n\n## Split the Domain into Subdomains\n\nIf possible apply a single _Bounded Context_ for each _Subdomain_&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h2&gt;Identify the possible Subdomains&lt;/h2&gt;\n    &lt;ol&gt;\n      &lt;li&gt;Core&lt;/li&gt;\n      &lt;li&gt;Support&lt;/li&gt;\n      &lt;li&gt;Generic&lt;/li&gt;\n    &lt;/ol&gt;\n    &lt;h2&gt;Split the Domain into Subdomains&lt;/h2&gt;\n    &lt;p&gt;If possible apply a single &lt;em&gt;Bounded Context&lt;/em&gt; for each &lt;em&gt;Subdomain&lt;/em&gt;&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;ddd/model-refinement-steps&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/ddd/model-refinement-steps.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Model Refinement Steps&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [],
                &quot;categories&quot;: [
                    &quot;Domain Driven Design&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\n# Distinguishing Entities and Value Objects\n\nConsider each object in turn and try to identify an identity.\n\nConsider\n- How to track the entity?\n- Are to instances with same values are the same?\n- Can it exist without some parent object?\n\n# Designing Associations\n\nSpecify traversal directions.\n\nConsider\n- How the is application used?\n\nAvoid\n- Bi-Directional associations\n\n# Identifying Aggregate Boundaries\n\n# Selecting Repositories\n\n# Walking through scenarios\n\n# Iterate\n&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Distinguishing Entities and Value Objects&lt;/h1&gt;\n    &lt;p&gt;Consider each object in turn and try to identify an identity.&lt;/p&gt;\n    &lt;p&gt;Consider&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;How to track the entity?&lt;/li&gt;\n      &lt;li&gt;Are to instances with same values are the same?&lt;/li&gt;\n      &lt;li&gt;Can it exist without some parent object?&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;h1&gt;Designing Associations&lt;/h1&gt;\n    &lt;p&gt;Specify traversal directions.&lt;/p&gt;\n    &lt;p&gt;Consider&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;How the is application used?&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;Avoid&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Bi-Directional associations&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;h1&gt;Identifying Aggregate Boundaries&lt;/h1&gt;\n    &lt;h1&gt;Selecting Repositories&lt;/h1&gt;\n    &lt;h1&gt;Walking through scenarios&lt;/h1&gt;\n    &lt;h1&gt;Iterate&lt;/h1&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;ddd/prologue&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/ddd/prologue.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Domain Driven Design, Prologue&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [],
                &quot;categories&quot;: [
                    &quot;Domain Driven Design&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;&lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;ddd/tactical-design&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/ddd/tactical-design.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Tactical Design&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [],
                &quot;categories&quot;: [
                    &quot;Domain Driven Design&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\n# Entities\n\nEntities are the building blocks of the model.\n\nEntities are differentiated by their Id, not by their attributes.\n\n# Value Objects\n\nValue objects are differentiated by their attributes and contain no Id.\n\nValue objects should be able to implemented in an immutable fashion.\n\n# Services\n\nIn the exact same words\n\n&gt; A SERVICE is an operation offered as an interface that stands alone in the model, without encapsulating state, as ENTITIES and VALUE OBJECTS do.\n\nCharacterisitcs of a good SERVICE\n1. The operation relates to a domain concept that is not a natural part of an ENTITY or VALUE OBJECT\n1. The interface is defined in terms of other elements of the domain model\n1. The operation is stateless\n\n# Aggregates\n\nIn the exact same words\n\n&gt; An AGGREGATE is a cluster of associated objects that we treat as a unit for the purpose of data changes\n\nEach AGGREGATE has a **root** and a **boundary**.\n\nThe **boundary** delineate objects within the AGGREGATE.\n\nThe **root** is a *single*, *specific* ENTITY that the outside can hold references to.\n\n# Factories\n\nIn the exact same words\n\n&gt; When creation of an object, or an entire AGGREGATE, becomes complicated or reveals too much of the internal structure, FACTORIES provide encapsulation\n\nWhile every ENTITY has a constructor receiving it&#x27;s idetity, FACTORIES will provide mechanisms to create complex ENTITIES and AGGREGATES.\n\nFACTORIES are responsible for ensuring that all invariants are met for the AGGREGATE&#x27;s objects.\n\n# Repositories\n\nIn the exact same words\n\n&gt; A REPOSITORY represents all objects of a certain type as a conceptual set (usually emulated).\n\nThe REPOSITORIES act like collections but they often provide additional query mechanisms/options.\n&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Entities&lt;/h1&gt;\n    &lt;p&gt;Entities are the building blocks of the model.&lt;/p&gt;\n    &lt;p&gt;Entities are differentiated by their Id, not by their attributes.&lt;/p&gt;\n    &lt;h1&gt;Value Objects&lt;/h1&gt;\n    &lt;p&gt;Value objects are differentiated by their attributes and contain no Id.&lt;/p&gt;\n    &lt;p&gt;Value objects should be able to implemented in an immutable fashion.&lt;/p&gt;\n    &lt;h1&gt;Services&lt;/h1&gt;\n    &lt;p&gt;In the exact same words&lt;/p&gt;\n    &lt;blockquote&gt;\n      &lt;p&gt;A SERVICE is an operation offered as an interface that stands alone in the model, without encapsulating state, as ENTITIES and VALUE OBJECTS do.&lt;/p&gt;\n    &lt;/blockquote&gt;\n    &lt;p&gt;Characterisitcs of a good SERVICE&lt;/p&gt;\n    &lt;ol&gt;\n      &lt;li&gt;The operation relates to a domain concept that is not a natural part of an ENTITY or VALUE OBJECT&lt;/li&gt;\n      &lt;li&gt;The interface is defined in terms of other elements of the domain model&lt;/li&gt;\n      &lt;li&gt;The operation is stateless&lt;/li&gt;\n    &lt;/ol&gt;\n    &lt;h1&gt;Aggregates&lt;/h1&gt;\n    &lt;p&gt;In the exact same words&lt;/p&gt;\n    &lt;blockquote&gt;\n      &lt;p&gt;An AGGREGATE is a cluster of associated objects that we treat as a unit for the purpose of data changes&lt;/p&gt;\n    &lt;/blockquote&gt;\n    &lt;p&gt;Each AGGREGATE has a &lt;strong&gt;root&lt;/strong&gt; and a &lt;strong&gt;boundary&lt;/strong&gt;.&lt;/p&gt;\n    &lt;p&gt;The &lt;strong&gt;boundary&lt;/strong&gt; delineate objects within the AGGREGATE.&lt;/p&gt;\n    &lt;p&gt;The &lt;strong&gt;root&lt;/strong&gt; is a &lt;em&gt;single&lt;/em&gt;, &lt;em&gt;specific&lt;/em&gt; ENTITY that the outside can hold references to.&lt;/p&gt;\n    &lt;h1&gt;Factories&lt;/h1&gt;\n    &lt;p&gt;In the exact same words&lt;/p&gt;\n    &lt;blockquote&gt;\n      &lt;p&gt;When creation of an object, or an entire AGGREGATE, becomes complicated or reveals too much of the internal structure, FACTORIES provide encapsulation&lt;/p&gt;\n    &lt;/blockquote&gt;\n    &lt;p&gt;While every ENTITY has a constructor receiving it&#x27;s idetity, FACTORIES will provide mechanisms to create complex ENTITIES and AGGREGATES.&lt;/p&gt;\n    &lt;p&gt;FACTORIES are responsible for ensuring that all invariants are met for the AGGREGATE&#x27;s objects.&lt;/p&gt;\n    &lt;h1&gt;Repositories&lt;/h1&gt;\n    &lt;p&gt;In the exact same words&lt;/p&gt;\n    &lt;blockquote&gt;\n      &lt;p&gt;A REPOSITORY represents all objects of a certain type as a conceptual set (usually emulated).&lt;/p&gt;\n    &lt;/blockquote&gt;\n    &lt;p&gt;The REPOSITORIES act like collections but they often provide additional query mechanisms/options.&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;kubernetes/deployments&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/kubernetes/deployments.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Kubernetes Deployments&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [
                    &quot;Kubernetes&quot;
                ],
                &quot;categories&quot;: [
                    &quot;Kubernetes&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\n# Deployments\n\nA _Deployment_ manages _ReplicaSets_ and _ReplicaSets_ manage _Pods_.\n\n_ReplicaSet_ manage _Pods_ and bring self-healing and scaling capabilities while _Deployments_ manage _ReplicaSets_ and add rollout and rollback capabilities.\n\n### Self-healing and scalability\n\nIf _Pods_ managed by a _Deployment_ fail, they will be replaced - this is known as _self healing_.\n\nIf _Pods_ managed by a _Deployment_ see increased/decreased load, they will be _scaled_.\n\nIn Kubernetes there are 3 related concepts\n\n- _desired state_\n- _observerd state_\n- _reconciliation_\n\n_ReplicaSets_ are implemented as a controller running background process comparing the _desired state_ vs the _observed state_. If they are different it contacts the cluster to perform _reconciliation_.\n\n### Rolling updates\n\nZero downtime, rolling updates (a.k.a rollouts) can be supported when a service has backward and forward compatibility. One by one, the _ReplicaSet_ bring a replica down and introduces a new one with the designated version until all of the _Pods_ are updated with the desired version.\n\nIt is crucial that the services be stateless and backward/forward compatible for this to work.\n\n### Rollbacks\n\n### Commands\n\nTo scale a _Deployment_\n\n    kubectl scale deployment {deployment-name} --replicas {number-of-replicas}\n\nAfter changing image versions, initiate rollouts simply by reaplying a manifest\n\n    kubectl apply -f {manifest-path}\n\nWe can monitor the rollout progress by\n\n    kubectl rollout status deployment {deployment-name}\n\nTo pause a rollout\n\n    kubectl rollout pause deployment {deployment-name}\n\nTo resume a rollout\n\n    kubectl rollout resume deployment {deployment-name}\n\nIn the manifests we can specify ```revisionHistoryLimit``` for containers. \n\nTo show rollout history\n\n    kubectl rollout history deployment {deployment-name}\n\nTo rollback to a revision\n\n    kubectl rollout undo deployment {deployment-name} --to-revision={revision-number}\n\n\n&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Deployments&lt;/h1&gt;\n    &lt;p&gt;A &lt;em&gt;Deployment&lt;/em&gt; manages &lt;em&gt;ReplicaSets&lt;/em&gt; and &lt;em&gt;ReplicaSets&lt;/em&gt; manage &lt;em&gt;Pods&lt;/em&gt;.&lt;/p&gt;\n    &lt;p&gt;&lt;em&gt;ReplicaSet&lt;/em&gt; manage &lt;em&gt;Pods&lt;/em&gt; and bring self-healing and scaling capabilities while &lt;em&gt;Deployments&lt;/em&gt; manage &lt;em&gt;ReplicaSets&lt;/em&gt; and add rollout and rollback capabilities.&lt;/p&gt;\n    &lt;h3&gt;Self-healing and scalability&lt;/h3&gt;\n    &lt;p&gt;If &lt;em&gt;Pods&lt;/em&gt; managed by a &lt;em&gt;Deployment&lt;/em&gt; fail, they will be replaced - this is known as &lt;em&gt;self healing&lt;/em&gt;.&lt;/p&gt;\n    &lt;p&gt;If &lt;em&gt;Pods&lt;/em&gt; managed by a &lt;em&gt;Deployment&lt;/em&gt; see increased/decreased load, they will be &lt;em&gt;scaled&lt;/em&gt;.&lt;/p&gt;\n    &lt;p&gt;In Kubernetes there are 3 related concepts&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;&lt;em&gt;desired state&lt;/em&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;em&gt;observerd state&lt;/em&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;em&gt;reconciliation&lt;/em&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;&lt;em&gt;ReplicaSets&lt;/em&gt; are implemented as a controller running background process comparing the &lt;em&gt;desired state&lt;/em&gt; vs the &lt;em&gt;observed state&lt;/em&gt;. If they are different it contacts the cluster to perform &lt;em&gt;reconciliation&lt;/em&gt;.&lt;/p&gt;\n    &lt;h3&gt;Rolling updates&lt;/h3&gt;\n    &lt;p&gt;Zero downtime, rolling updates (a.k.a rollouts) can be supported when a service has backward and forward compatibility. One by one, the &lt;em&gt;ReplicaSet&lt;/em&gt; bring a replica down and introduces a new one with the designated version until all of the &lt;em&gt;Pods&lt;/em&gt; are updated with the desired version.&lt;/p&gt;\n    &lt;p&gt;It is crucial that the services be stateless and backward/forward compatible for this to work.&lt;/p&gt;\n    &lt;h3&gt;Rollbacks&lt;/h3&gt;\n    &lt;h3&gt;Commands&lt;/h3&gt;\n    &lt;p&gt;To scale a &lt;em&gt;Deployment&lt;/em&gt;&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-fortran\&quot;&gt;kubectl &lt;span class=\&quot;hljs-built_in\&quot;&gt;scale&lt;/span&gt; deployment {deployment-&lt;span class=\&quot;hljs-keyword\&quot;&gt;name&lt;/span&gt;} --replicas {&lt;span class=\&quot;hljs-keyword\&quot;&gt;number&lt;/span&gt;-of-replicas}\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;After changing image versions, initiate rollouts simply by reaplying a manifest&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-puppet\&quot;&gt;kubectl apply -&lt;span class=\&quot;hljs-keyword\&quot;&gt;f&lt;/span&gt; {&lt;span class=\&quot;hljs-literal\&quot;&gt;manifest&lt;/span&gt;-&lt;span class=\&quot;hljs-built_in\&quot;&gt;path&lt;/span&gt;}\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;We can monitor the rollout progress by&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-fortran\&quot;&gt;kubectl rollout &lt;span class=\&quot;hljs-keyword\&quot;&gt;status&lt;/span&gt; deployment {deployment-&lt;span class=\&quot;hljs-keyword\&quot;&gt;name&lt;/span&gt;}\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;To pause a rollout&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-fortran\&quot;&gt;kubectl rollout &lt;span class=\&quot;hljs-keyword\&quot;&gt;pause&lt;/span&gt; deployment {deployment-&lt;span class=\&quot;hljs-keyword\&quot;&gt;name&lt;/span&gt;}\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;To resume a rollout&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-basic\&quot;&gt;kubectl rollout &lt;span class=\&quot;hljs-keyword\&quot;&gt;resume&lt;/span&gt; deployment {deployment-&lt;span class=\&quot;hljs-keyword\&quot;&gt;name&lt;/span&gt;}\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;In the manifests we can specify &lt;code&gt;revisionHistoryLimit&lt;/code&gt; for containers.&lt;/p&gt;\n    &lt;p&gt;To show rollout history&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-properties\&quot;&gt;&lt;span class=\&quot;hljs-attr\&quot;&gt;kubectl&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;rollout history deployment {deployment-name}&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;To rollback to a revision&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-vim\&quot;&gt;kubectl rollout &lt;span class=\&quot;hljs-keyword\&quot;&gt;undo&lt;/span&gt; deployment {deployment-name} --&lt;span class=\&quot;hljs-keyword\&quot;&gt;to&lt;/span&gt;-revision={revision-&lt;span class=\&quot;hljs-keyword\&quot;&gt;number&lt;/span&gt;}\n&lt;/code&gt;&lt;/pre&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;kubernetes/service-discovery&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/kubernetes/service-discovery.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Service Discovery&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [
                    &quot;Kubernetes&quot;
                ],
                &quot;categories&quot;: [
                    &quot;Kubernetes&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\n# Service Discovery\n\nService discovery is a mean for applications to find one another in the cluster.\n\nThere are two major components to service discovery\n\n- Registration\n- Discovery\n\nService registration is when an application registers itself in a _service registry_.\n\nKubernetes uses its internal DNS as a _service registry_, and as we know, _Services_ are automatically registered with DNS.\n\nFor discovery to work, Kubernetes provides a well-known internal DNS services that are called the __cluster DNS__, in the namespace __kube-system__. Every _Pod_ in the cluster is automatically configured to where this service is. The relevant _Pods_ are managed by a _Deployment_ called __coredns__ and frontend by a _Service_ called __kube-dns__. \n\nFor illustration\n\n```\nkubectl get pods --namespace kube-system --selector k8s-app=kube-dns\n\nNAME                       READY   STATUS    RESTARTS   AGE\ncoredns-6d4b75cb6d-4lpv9   1/1     Running   0          139m\ncoredns-6d4b75cb6d-vmkfz   1/1     Running   0          139m\n\n```\n\nWe specify a _Service_ DNS using it&#x27;s name (in the metadata)\n&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Service Discovery&lt;/h1&gt;\n    &lt;p&gt;Service discovery is a mean for applications to find one another in the cluster.&lt;/p&gt;\n    &lt;p&gt;There are two major components to service discovery&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Registration&lt;/li&gt;\n      &lt;li&gt;Discovery&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;Service registration is when an application registers itself in a &lt;em&gt;service registry&lt;/em&gt;.&lt;/p&gt;\n    &lt;p&gt;Kubernetes uses its internal DNS as a &lt;em&gt;service registry&lt;/em&gt;, and as we know, &lt;em&gt;Services&lt;/em&gt; are automatically registered with DNS.&lt;/p&gt;\n    &lt;p&gt;For discovery to work, Kubernetes provides a well-known internal DNS services that are called the &lt;strong&gt;cluster DNS&lt;/strong&gt;, in the namespace &lt;strong&gt;kube-system&lt;/strong&gt;. Every &lt;em&gt;Pod&lt;/em&gt; in the cluster is automatically configured to where this service is. The relevant &lt;em&gt;Pods&lt;/em&gt; are managed by a &lt;em&gt;Deployment&lt;/em&gt; called &lt;strong&gt;coredns&lt;/strong&gt; and frontend by a &lt;em&gt;Service&lt;/em&gt; called &lt;strong&gt;kube-dns&lt;/strong&gt;.&lt;/p&gt;\n    &lt;p&gt;For illustration&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-angelscript\&quot;&gt;kubectl &lt;span class=\&quot;hljs-keyword\&quot;&gt;get&lt;/span&gt; pods --&lt;span class=\&quot;hljs-keyword\&quot;&gt;namespace&lt;/span&gt; &lt;span class=\&quot;hljs-symbol\&quot;&gt;kube&lt;/span&gt;-&lt;span class=\&quot;hljs-symbol\&quot;&gt;system&lt;/span&gt; --&lt;span class=\&quot;hljs-symbol\&quot;&gt;selector&lt;/span&gt; &lt;span class=\&quot;hljs-symbol\&quot;&gt;k8s&lt;/span&gt;-&lt;span class=\&quot;hljs-symbol\&quot;&gt;app&lt;/span&gt;=&lt;span class=\&quot;hljs-symbol\&quot;&gt;kube&lt;/span&gt;-&lt;span class=\&quot;hljs-symbol\&quot;&gt;dns&lt;/span&gt;\n\n&lt;span class=\&quot;hljs-symbol\&quot;&gt;NAME&lt;/span&gt;                       &lt;span class=\&quot;hljs-symbol\&quot;&gt;READY&lt;/span&gt;   &lt;span class=\&quot;hljs-symbol\&quot;&gt;STATUS&lt;/span&gt;    &lt;span class=\&quot;hljs-symbol\&quot;&gt;RESTARTS&lt;/span&gt;   &lt;span class=\&quot;hljs-symbol\&quot;&gt;AGE&lt;/span&gt;\n&lt;span class=\&quot;hljs-symbol\&quot;&gt;coredns&lt;/span&gt;-&lt;span class=\&quot;hljs-symbol\&quot;&gt;6d4b75cb6d&lt;/span&gt;-&lt;span class=\&quot;hljs-symbol\&quot;&gt;4lpv9&lt;/span&gt;   &lt;span class=\&quot;hljs-symbol\&quot;&gt;1&lt;/span&gt;/&lt;span class=\&quot;hljs-symbol\&quot;&gt;1&lt;/span&gt;     &lt;span class=\&quot;hljs-symbol\&quot;&gt;Running&lt;/span&gt;   &lt;span class=\&quot;hljs-symbol\&quot;&gt;0&lt;/span&gt;          &lt;span class=\&quot;hljs-symbol\&quot;&gt;139m&lt;/span&gt;\n&lt;span class=\&quot;hljs-symbol\&quot;&gt;coredns&lt;/span&gt;-&lt;span class=\&quot;hljs-symbol\&quot;&gt;6d4b75cb6d&lt;/span&gt;-&lt;span class=\&quot;hljs-symbol\&quot;&gt;vmkfz&lt;/span&gt;   &lt;span class=\&quot;hljs-symbol\&quot;&gt;1&lt;/span&gt;/&lt;span class=\&quot;hljs-symbol\&quot;&gt;1&lt;/span&gt;     &lt;span class=\&quot;hljs-symbol\&quot;&gt;Running&lt;/span&gt;   &lt;span class=\&quot;hljs-symbol\&quot;&gt;0&lt;/span&gt;          &lt;span class=\&quot;hljs-symbol\&quot;&gt;139m&lt;/span&gt;\n\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;We specify a &lt;em&gt;Service&lt;/em&gt; DNS using it&#x27;s name (in the metadata)&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;kubernetes/services&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/kubernetes/services.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Kubernetes Services&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [
                    &quot;Kubernetes&quot;
                ],
                &quot;categories&quot;: [
                    &quot;Kubernetes&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\n# Services \n\n_Service_ provides reliable access to _Pods_.\n\nMain _Service_ concepts\n\n- _Services_ are REST objects in the API that we define in a manifest file or post to the API server.\n- Every service gets it&#x27;s own __stable IP address__, it&#x27;s own __stable DNS name__ and it&#x27;s own __stable port__.\n- _Services_ use __labels__ and __selectors__ to dynamically select the _Pods_ they send traffic to.\n\n_Services_ get a list of healthy pods that match the relevant selctors using a Kubernetes object called an _Endpoint_. Kubernetes is continuously monitoring the state of the _Pods_ and updates the relevant _Endpoints&#x27;_ lists.\n\n```bash\n              +----------------+     +------------+     +--------------------------+\n{request} --&gt; | DNS resolution | --&gt; | Service IP | --&gt; | Pod in the Endpoint List |\n              +----------------+     +------------+     +--------------------------+\n```\n\nKubernetes native applications can query the API and directly find the _Service_ IP, bypassing DNS resolution.\n\n&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Services&lt;/h1&gt;\n    &lt;p&gt;&lt;em&gt;Service&lt;/em&gt; provides reliable access to &lt;em&gt;Pods&lt;/em&gt;.&lt;/p&gt;\n    &lt;p&gt;Main &lt;em&gt;Service&lt;/em&gt; concepts&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;&lt;em&gt;Services&lt;/em&gt; are REST objects in the API that we define in a manifest file or post to the API server.&lt;/li&gt;\n      &lt;li&gt;Every service gets it&#x27;s own &lt;strong&gt;stable IP address&lt;/strong&gt;, it&#x27;s own &lt;strong&gt;stable DNS name&lt;/strong&gt; and it&#x27;s own &lt;strong&gt;stable port&lt;/strong&gt;.&lt;/li&gt;\n      &lt;li&gt;&lt;em&gt;Services&lt;/em&gt; use &lt;strong&gt;labels&lt;/strong&gt; and &lt;strong&gt;selectors&lt;/strong&gt; to dynamically select the &lt;em&gt;Pods&lt;/em&gt; they send traffic to.&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;&lt;em&gt;Services&lt;/em&gt; get a list of healthy pods that match the relevant selctors using a Kubernetes object called an &lt;em&gt;Endpoint&lt;/em&gt;. Kubernetes is continuously monitoring the state of the &lt;em&gt;Pods&lt;/em&gt; and updates the relevant &lt;em&gt;Endpoints&#x27;&lt;/em&gt; lists.&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-bash\&quot;&gt;              +----------------+     +------------+     +--------------------------+\n{request} --&gt; | DNS resolution | --&gt; | Service IP | --&gt; | Pod &lt;span class=\&quot;hljs-keyword\&quot;&gt;in&lt;/span&gt; the Endpoint List |\n              +----------------+     +------------+     +--------------------------+\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;Kubernetes native applications can query the API and directly find the &lt;em&gt;Service&lt;/em&gt; IP, bypassing DNS resolution.&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;kubernetes/simple-on-prem-cluster&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/kubernetes/simple-on-prem-cluster.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Simple on-prem Kuberenetes cluster&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [
                    &quot;Kubernetes&quot;
                ],
                &quot;categories&quot;: [
                    &quot;Kubernetes&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\n# Kubes\n\nDeploy a kubernetes cluster.\n\nWe will setup a simple kubernetes cluster will describe the concepts and process.\n\nThe OS on all nodes is debian bullseye - I specifically executing this using vagrant&#x27;s box ```debian/bullseye64```.\n\nTo document the steps we will provide bash script snippets for now. But, the goal is to provide configuration files content for something like ansible or chef.\n\n## Container runtime\n\nKubernetes is an orchestration infrastructure and does not provide any containerization - It relies on a different containarization platform, a.k.a the [Container Runtime](https://kubernetes.io/docs/setup/production-environment/container-runtimes/).\n\nWe will use [docker](https://www.docker.com/). Let&#x27;s install it by following the documentation [Here](https://docs.docker.com/engine/install/debian/).\n\n```\nsudo apt-get remove docker docker-engine docker.io containerd runc\n\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg lsb-release\n\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n\necho \\\n  \&quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\\n  $(lsb_release -cs) stable\&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io\n```\n\nAnother thing to do is docker to utilize systemd for cgroup management (You can read more about it in the container runtimes documentation). To apply this setting we need to edit docker&#x27;s configuration. There are multiple to do so, for example - edit systemd&#x27;s service that initiates docker. Another approach is to edit docker&#x27;s global configuration file which is at ```/etc/docker/daemon.json```. Hence, edit (create if missing) the mentioned file and add\n\n```json\n{\n    ... other configurations\n    \&quot;exec-opts\&quot;: [\&quot;native.cgroupdriver=systemd\&quot;, ... more exec opts if exists]\n}\n```\n\nOnce done, reboot docker by running ```sudo systemctl restart docker```\n\n## Kube Components\n\nWe will not rely on the package manager to install the components.\n\nDefine the relevant variables\n\n&gt; Note that the cni directory does not include the version! Later we will install a network plugin, it&#x27;ll be in the same directory\n\n```\nARCH=\&quot;amd64\&quot;\nCNI_VERSION=\&quot;v0.8.2\&quot;\nCNI_DIR=\&quot;/opt/cni/bin\&quot;\nCRICTL_VERSION=\&quot;v1.23.0\&quot;\nCRICTL_DIR=\&quot;/opt/cri/$CRICTL_VERSION/bin\&quot;\nKUBERNETES_VERSION=\&quot;v1.23.3\&quot;\nKUBERNETES_DIR=\&quot;/opt/kubernetes/$KUBERNETES_VERSION\&quot;\n\n# Install [CNI](https://www.cni.dev/)\n\nsudo mkdir -p $CNI_DIR\ncurl -L \&quot;https://github.com/containernetworking/plugins/releases/download/${CNI_VERSION}/cni-plugins-linux-${ARCH}-${CNI_VERSION}.tgz\&quot; | sudo tar -C $CNI_DIR -xz\n\n# Install [CRI](https://kubernetes.io/docs/concepts/architecture/cri/)\n\nsudo mkdir -p $CRICTL_DIR\ncurl -L \&quot;https://github.com/kubernetes-sigs/cri-tools/releases/download/${CRICTL_VERSION}/crictl-${CRICTL_VERSION}-linux-${ARCH}.tar.gz\&quot; | sudo tar -C $CRICTL_DIR -xz\n\n# Install Kube components\n\nsudo mkdir -p $KUBERNETES_DIR\ncd $KUBERNETES_DIR\nfor component in kubeadm kubectl kubelet; do\n  sudo curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/$KUBERNETES_VERSION/bin/linux/$ARCH/$component\n  sudo chmod +x $component\ndone\n\n# and services\n\nRELEASE_VERSION=\&quot;v0.4.0\&quot;\ncurl -sSL \&quot;https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service\&quot; | sed \&quot;s:/usr/bin:${KUBERNETES_DIR}:g\&quot; | sudo tee /etc/systemd/system/kubelet.service\nsudo mkdir -p /etc/systemd/system/kubelet.service.d\ncurl -sSL \&quot;https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf\&quot; | sed \&quot;s:/usr/bin:${KUBERNETES_DIR}:g\&quot; | sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n```\n\nenable, and start kubelet\n\n```\nsudo systemctl enable --now kubelet\n```\n\n## Initialization\n\nInstall prerequesites for kubeadm\n\n```\nsudo apt-get update \nsudo apt install ethtool socat conntrack\n```\n\nCreate an update alternative\n\n```\nsudo update-alternatives --install /usr/bin/kubeadm kubeadm $KUBERNETES_DIR/kubeadm 100\nsudo update-alternatives --install /usr/bin/kubelet kubelet $KUBERNETES_DIR/kubelet 100\nsudo update-alternatives --install /usr/bin/kubectl kubectl $KUBERNETES_DIR/kubectl 100\n```\n\nRun @controlplane\n\n&gt; TODO: load balancer, hostnames\n\nInitialize configuration such that the network is 10.10.0.0/16\n\n```\nsudo kubeadm init --pod-network-cidr 10.10.0.0/16 --apiserver-advertise-address {ip}\n```\n\nFor documentation, you should see something like\n\n```\nYour Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun \&quot;kubectl apply -f [podnetwork].yaml\&quot; with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 192.168.121.210:6443 --token clns4a.b29f6anjipygy0e2 \\\n\t--discovery-token-ca-cert-hash sha256:833f599cc9ab27eb5010c499e9c77e8e3263fb991d8e9e78ef187ba97e1efb59\n```\n\nDo as it says, run\n\n```\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n```\n\nWe will use [Weave Net](https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/weave-network-policy/) as a network plugin\n\n```\nkubectl apply -f \&quot;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d &#x27;\\n&#x27;)\&quot;\n```\n&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Kubes&lt;/h1&gt;\n    &lt;p&gt;Deploy a kubernetes cluster.&lt;/p&gt;\n    &lt;p&gt;We will setup a simple kubernetes cluster will describe the concepts and process.&lt;/p&gt;\n    &lt;p&gt;The OS on all nodes is debian bullseye - I specifically executing this using vagrant&#x27;s box &lt;code&gt;debian/bullseye64&lt;/code&gt;.&lt;/p&gt;\n    &lt;p&gt;To document the steps we will provide bash script snippets for now. But, the goal is to provide configuration files content for something like ansible or chef.&lt;/p&gt;\n    &lt;h2&gt;Container runtime&lt;/h2&gt;\n    &lt;p&gt;Kubernetes is an orchestration infrastructure and does not provide any containerization - It relies on a different containarization platform, a.k.a the &lt;a href=\&quot;https://kubernetes.io/docs/setup/production-environment/container-runtimes/\&quot;&gt;Container Runtime&lt;/a&gt;.&lt;/p&gt;\n    &lt;p&gt;We will use &lt;a href=\&quot;https://www.docker.com/\&quot;&gt;docker&lt;/a&gt;. Let&#x27;s install it by following the documentation &lt;a href=\&quot;https://docs.docker.com/engine/install/debian/\&quot;&gt;Here&lt;/a&gt;.&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-basic\&quot;&gt;sudo apt-&lt;span class=\&quot;hljs-keyword\&quot;&gt;get&lt;/span&gt; &lt;span class=\&quot;hljs-comment\&quot;&gt;remove docker docker-engine docker.io containerd runc&lt;/span&gt;\n\nsudo apt-&lt;span class=\&quot;hljs-keyword\&quot;&gt;get&lt;/span&gt; update\nsudo apt-&lt;span class=\&quot;hljs-keyword\&quot;&gt;get&lt;/span&gt; install ca-certificates curl gnupg lsb-release\n\ncurl -fsSL https://download.docker.&lt;span class=\&quot;hljs-keyword\&quot;&gt;com&lt;/span&gt;/linux/debian/gpg | sudo gpg --dearmor -o /&lt;span class=\&quot;hljs-keyword\&quot;&gt;usr&lt;/span&gt;/share/keyrings/docker-archive-keyring.gpg\n\necho \\\n  &lt;span class=\&quot;hljs-string\&quot;&gt;\&quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\\n  $(lsb_release -cs) stable\&quot;&lt;/span&gt; | sudo tee /etc/apt/sources.&lt;span class=\&quot;hljs-keyword\&quot;&gt;list&lt;/span&gt;.d/docker.&lt;span class=\&quot;hljs-keyword\&quot;&gt;list&lt;/span&gt; &gt; /dev/null\n\nsudo apt-&lt;span class=\&quot;hljs-keyword\&quot;&gt;get&lt;/span&gt; update\nsudo apt-&lt;span class=\&quot;hljs-keyword\&quot;&gt;get&lt;/span&gt; install docker-ce docker-ce-cli containerd.io\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;Another thing to do is docker to utilize systemd for cgroup management (You can read more about it in the container runtimes documentation). To apply this setting we need to edit docker&#x27;s configuration. There are multiple to do so, for example - edit systemd&#x27;s service that initiates docker. Another approach is to edit docker&#x27;s global configuration file which is at &lt;code&gt;/etc/docker/daemon.json&lt;/code&gt;. Hence, edit (create if missing) the mentioned file and add&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-json\&quot;&gt;&lt;span class=\&quot;hljs-punctuation\&quot;&gt;{&lt;/span&gt;\n    ... other configurations\n    &lt;span class=\&quot;hljs-attr\&quot;&gt;\&quot;exec-opts\&quot;&lt;/span&gt;&lt;span class=\&quot;hljs-punctuation\&quot;&gt;:&lt;/span&gt; &lt;span class=\&quot;hljs-punctuation\&quot;&gt;[&lt;/span&gt;&lt;span class=\&quot;hljs-string\&quot;&gt;\&quot;native.cgroupdriver=systemd\&quot;&lt;/span&gt;&lt;span class=\&quot;hljs-punctuation\&quot;&gt;,&lt;/span&gt; ... more exec opts if exists&lt;span class=\&quot;hljs-punctuation\&quot;&gt;]&lt;/span&gt;\n&lt;span class=\&quot;hljs-punctuation\&quot;&gt;}&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;Once done, reboot docker by running &lt;code&gt;sudo systemctl restart docker&lt;/code&gt;&lt;/p&gt;\n    &lt;h2&gt;Kube Components&lt;/h2&gt;\n    &lt;p&gt;We will not rely on the package manager to install the components.&lt;/p&gt;\n    &lt;p&gt;Define the relevant variables&lt;/p&gt;\n    &lt;blockquote&gt;\n      &lt;p&gt;Note that the cni directory does not include the version! Later we will install a network plugin, it&#x27;ll be in the same directory&lt;/p&gt;\n    &lt;/blockquote&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-bash\&quot;&gt;ARCH=&lt;span class=\&quot;hljs-string\&quot;&gt;\&quot;amd64\&quot;&lt;/span&gt;\nCNI_VERSION=&lt;span class=\&quot;hljs-string\&quot;&gt;\&quot;v0.8.2\&quot;&lt;/span&gt;\nCNI_DIR=&lt;span class=\&quot;hljs-string\&quot;&gt;\&quot;/opt/cni/bin\&quot;&lt;/span&gt;\nCRICTL_VERSION=&lt;span class=\&quot;hljs-string\&quot;&gt;\&quot;v1.23.0\&quot;&lt;/span&gt;\nCRICTL_DIR=&lt;span class=\&quot;hljs-string\&quot;&gt;\&quot;/opt/cri/&lt;span class=\&quot;hljs-variable\&quot;&gt;$CRICTL_VERSION&lt;/span&gt;/bin\&quot;&lt;/span&gt;\nKUBERNETES_VERSION=&lt;span class=\&quot;hljs-string\&quot;&gt;\&quot;v1.23.3\&quot;&lt;/span&gt;\nKUBERNETES_DIR=&lt;span class=\&quot;hljs-string\&quot;&gt;\&quot;/opt/kubernetes/&lt;span class=\&quot;hljs-variable\&quot;&gt;$KUBERNETES_VERSION&lt;/span&gt;\&quot;&lt;/span&gt;\n\n&lt;span class=\&quot;hljs-comment\&quot;&gt;# Install [CNI](https://www.cni.dev/)&lt;/span&gt;\n\nsudo &lt;span class=\&quot;hljs-built_in\&quot;&gt;mkdir&lt;/span&gt; -p &lt;span class=\&quot;hljs-variable\&quot;&gt;$CNI_DIR&lt;/span&gt;\ncurl -L &lt;span class=\&quot;hljs-string\&quot;&gt;\&quot;https://github.com/containernetworking/plugins/releases/download/&lt;span class=\&quot;hljs-variable\&quot;&gt;${CNI_VERSION}&lt;/span&gt;/cni-plugins-linux-&lt;span class=\&quot;hljs-variable\&quot;&gt;${ARCH}&lt;/span&gt;-&lt;span class=\&quot;hljs-variable\&quot;&gt;${CNI_VERSION}&lt;/span&gt;.tgz\&quot;&lt;/span&gt; | sudo tar -C &lt;span class=\&quot;hljs-variable\&quot;&gt;$CNI_DIR&lt;/span&gt; -xz\n\n&lt;span class=\&quot;hljs-comment\&quot;&gt;# Install [CRI](https://kubernetes.io/docs/concepts/architecture/cri/)&lt;/span&gt;\n\nsudo &lt;span class=\&quot;hljs-built_in\&quot;&gt;mkdir&lt;/span&gt; -p &lt;span class=\&quot;hljs-variable\&quot;&gt;$CRICTL_DIR&lt;/span&gt;\ncurl -L &lt;span class=\&quot;hljs-string\&quot;&gt;\&quot;https://github.com/kubernetes-sigs/cri-tools/releases/download/&lt;span class=\&quot;hljs-variable\&quot;&gt;${CRICTL_VERSION}&lt;/span&gt;/crictl-&lt;span class=\&quot;hljs-variable\&quot;&gt;${CRICTL_VERSION}&lt;/span&gt;-linux-&lt;span class=\&quot;hljs-variable\&quot;&gt;${ARCH}&lt;/span&gt;.tar.gz\&quot;&lt;/span&gt; | sudo tar -C &lt;span class=\&quot;hljs-variable\&quot;&gt;$CRICTL_DIR&lt;/span&gt; -xz\n\n&lt;span class=\&quot;hljs-comment\&quot;&gt;# Install Kube components&lt;/span&gt;\n\nsudo &lt;span class=\&quot;hljs-built_in\&quot;&gt;mkdir&lt;/span&gt; -p &lt;span class=\&quot;hljs-variable\&quot;&gt;$KUBERNETES_DIR&lt;/span&gt;\n&lt;span class=\&quot;hljs-built_in\&quot;&gt;cd&lt;/span&gt; &lt;span class=\&quot;hljs-variable\&quot;&gt;$KUBERNETES_DIR&lt;/span&gt;\n&lt;span class=\&quot;hljs-keyword\&quot;&gt;for&lt;/span&gt; component &lt;span class=\&quot;hljs-keyword\&quot;&gt;in&lt;/span&gt; kubeadm kubectl kubelet; &lt;span class=\&quot;hljs-keyword\&quot;&gt;do&lt;/span&gt;\n  sudo curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/&lt;span class=\&quot;hljs-variable\&quot;&gt;$KUBERNETES_VERSION&lt;/span&gt;/bin/linux/&lt;span class=\&quot;hljs-variable\&quot;&gt;$ARCH&lt;/span&gt;/&lt;span class=\&quot;hljs-variable\&quot;&gt;$component&lt;/span&gt;\n  sudo &lt;span class=\&quot;hljs-built_in\&quot;&gt;chmod&lt;/span&gt; +x &lt;span class=\&quot;hljs-variable\&quot;&gt;$component&lt;/span&gt;\n&lt;span class=\&quot;hljs-keyword\&quot;&gt;done&lt;/span&gt;\n\n&lt;span class=\&quot;hljs-comment\&quot;&gt;# and services&lt;/span&gt;\n\nRELEASE_VERSION=&lt;span class=\&quot;hljs-string\&quot;&gt;\&quot;v0.4.0\&quot;&lt;/span&gt;\ncurl -sSL &lt;span class=\&quot;hljs-string\&quot;&gt;\&quot;https://raw.githubusercontent.com/kubernetes/release/&lt;span class=\&quot;hljs-variable\&quot;&gt;${RELEASE_VERSION}&lt;/span&gt;/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service\&quot;&lt;/span&gt; | sed &lt;span class=\&quot;hljs-string\&quot;&gt;\&quot;s:/usr/bin:&lt;span class=\&quot;hljs-variable\&quot;&gt;${KUBERNETES_DIR}&lt;/span&gt;:g\&quot;&lt;/span&gt; | sudo &lt;span class=\&quot;hljs-built_in\&quot;&gt;tee&lt;/span&gt; /etc/systemd/system/kubelet.service\nsudo &lt;span class=\&quot;hljs-built_in\&quot;&gt;mkdir&lt;/span&gt; -p /etc/systemd/system/kubelet.service.d\ncurl -sSL &lt;span class=\&quot;hljs-string\&quot;&gt;\&quot;https://raw.githubusercontent.com/kubernetes/release/&lt;span class=\&quot;hljs-variable\&quot;&gt;${RELEASE_VERSION}&lt;/span&gt;/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf\&quot;&lt;/span&gt; | sed &lt;span class=\&quot;hljs-string\&quot;&gt;\&quot;s:/usr/bin:&lt;span class=\&quot;hljs-variable\&quot;&gt;${KUBERNETES_DIR}&lt;/span&gt;:g\&quot;&lt;/span&gt; | sudo &lt;span class=\&quot;hljs-built_in\&quot;&gt;tee&lt;/span&gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;enable, and start kubelet&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-pgsql\&quot;&gt;sudo systemctl &lt;span class=\&quot;hljs-keyword\&quot;&gt;enable&lt;/span&gt; &lt;span class=\&quot;hljs-comment\&quot;&gt;--now kubelet&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;h2&gt;Initialization&lt;/h2&gt;\n    &lt;p&gt;Install prerequesites for kubeadm&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-properties\&quot;&gt;&lt;span class=\&quot;hljs-attr\&quot;&gt;sudo&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;apt-get update &lt;/span&gt;\n&lt;span class=\&quot;hljs-attr\&quot;&gt;sudo&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;apt install ethtool socat conntrack&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;Create an update alternative&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-awk\&quot;&gt;sudo update-alternatives --install &lt;span class=\&quot;hljs-regexp\&quot;&gt;/usr/&lt;/span&gt;bin&lt;span class=\&quot;hljs-regexp\&quot;&gt;/kubeadm kubeadm $KUBERNETES_DIR/&lt;/span&gt;kubeadm &lt;span class=\&quot;hljs-number\&quot;&gt;100&lt;/span&gt;\nsudo update-alternatives --install &lt;span class=\&quot;hljs-regexp\&quot;&gt;/usr/&lt;/span&gt;bin&lt;span class=\&quot;hljs-regexp\&quot;&gt;/kubelet kubelet $KUBERNETES_DIR/&lt;/span&gt;kubelet &lt;span class=\&quot;hljs-number\&quot;&gt;100&lt;/span&gt;\nsudo update-alternatives --install &lt;span class=\&quot;hljs-regexp\&quot;&gt;/usr/&lt;/span&gt;bin&lt;span class=\&quot;hljs-regexp\&quot;&gt;/kubectl kubectl $KUBERNETES_DIR/&lt;/span&gt;kubectl &lt;span class=\&quot;hljs-number\&quot;&gt;100&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;Run @controlplane&lt;/p&gt;\n    &lt;blockquote&gt;\n      &lt;p&gt;TODO: load balancer, hostnames&lt;/p&gt;\n    &lt;/blockquote&gt;\n    &lt;p&gt;Initialize configuration such that the network is 10.10.0.0/16&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-apache\&quot;&gt;&lt;span class=\&quot;hljs-attribute\&quot;&gt;sudo&lt;/span&gt; kubeadm init --pod-network-cidr &lt;span class=\&quot;hljs-number\&quot;&gt;10.10.0.0&lt;/span&gt;/&lt;span class=\&quot;hljs-number\&quot;&gt;16&lt;/span&gt; --apiserver-advertise-address {ip}\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;For documentation, you should see something like&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-pgsql\&quot;&gt;Your Kubernetes control-plane has initialized successfully!\n\n&lt;span class=\&quot;hljs-keyword\&quot;&gt;To&lt;/span&gt; &lt;span class=\&quot;hljs-keyword\&quot;&gt;start&lt;/span&gt; &lt;span class=\&quot;hljs-keyword\&quot;&gt;using&lt;/span&gt; your &lt;span class=\&quot;hljs-keyword\&quot;&gt;cluster&lt;/span&gt;, you need &lt;span class=\&quot;hljs-keyword\&quot;&gt;to&lt;/span&gt; run the &lt;span class=\&quot;hljs-keyword\&quot;&gt;following&lt;/span&gt; &lt;span class=\&quot;hljs-keyword\&quot;&gt;as&lt;/span&gt; a regular &lt;span class=\&quot;hljs-keyword\&quot;&gt;user&lt;/span&gt;:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/&lt;span class=\&quot;hljs-keyword\&quot;&gt;admin&lt;/span&gt;.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, &lt;span class=\&quot;hljs-keyword\&quot;&gt;if&lt;/span&gt; you are the root &lt;span class=\&quot;hljs-keyword\&quot;&gt;user&lt;/span&gt;, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/&lt;span class=\&quot;hljs-keyword\&quot;&gt;admin&lt;/span&gt;.conf\n\nYou should now deploy a pod network &lt;span class=\&quot;hljs-keyword\&quot;&gt;to&lt;/span&gt; the &lt;span class=\&quot;hljs-keyword\&quot;&gt;cluster&lt;/span&gt;.\nRun \&quot;kubectl apply -f [podnetwork].yaml\&quot; &lt;span class=\&quot;hljs-keyword\&quot;&gt;with&lt;/span&gt; one &lt;span class=\&quot;hljs-keyword\&quot;&gt;of&lt;/span&gt; the &lt;span class=\&quot;hljs-keyword\&quot;&gt;options&lt;/span&gt; listed at:\n  https://kubernetes.io/docs/concepts/&lt;span class=\&quot;hljs-keyword\&quot;&gt;cluster&lt;/span&gt;-administration/addons/\n\n&lt;span class=\&quot;hljs-keyword\&quot;&gt;Then&lt;/span&gt; you can &lt;span class=\&quot;hljs-keyword\&quot;&gt;join&lt;/span&gt; &lt;span class=\&quot;hljs-keyword\&quot;&gt;any&lt;/span&gt; number &lt;span class=\&quot;hljs-keyword\&quot;&gt;of&lt;/span&gt; worker nodes &lt;span class=\&quot;hljs-keyword\&quot;&gt;by&lt;/span&gt; running the &lt;span class=\&quot;hljs-keyword\&quot;&gt;following&lt;/span&gt; &lt;span class=\&quot;hljs-keyword\&quot;&gt;on&lt;/span&gt; &lt;span class=\&quot;hljs-keyword\&quot;&gt;each&lt;/span&gt; &lt;span class=\&quot;hljs-keyword\&quot;&gt;as&lt;/span&gt; root:\n\nkubeadm &lt;span class=\&quot;hljs-keyword\&quot;&gt;join&lt;/span&gt; &lt;span class=\&quot;hljs-number\&quot;&gt;192.168&lt;/span&gt;&lt;span class=\&quot;hljs-number\&quot;&gt;.121&lt;/span&gt;&lt;span class=\&quot;hljs-number\&quot;&gt;.210&lt;/span&gt;:&lt;span class=\&quot;hljs-number\&quot;&gt;6443&lt;/span&gt; &lt;span class=\&quot;hljs-comment\&quot;&gt;--token clns4a.b29f6anjipygy0e2 \\&lt;/span&gt;\n\t&lt;span class=\&quot;hljs-comment\&quot;&gt;--discovery-token-ca-cert-hash sha256:833f599cc9ab27eb5010c499e9c77e8e3263fb991d8e9e78ef187ba97e1efb59&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;Do as it says, run&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-bash\&quot;&gt;&lt;span class=\&quot;hljs-built_in\&quot;&gt;mkdir&lt;/span&gt; -p &lt;span class=\&quot;hljs-variable\&quot;&gt;$HOME&lt;/span&gt;/.kube\nsudo &lt;span class=\&quot;hljs-built_in\&quot;&gt;cp&lt;/span&gt; -i /etc/kubernetes/admin.conf &lt;span class=\&quot;hljs-variable\&quot;&gt;$HOME&lt;/span&gt;/.kube/config\nsudo &lt;span class=\&quot;hljs-built_in\&quot;&gt;chown&lt;/span&gt; $(&lt;span class=\&quot;hljs-built_in\&quot;&gt;id&lt;/span&gt; -u):$(&lt;span class=\&quot;hljs-built_in\&quot;&gt;id&lt;/span&gt; -g) &lt;span class=\&quot;hljs-variable\&quot;&gt;$HOME&lt;/span&gt;/.kube/config\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;We will use &lt;a href=\&quot;https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/weave-network-policy/\&quot;&gt;Weave Net&lt;/a&gt; as a network plugin&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-powershell\&quot;&gt;kubectl apply &lt;span class=\&quot;hljs-operator\&quot;&gt;-f&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;\&quot;https://cloud.weave.works/k8s/net?k8s-version=&lt;span class=\&quot;hljs-variable\&quot;&gt;$&lt;/span&gt;(kubectl version | base64 | tr -d &#x27;\\n&#x27;)\&quot;&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;kubernetes/storage&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/kubernetes/storage.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Storage&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [
                    &quot;Kubernetes&quot;
                ],
                &quot;categories&quot;: [
                    &quot;Kubernetes&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\nKubernetes abstracts the storage through a __plugin layer__. The plugin layer is the interface that connects external storage with Kubernetes. The layer can mediate between any kind of storage, be it file based, bloc of object based, on-prem or cloud.\n\nMost plugins are based on the __Container Storage Interface (CSI)__ which is an open standard.\n\n&gt; TBD\n&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;p&gt;Kubernetes abstracts the storage through a &lt;strong&gt;plugin layer&lt;/strong&gt;. The plugin layer is the interface that connects external storage with Kubernetes. The layer can mediate between any kind of storage, be it file based, bloc of object based, on-prem or cloud.&lt;/p&gt;\n    &lt;p&gt;Most plugins are based on the &lt;strong&gt;Container Storage Interface (CSI)&lt;/strong&gt; which is an open standard.&lt;/p&gt;\n    &lt;blockquote&gt;\n      &lt;p&gt;TBD&lt;/p&gt;\n    &lt;/blockquote&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;kubernetes/technical-overview&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/kubernetes/technical-overview.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Kubernetes technical overview&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [
                    &quot;Kubernetes&quot;
                ],
                &quot;categories&quot;: [
                    &quot;Kubernetes&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\n## Application packaging\n\nAn application should be\n\n1. Packaged as a container\n1. Wrapped in a _Pod_\n1. Deployed via a declerative manifest file\n\n## The declerative model \n\nAccording to the _declerative model_, we only declare about how we want the application to look like. It is Kubernetes&#x27; jpb to make sure the cluster behaves as intended.\n\n_Manifests_ simple YAML files and they tell Kubernetes how the application should look like - the _desired state_.\n\n_Controllers_ are constantly running and monitor the application&#x27;s state, reconciling and difference betweeen the _observerd state_ and the _desired state_.\n\n## Pods\n\nIn Kubernetes, _Pods_ are the atomic unit of scheduling. Kubernetes demands that every container runs inside a pod.\n\nA simple model is to run a sigle container in every pod. \n\nEffectively, a _Pod_ is a construct for running one or more containers.\n\nPods are the most basic unit of scaling. We scale applications by adding or removing pods (not containers).\n\nPods are deployed atomiclly. A pod is ready only when all containers are up and running. A single pod executes on a single Node.\n\nWhen a pod dies, a new one takes it&#x27;s place. The new pod is a different instance with the same semantics, it has different id, ip etc..\n\nPods are immutable. If we want to change a pod&#x27;s configuration, we must create a new pod to take it&#x27;s place.\n\n### Pod theory\n\nThere are 3 main reasons for Pods to exist\n\n1. Pods augment containers 1. Pods assist in scheduling\n1. Pods enable resource sharing\n\nThe augmentation is done in the following ways\n\n- Labels / annotations\n- Restart policies\n- Probes (startup, readiness, liveness etc...)\n- Affinity / anti-affinity rules (Affinities are related to specifying how specific pods behave with other pods)\n- Termination control\n- Security policies\n- Resource requests and limits (min/max values on CPU, memory and I/O)\n\nPods have __Labels__ which lets us group Pods and associate them with other objects. \n\nRegarding resource sharing, Pods provide _shared execution environment_ for one or more containers. It includes\n\n- Filesystem\n- Network stack (IP address, routing, ports)\n- Memory\n- Volumes\n\nPods can be deployed either directly via a Pod manifest or indirectly via a controller. Pods deployed directly are called _static pods_.\n\n## Deployments\n\nA _Deployment_ is a higher-level controller. Usually we will deploy pods indirectly via a deployment.\n\nThe deployment controller monitors the state of a wrapped pod providing extra features such as self-healing, scaling, zero-downtime rollouts and versioned rollbacks.\n\n## Services\n\nA _Service_ is a Kubernetes contstruct which provides reliable networking for a set of pods.\n\nAs we know, pods are immutable and modifications due to auto scaling, rollbacks etc... result in replacements of pods - and the effective IPs.\n\n_Services_ provide reliable names and IPs and provide load balancing capabilities over a set of pods.\n\n## Examples of controllers\n\n- Deployments\n- DaemonSets\n- StatefulSets\n\n## Generall usefull commands\n\nList all possible Pod attributes\n\n    kubectl explain pods --recursive\n\n## Multi container patterns\n\nKubernetes offers several well-defined multi-container Pod patterns\n\n### Sidecar pattern\n\nThis pattern has a _main_ application container and a _sidecar_ container. The _sidecar&#x27;s_ job is to augment and perform secondary tasks for the _main_ application container.\n\n### Adapter pattern\n\nThis pattern is a specific variation of the _sidecar pattern_ where the _sidecar_ container takes non-standardized output from the _main_ container and standardize it as required by an external system.\n\n### Ambassador pattern\n\nThis is another variation of the _sidecar pattern_ where the _sidecar_ brokers connectivity to an external system.\n\n### Init pattern\n\nThis pattern has an _init_ container that&#x27;s gauranteed to start and complete before your _main_ application container. It is also gauranteed to run exactly once!\n&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h2&gt;Application packaging&lt;/h2&gt;\n    &lt;p&gt;An application should be&lt;/p&gt;\n    &lt;ol&gt;\n      &lt;li&gt;Packaged as a container&lt;/li&gt;\n      &lt;li&gt;Wrapped in a &lt;em&gt;Pod&lt;/em&gt;&lt;/li&gt;\n      &lt;li&gt;Deployed via a declerative manifest file&lt;/li&gt;\n    &lt;/ol&gt;\n    &lt;h2&gt;The declerative model&lt;/h2&gt;\n    &lt;p&gt;According to the &lt;em&gt;declerative model&lt;/em&gt;, we only declare about how we want the application to look like. It is Kubernetes&#x27; jpb to make sure the cluster behaves as intended.&lt;/p&gt;\n    &lt;p&gt;&lt;em&gt;Manifests&lt;/em&gt; simple YAML files and they tell Kubernetes how the application should look like - the &lt;em&gt;desired state&lt;/em&gt;.&lt;/p&gt;\n    &lt;p&gt;&lt;em&gt;Controllers&lt;/em&gt; are constantly running and monitor the application&#x27;s state, reconciling and difference betweeen the &lt;em&gt;observerd state&lt;/em&gt; and the &lt;em&gt;desired state&lt;/em&gt;.&lt;/p&gt;\n    &lt;h2&gt;Pods&lt;/h2&gt;\n    &lt;p&gt;In Kubernetes, &lt;em&gt;Pods&lt;/em&gt; are the atomic unit of scheduling. Kubernetes demands that every container runs inside a pod.&lt;/p&gt;\n    &lt;p&gt;A simple model is to run a sigle container in every pod.&lt;/p&gt;\n    &lt;p&gt;Effectively, a &lt;em&gt;Pod&lt;/em&gt; is a construct for running one or more containers.&lt;/p&gt;\n    &lt;p&gt;Pods are the most basic unit of scaling. We scale applications by adding or removing pods (not containers).&lt;/p&gt;\n    &lt;p&gt;Pods are deployed atomiclly. A pod is ready only when all containers are up and running. A single pod executes on a single Node.&lt;/p&gt;\n    &lt;p&gt;When a pod dies, a new one takes it&#x27;s place. The new pod is a different instance with the same semantics, it has different id, ip etc..&lt;/p&gt;\n    &lt;p&gt;Pods are immutable. If we want to change a pod&#x27;s configuration, we must create a new pod to take it&#x27;s place.&lt;/p&gt;\n    &lt;h3&gt;Pod theory&lt;/h3&gt;\n    &lt;p&gt;There are 3 main reasons for Pods to exist&lt;/p&gt;\n    &lt;ol&gt;\n      &lt;li&gt;Pods augment containers 1. Pods assist in scheduling&lt;/li&gt;\n      &lt;li&gt;Pods enable resource sharing&lt;/li&gt;\n    &lt;/ol&gt;\n    &lt;p&gt;The augmentation is done in the following ways&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Labels / annotations&lt;/li&gt;\n      &lt;li&gt;Restart policies&lt;/li&gt;\n      &lt;li&gt;Probes (startup, readiness, liveness etc...)&lt;/li&gt;\n      &lt;li&gt;Affinity / anti-affinity rules (Affinities are related to specifying how specific pods behave with other pods)&lt;/li&gt;\n      &lt;li&gt;Termination control&lt;/li&gt;\n      &lt;li&gt;Security policies&lt;/li&gt;\n      &lt;li&gt;Resource requests and limits (min/max values on CPU, memory and I/O)&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;Pods have &lt;strong&gt;Labels&lt;/strong&gt; which lets us group Pods and associate them with other objects.&lt;/p&gt;\n    &lt;p&gt;Regarding resource sharing, Pods provide &lt;em&gt;shared execution environment&lt;/em&gt; for one or more containers. It includes&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Filesystem&lt;/li&gt;\n      &lt;li&gt;Network stack (IP address, routing, ports)&lt;/li&gt;\n      &lt;li&gt;Memory&lt;/li&gt;\n      &lt;li&gt;Volumes&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;Pods can be deployed either directly via a Pod manifest or indirectly via a controller. Pods deployed directly are called &lt;em&gt;static pods&lt;/em&gt;.&lt;/p&gt;\n    &lt;h2&gt;Deployments&lt;/h2&gt;\n    &lt;p&gt;A &lt;em&gt;Deployment&lt;/em&gt; is a higher-level controller. Usually we will deploy pods indirectly via a deployment.&lt;/p&gt;\n    &lt;p&gt;The deployment controller monitors the state of a wrapped pod providing extra features such as self-healing, scaling, zero-downtime rollouts and versioned rollbacks.&lt;/p&gt;\n    &lt;h2&gt;Services&lt;/h2&gt;\n    &lt;p&gt;A &lt;em&gt;Service&lt;/em&gt; is a Kubernetes contstruct which provides reliable networking for a set of pods.&lt;/p&gt;\n    &lt;p&gt;As we know, pods are immutable and modifications due to auto scaling, rollbacks etc... result in replacements of pods - and the effective IPs.&lt;/p&gt;\n    &lt;p&gt;&lt;em&gt;Services&lt;/em&gt; provide reliable names and IPs and provide load balancing capabilities over a set of pods.&lt;/p&gt;\n    &lt;h2&gt;Examples of controllers&lt;/h2&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Deployments&lt;/li&gt;\n      &lt;li&gt;DaemonSets&lt;/li&gt;\n      &lt;li&gt;StatefulSets&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;h2&gt;Generall usefull commands&lt;/h2&gt;\n    &lt;p&gt;List all possible Pod attributes&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-pgsql\&quot;&gt;kubectl &lt;span class=\&quot;hljs-keyword\&quot;&gt;explain&lt;/span&gt; pods &lt;span class=\&quot;hljs-comment\&quot;&gt;--recursive&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;h2&gt;Multi container patterns&lt;/h2&gt;\n    &lt;p&gt;Kubernetes offers several well-defined multi-container Pod patterns&lt;/p&gt;\n    &lt;h3&gt;Sidecar pattern&lt;/h3&gt;\n    &lt;p&gt;This pattern has a &lt;em&gt;main&lt;/em&gt; application container and a &lt;em&gt;sidecar&lt;/em&gt; container. The &lt;em&gt;sidecar&#x27;s&lt;/em&gt; job is to augment and perform secondary tasks for the &lt;em&gt;main&lt;/em&gt; application container.&lt;/p&gt;\n    &lt;h3&gt;Adapter pattern&lt;/h3&gt;\n    &lt;p&gt;This pattern is a specific variation of the &lt;em&gt;sidecar pattern&lt;/em&gt; where the &lt;em&gt;sidecar&lt;/em&gt; container takes non-standardized output from the &lt;em&gt;main&lt;/em&gt; container and standardize it as required by an external system.&lt;/p&gt;\n    &lt;h3&gt;Ambassador pattern&lt;/h3&gt;\n    &lt;p&gt;This is another variation of the &lt;em&gt;sidecar pattern&lt;/em&gt; where the &lt;em&gt;sidecar&lt;/em&gt; brokers connectivity to an external system.&lt;/p&gt;\n    &lt;h3&gt;Init pattern&lt;/h3&gt;\n    &lt;p&gt;This pattern has an &lt;em&gt;init&lt;/em&gt; container that&#x27;s gauranteed to start and complete before your &lt;em&gt;main&lt;/em&gt; application container. It is also gauranteed to run exactly once!&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;markdown&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/markdown.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Markdown&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [],
                &quot;categories&quot;: []
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\n# Header 1\n## Header 2\n### Header 3\n#### Header 4\n##### Header 5\n###### Header 6\n\nContrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \&quot;de Finibus Bonorum et Malorum\&quot; (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, \&quot;Lorem ipsum dolor sit amet..\&quot;, comes from a line in section 1.10.32.\n\nThe standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from \&quot;de Finibus Bonorum et Malorum\&quot; by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham.\n\n&gt; This is some quote\n\n__Some Math__\n\n$\\forall h \\in H$\n\n__Ordered list__\n\n1. First Item  \n1. Second Item  \n1. Third Item  \n\n__Unordered list__\n\n- First  \n- Second  \n- Third  \n\n\n| id | name              | phone            | description      |\n|----|-------------------|------------------|------------------|\n| 1  | Ricky Phelps      | +123 123 5555555 | Some description |\n| 2  | Asha Valdez       | +123 123 5555555 |                  |\n| 3  | Katelyn Dougherty | +123 123 5555555 |                  |\n\n```python\n# Some example python code\nclass BinaryOperation:\n  def __init__(self, operation):\n    self.operation = operation\n  \n  def operate(self, x, y):\n    return self.operation(x, y)\n\n  def __call__(self, x, y):\n    return self.operate(x, y)\n```&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Header 1&lt;/h1&gt;\n    &lt;h2&gt;Header 2&lt;/h2&gt;\n    &lt;h3&gt;Header 3&lt;/h3&gt;\n    &lt;h4&gt;Header 4&lt;/h4&gt;\n    &lt;h5&gt;Header 5&lt;/h5&gt;\n    &lt;h6&gt;Header 6&lt;/h6&gt;\n    &lt;p&gt;Contrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \&quot;de Finibus Bonorum et Malorum\&quot; (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, \&quot;Lorem ipsum dolor sit amet..\&quot;, comes from a line in section 1.10.32.&lt;/p&gt;\n    &lt;p&gt;The standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from \&quot;de Finibus Bonorum et Malorum\&quot; by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham.&lt;/p&gt;\n    &lt;blockquote&gt;\n      &lt;p&gt;This is some quote&lt;/p&gt;\n    &lt;/blockquote&gt;\n    &lt;p&gt;&lt;strong&gt;Some Math&lt;/strong&gt;&lt;/p&gt;\n    &lt;p&gt;$\\forall h \\in H$&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Ordered list&lt;/strong&gt;&lt;/p&gt;\n    &lt;ol&gt;\n      &lt;li&gt;First Item&lt;/li&gt;\n      &lt;li&gt;Second Item&lt;/li&gt;\n      &lt;li&gt;Third Item&lt;/li&gt;\n    &lt;/ol&gt;\n    &lt;p&gt;&lt;strong&gt;Unordered list&lt;/strong&gt;&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;First&lt;/li&gt;\n      &lt;li&gt;Second&lt;/li&gt;\n      &lt;li&gt;Third&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;table&gt;\n      &lt;thead&gt;\n        &lt;tr&gt;\n          &lt;th&gt;id&lt;/th&gt;\n          &lt;th&gt;name&lt;/th&gt;\n          &lt;th&gt;phone&lt;/th&gt;\n          &lt;th&gt;description&lt;/th&gt;\n        &lt;/tr&gt;\n      &lt;/thead&gt;\n      &lt;tbody&gt;\n        &lt;tr&gt;\n          &lt;td&gt;1&lt;/td&gt;\n          &lt;td&gt;Ricky Phelps&lt;/td&gt;\n          &lt;td&gt;+123 123 5555555&lt;/td&gt;\n          &lt;td&gt;Some description&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td&gt;2&lt;/td&gt;\n          &lt;td&gt;Asha Valdez&lt;/td&gt;\n          &lt;td&gt;+123 123 5555555&lt;/td&gt;\n          &lt;td&gt;&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td&gt;3&lt;/td&gt;\n          &lt;td&gt;Katelyn Dougherty&lt;/td&gt;\n          &lt;td&gt;+123 123 5555555&lt;/td&gt;\n          &lt;td&gt;&lt;/td&gt;\n        &lt;/tr&gt;\n      &lt;/tbody&gt;\n    &lt;/table&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-python\&quot;&gt;&lt;span class=\&quot;hljs-comment\&quot;&gt;# Some example python code&lt;/span&gt;\n&lt;span class=\&quot;hljs-keyword\&quot;&gt;class&lt;/span&gt; &lt;span class=\&quot;hljs-title class_\&quot;&gt;BinaryOperation&lt;/span&gt;:\n  &lt;span class=\&quot;hljs-keyword\&quot;&gt;def&lt;/span&gt; &lt;span class=\&quot;hljs-title function_\&quot;&gt;__init__&lt;/span&gt;(&lt;span class=\&quot;hljs-params\&quot;&gt;self, operation&lt;/span&gt;):\n    self.operation = operation\n  \n  &lt;span class=\&quot;hljs-keyword\&quot;&gt;def&lt;/span&gt; &lt;span class=\&quot;hljs-title function_\&quot;&gt;operate&lt;/span&gt;(&lt;span class=\&quot;hljs-params\&quot;&gt;self, x, y&lt;/span&gt;):\n    &lt;span class=\&quot;hljs-keyword\&quot;&gt;return&lt;/span&gt; self.operation(x, y)\n\n  &lt;span class=\&quot;hljs-keyword\&quot;&gt;def&lt;/span&gt; &lt;span class=\&quot;hljs-title function_\&quot;&gt;__call__&lt;/span&gt;(&lt;span class=\&quot;hljs-params\&quot;&gt;self, x, y&lt;/span&gt;):\n    &lt;span class=\&quot;hljs-keyword\&quot;&gt;return&lt;/span&gt; self.operate(x, y)\n&lt;/code&gt;&lt;/pre&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;posts/archlinux-installation-guide&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/posts/archlinux-installation-guide.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Archlinux Installation Guide&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [
                    &quot;Archlinux&quot;,
                    &quot;Linux&quot;
                ],
                &quot;categories&quot;: [
                    &quot;Guides&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\n# arch boot spec\n\nthis is specific to my machine and software of choice\n- efi boot\n- disk at sdb \n- netctl as network manager in boot environment\n- networkmanager as network manager in installation\n\ni edit this file in parallel to the installation and learning process; if you want to do this as well, install git from the boot environment (or use elinks to view the file through github)\n\n## connect to network\n\n```wifi-menu``` and follow instructions\n\ncheck connectivity\n\n```ping 8.8.8.8```\n\nin ```/etc/nsswitch.conf```, at the ```hosts``` entry, make sure ```dns``` is before ```[!UNAVAIL=return]```\n\ncheck dns acquisition\n\n```ping google.com```\n\n## partition, filesystems and mount\n\nbe careful, assume each step to erase all data on partition layout of the disk!\n\nmake sure on which device you want to work on, using ```lsblk```\n\nerase labels ```wipefs -a /dev/sdb```\n\nexample partition scheme below\n\n&gt; you can use ```fdisk``` or ```cfdisk``` for example\n\npartition | size  | type             | desc\n----------|-------|------------------|----------------\nsdb1      | 550MB | EFI System       | boot partition\nsdb2      | 24GB  | Linux swap       | swap partition\nsdb3      | 32GB  | Linux filesystem  | root partition\nsdb4      | rest  | Linux filesystem  | home partition\n\nmake filesystems for the partitions\n\n```bash\nmkfs.fat -F32 /dev/sdb1\n\nmkswap /dev/sdb2\nswapon /dev/sdb2\n\nmkfs.ext4 /dev/sdb3\nmkfs.ext4 /dev/sdb4\n```\n\nmount partitions to filesystem\n\n```bash\nmount /dev/sdb3 /mnt\n\nmkdir /mnt/boot\nmkdir /mnt/boot/efi\nmkdir /mnt/home\n\nmount /dev/sdb4 /mnt/home\nmount /dev/sdb1 /mnt/boot/efi\n```\n\n## archlinux installation\n\n```bash\npacstrap /mnt \\\n    base base-devel \\\n    linux linux-firmware \\\n    grub efibootmgr \\\n    networkmanager \\\n    vim git\n```\n\n## installation setup\n\ngenerate fstab - ```genfstab -U /mnt &gt;&gt; /mnt/etc/fstab```\n\nchange root to installation - ```arch-chroot /mnt```\n\nedit file ```/etc/locale.gen``` and uncomment desired locale\n\ngenerate locale - ```locale-gen```\n\nadd set language - ```echo \&quot;LANG=en_US.UTF-8\&quot; &gt; /etc/locale.conf```\n\nset timezone -\n\n```bash\nln -sf /usr/share/zoneinfo/Asia/Jerusalem /etc/localtime\nhwclock --systohc\n```\n\nset root password for linux installation - ```passwd```\n\n## grub\n\n```bash\ngrub-install --target=x86_64-efi --efi-directory=/boot/efi\ngrub-mkconfig -o /boot/grub/grub.cfg\n```\n\n## finalize\n\nback to bootable environment\n\n```exit```\n\nunmount all partitions\n\n```umount -R /mnt```\n\n```reboot```\n&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;arch boot spec&lt;/h1&gt;\n    &lt;p&gt;this is specific to my machine and software of choice&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;efi boot&lt;/li&gt;\n      &lt;li&gt;disk at sdb&lt;/li&gt;\n      &lt;li&gt;netctl as network manager in boot environment&lt;/li&gt;\n      &lt;li&gt;networkmanager as network manager in installation&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;i edit this file in parallel to the installation and learning process; if you want to do this as well, install git from the boot environment (or use elinks to view the file through github)&lt;/p&gt;\n    &lt;h2&gt;connect to network&lt;/h2&gt;\n    &lt;p&gt;&lt;code&gt;wifi-menu&lt;/code&gt; and follow instructions&lt;/p&gt;\n    &lt;p&gt;check connectivity&lt;/p&gt;\n    &lt;p&gt;&lt;code&gt;ping 8.8.8.8&lt;/code&gt;&lt;/p&gt;\n    &lt;p&gt;in &lt;code&gt;/etc/nsswitch.conf&lt;/code&gt;, at the &lt;code&gt;hosts&lt;/code&gt; entry, make sure &lt;code&gt;dns&lt;/code&gt; is before &lt;code&gt;[!UNAVAIL=return]&lt;/code&gt;&lt;/p&gt;\n    &lt;p&gt;check dns acquisition&lt;/p&gt;\n    &lt;p&gt;&lt;code&gt;ping google.com&lt;/code&gt;&lt;/p&gt;\n    &lt;h2&gt;partition, filesystems and mount&lt;/h2&gt;\n    &lt;p&gt;be careful, assume each step to erase all data on partition layout of the disk!&lt;/p&gt;\n    &lt;p&gt;make sure on which device you want to work on, using &lt;code&gt;lsblk&lt;/code&gt;&lt;/p&gt;\n    &lt;p&gt;erase labels &lt;code&gt;wipefs -a /dev/sdb&lt;/code&gt;&lt;/p&gt;\n    &lt;p&gt;example partition scheme below&lt;/p&gt;\n    &lt;blockquote&gt;\n      &lt;p&gt;you can use &lt;code&gt;fdisk&lt;/code&gt; or &lt;code&gt;cfdisk&lt;/code&gt; for example&lt;/p&gt;\n    &lt;/blockquote&gt;\n    &lt;table&gt;\n      &lt;thead&gt;\n        &lt;tr&gt;\n          &lt;th&gt;partition&lt;/th&gt;\n          &lt;th&gt;size&lt;/th&gt;\n          &lt;th&gt;type&lt;/th&gt;\n          &lt;th&gt;desc&lt;/th&gt;\n        &lt;/tr&gt;\n      &lt;/thead&gt;\n      &lt;tbody&gt;\n        &lt;tr&gt;\n          &lt;td&gt;sdb1&lt;/td&gt;\n          &lt;td&gt;550MB&lt;/td&gt;\n          &lt;td&gt;EFI System&lt;/td&gt;\n          &lt;td&gt;boot partition&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td&gt;sdb2&lt;/td&gt;\n          &lt;td&gt;24GB&lt;/td&gt;\n          &lt;td&gt;Linux swap&lt;/td&gt;\n          &lt;td&gt;swap partition&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td&gt;sdb3&lt;/td&gt;\n          &lt;td&gt;32GB&lt;/td&gt;\n          &lt;td&gt;Linux filesystem&lt;/td&gt;\n          &lt;td&gt;root partition&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td&gt;sdb4&lt;/td&gt;\n          &lt;td&gt;rest&lt;/td&gt;\n          &lt;td&gt;Linux filesystem&lt;/td&gt;\n          &lt;td&gt;home partition&lt;/td&gt;\n        &lt;/tr&gt;\n      &lt;/tbody&gt;\n    &lt;/table&gt;\n    &lt;p&gt;make filesystems for the partitions&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-bash\&quot;&gt;mkfs.fat -F32 /dev/sdb1\n\nmkswap /dev/sdb2\nswapon /dev/sdb2\n\nmkfs.ext4 /dev/sdb3\nmkfs.ext4 /dev/sdb4\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;mount partitions to filesystem&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-bash\&quot;&gt;mount /dev/sdb3 /mnt\n\n&lt;span class=\&quot;hljs-built_in\&quot;&gt;mkdir&lt;/span&gt; /mnt/boot\n&lt;span class=\&quot;hljs-built_in\&quot;&gt;mkdir&lt;/span&gt; /mnt/boot/efi\n&lt;span class=\&quot;hljs-built_in\&quot;&gt;mkdir&lt;/span&gt; /mnt/home\n\nmount /dev/sdb4 /mnt/home\nmount /dev/sdb1 /mnt/boot/efi\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;h2&gt;archlinux installation&lt;/h2&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-bash\&quot;&gt;pacstrap /mnt \\\n    base base-devel \\\n    linux linux-firmware \\\n    grub efibootmgr \\\n    networkmanager \\\n    vim git\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;h2&gt;installation setup&lt;/h2&gt;\n    &lt;p&gt;generate fstab - &lt;code&gt;genfstab -U /mnt &gt;&gt; /mnt/etc/fstab&lt;/code&gt;&lt;/p&gt;\n    &lt;p&gt;change root to installation - &lt;code&gt;arch-chroot /mnt&lt;/code&gt;&lt;/p&gt;\n    &lt;p&gt;edit file &lt;code&gt;/etc/locale.gen&lt;/code&gt; and uncomment desired locale&lt;/p&gt;\n    &lt;p&gt;generate locale - &lt;code&gt;locale-gen&lt;/code&gt;&lt;/p&gt;\n    &lt;p&gt;add set language - &lt;code&gt;echo \&quot;LANG=en_US.UTF-8\&quot; &gt; /etc/locale.conf&lt;/code&gt;&lt;/p&gt;\n    &lt;p&gt;set timezone -&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-bash\&quot;&gt;&lt;span class=\&quot;hljs-built_in\&quot;&gt;ln&lt;/span&gt; -sf /usr/share/zoneinfo/Asia/Jerusalem /etc/localtime\nhwclock --systohc\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;set root password for linux installation - &lt;code&gt;passwd&lt;/code&gt;&lt;/p&gt;\n    &lt;h2&gt;grub&lt;/h2&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-bash\&quot;&gt;grub-install --target=x86_64-efi --efi-directory=/boot/efi\ngrub-mkconfig -o /boot/grub/grub.cfg\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;h2&gt;finalize&lt;/h2&gt;\n    &lt;p&gt;back to bootable environment&lt;/p&gt;\n    &lt;p&gt;&lt;code&gt;exit&lt;/code&gt;&lt;/p&gt;\n    &lt;p&gt;unmount all partitions&lt;/p&gt;\n    &lt;p&gt;&lt;code&gt;umount -R /mnt&lt;/code&gt;&lt;/p&gt;\n    &lt;p&gt;&lt;code&gt;reboot&lt;/code&gt;&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;posts/data-engineering-demystified-summary&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/posts/data-engineering-demystified-summary.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Data Engineering Demistified Summary&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [],
                &quot;categories&quot;: []
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\n# Demistified summary\n\nSee http://big-data-demystified.ninja\n\nSpecifically\n- http://big-data-demystified.ninja/2020/07/09/data-lake-architecture-best-practices\n\n## Using Version Control Systems\n\nEnvironment suggestions\n\n- Dev. Has Read only access of Production data.\n- Pre Production. Because some thing can only be tested against production.\n- Production.\n\n\n## Airflow Coding Guidlines\n\n**Keep it simple**. Avoid object oriented abstraction. Very hard to retrospectively understand DAGs, simplicity is king here.\n\n**Avoid using Sensors - Airflow Term?**. Unpredictabilty in production.\n\nIt is prefereable if Jobs are\n\n- Recurrentable - Running the same job again won&#x27;t change nothing\n- Debugable - can debug the job easily\n- Write after Delete - When deleting data, insert it right after, don&#x27;t do that in another stage \n**Monitoring is very important!!!**\n\nPros\n- flexibility\n- customizability\n- scale\n- cost\n\nCons\n- learning curve\n- diy\n- time to market\n- open source\n- unclear errors\n\n## Data lake\n\n1. Data quality - MonteCarlo, Great Expectations\n1. Data pipeline stability - Databand and Honey Comb\n1. Data lineage - Apache atlas, amundsen\n1. Data classification\n\n## Cleansing and Preparing data\n\nApproaches\n\n**Python parser**\n\nPros - Simple\nCons - non uniform, hard to maintain\n\n**Dataframe / Pandas**\n\nPros - Simple, Generic, Flexible\nCons - Not scalable, bounded to the RAM\n\n**ELT**\n\nPros - Simple, Generic, Peta-Scale \nCons - Requires Good SQL / Big Data understanding\n\n**Pyspark / Scala**\n\nCons\n- Hard to learn\n- Trivial for simple cases / overkill\n\nPros\n- Good for schema evolution\n\n## 3rd party - APIs, Tips and Tricks\n\n\n## 4 V&#x27;s\n\n- Volume\n- Veracity\n- Velocity\n- Variety\n\n## Triangle\n\nWhat is the criteria?\n\n```\n       Faster\n\n    /          \\\n\nCheaper  -   Simpler\n```\n&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Demistified summary&lt;/h1&gt;\n    &lt;p&gt;See &lt;a href=\&quot;http://big-data-demystified.ninja\&quot;&gt;http://big-data-demystified.ninja&lt;/a&gt;&lt;/p&gt;\n    &lt;p&gt;Specifically&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;&lt;a href=\&quot;http://big-data-demystified.ninja/2020/07/09/data-lake-architecture-best-practices\&quot;&gt;http://big-data-demystified.ninja/2020/07/09/data-lake-architecture-best-practices&lt;/a&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;h2&gt;Using Version Control Systems&lt;/h2&gt;\n    &lt;p&gt;Environment suggestions&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Dev. Has Read only access of Production data.&lt;/li&gt;\n      &lt;li&gt;Pre Production. Because some thing can only be tested against production.&lt;/li&gt;\n      &lt;li&gt;Production.&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;h2&gt;Airflow Coding Guidlines&lt;/h2&gt;\n    &lt;p&gt;&lt;strong&gt;Keep it simple&lt;/strong&gt;. Avoid object oriented abstraction. Very hard to retrospectively understand DAGs, simplicity is king here.&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Avoid using Sensors - Airflow Term?&lt;/strong&gt;. Unpredictabilty in production.&lt;/p&gt;\n    &lt;p&gt;It is prefereable if Jobs are&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Recurrentable - Running the same job again won&#x27;t change nothing&lt;/li&gt;\n      &lt;li&gt;Debugable - can debug the job easily&lt;/li&gt;\n      &lt;li&gt;\n        Write after Delete - When deleting data, insert it right after, don&#x27;t do that in another stage\n        &lt;strong&gt;Monitoring is very important!!!&lt;/strong&gt;\n      &lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;Pros&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;flexibility&lt;/li&gt;\n      &lt;li&gt;customizability&lt;/li&gt;\n      &lt;li&gt;scale&lt;/li&gt;\n      &lt;li&gt;cost&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;Cons&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;learning curve&lt;/li&gt;\n      &lt;li&gt;diy&lt;/li&gt;\n      &lt;li&gt;time to market&lt;/li&gt;\n      &lt;li&gt;open source&lt;/li&gt;\n      &lt;li&gt;unclear errors&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;h2&gt;Data lake&lt;/h2&gt;\n    &lt;ol&gt;\n      &lt;li&gt;Data quality - MonteCarlo, Great Expectations&lt;/li&gt;\n      &lt;li&gt;Data pipeline stability - Databand and Honey Comb&lt;/li&gt;\n      &lt;li&gt;Data lineage - Apache atlas, amundsen&lt;/li&gt;\n      &lt;li&gt;Data classification&lt;/li&gt;\n    &lt;/ol&gt;\n    &lt;h2&gt;Cleansing and Preparing data&lt;/h2&gt;\n    &lt;p&gt;Approaches&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Python parser&lt;/strong&gt;&lt;/p&gt;\n    &lt;p&gt;\n      Pros - Simple\n      Cons - non uniform, hard to maintain\n    &lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Dataframe / Pandas&lt;/strong&gt;&lt;/p&gt;\n    &lt;p&gt;\n      Pros - Simple, Generic, Flexible\n      Cons - Not scalable, bounded to the RAM\n    &lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;ELT&lt;/strong&gt;&lt;/p&gt;\n    &lt;p&gt;\n      Pros - Simple, Generic, Peta-Scale\n      Cons - Requires Good SQL / Big Data understanding\n    &lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Pyspark / Scala&lt;/strong&gt;&lt;/p&gt;\n    &lt;p&gt;Cons&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Hard to learn&lt;/li&gt;\n      &lt;li&gt;Trivial for simple cases / overkill&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;Pros&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Good for schema evolution&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;h2&gt;3rd party - APIs, Tips and Tricks&lt;/h2&gt;\n    &lt;h2&gt;4 V&#x27;s&lt;/h2&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Volume&lt;/li&gt;\n      &lt;li&gt;Veracity&lt;/li&gt;\n      &lt;li&gt;Velocity&lt;/li&gt;\n      &lt;li&gt;Variety&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;h2&gt;Triangle&lt;/h2&gt;\n    &lt;p&gt;What is the criteria?&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-properties\&quot;&gt;       &lt;span class=\&quot;hljs-attr\&quot;&gt;Faster&lt;/span&gt;\n\n    &lt;span class=\&quot;hljs-attr\&quot;&gt;/&lt;/span&gt;          &lt;span class=\&quot;hljs-string\&quot;&gt;\\\n&lt;/span&gt;\n&lt;span class=\&quot;hljs-attr\&quot;&gt;Cheaper&lt;/span&gt;  &lt;span class=\&quot;hljs-string\&quot;&gt;-   Simpler&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;posts/elasticsearch&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/posts/elasticsearch.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Elasticsearch&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [],
                &quot;categories&quot;: []
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\nConcepts and examples to get reminded of once in a whilei\n\n## What is Elasticsearch \n\n_Elasticsearch_ is a distributed search and analytics engine, providing a _near real-time_ search for a large array of data types, structured or un-structured.\n\nSome of the usecases\n- Backend for search boxes\n- Metrics and Log analysis\n\nElasticsearch uses _Apache Lucene_ under the hood as it&#x27;s underlying search engine.\n\n## Documents &amp; Indices\n\nTechnically, and I quote - \&quot;Elasticsearch is a distributed documented store\&quot;. It serializes any kind of structure into JSON documents. In an Elasticsearch cluster, the documents are distributed accross multiple nodes and are accessible from any (really?) of the nodes.\n\n__TODO__ Are the documents distributed accross all nodes, or a part of the cluster? It actually seems unreasonable to me that it is a full replication.\n\nElasticsearch maintains an _Inverted Index_ to provide fast search capabilities on all of a document&#x27;s fields (They say that a document is fully searchable within 1 sec). Practically, the index is the data structure the collects multiple documents and provides fast access to them according to search criteria.\n\nWe can think of the model as such\n```\nField\n    key: String\n    value: Any\n\nDocument\n    fields: Field[]\n\nIndex\n    documents: Document[]\n```\n\nBy default, Elasticsearch index all fields in all documents, and it has different mehcanisms to index different data types.\n\n## Search &amp; Aggregations\n\nElasticsearch provides a REST API that supports\n- Structured queries -  Queries that are structurely similiar to SQL queries\n- Full text queries - Queries that return all documents that match the query, sorted by relevance\n- Complex queries - A combination of the above\n\nElasticsearch can maintain aggregations of the data enabling us to analyze summaries providing insights regarding metrics, patterns and trends. For example, we can query for\n- Total numbers that match X\n- The average that match Y\n\nFurthermore, aggregations can operate alongside the search requests, meaning that we can gain aggregative information for all search results.\n\n### cat API\n\nThe ```_cat``` endpoint provides \&quot;Compact and Aligned Text\&quot; meaning, a general, __consumed by humans__ information.\n\nGet the list of cat APIs\n\n    /_cat\n\nGet information about indices using [cat indices API](https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-indices.html)\n\n    /_cat/indices\n    /_cat/indices/_all\n    /_cat/indices?format=json\n    /_cat/indices/my-index\n    /_cat/indices/my-index?format=json\n\n### search API\n\nThe ```&lt;target&gt;/_search``` endpoint provides searching functionality.\n\nThe ```&lt;target&gt;``` parameter is optional and is a comma seperated list indicating data streams, indices and aliases to search. It supports wildcards (*).\n\nThere are many query parameters that affects how the search is being performed and how the results are being returned. We won&#x27;t list them all here but here is a small list that give a vibe on the kind of possible parameters\n\n- q(string) - The query. Note that it can alternetively be provided through the body\n- explain(boolean) - If true, returns a defailed information about the score computation  \n- timeout(time units) - Sets the timeout for the request  \n- from(integer) - Starting document offset  \n- size(integer) - Number of hits to return  \n\nThe body also contains important information the important one is the \&quot;query\&quot; field.\n\n### shards\n\nShow shards\n\n    /_cat/shards\n    /_cat/shards?h=index,shard\n&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;p&gt;Concepts and examples to get reminded of once in a whilei&lt;/p&gt;\n    &lt;h2&gt;What is Elasticsearch&lt;/h2&gt;\n    &lt;p&gt;&lt;em&gt;Elasticsearch&lt;/em&gt; is a distributed search and analytics engine, providing a &lt;em&gt;near real-time&lt;/em&gt; search for a large array of data types, structured or un-structured.&lt;/p&gt;\n    &lt;p&gt;Some of the usecases&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Backend for search boxes&lt;/li&gt;\n      &lt;li&gt;Metrics and Log analysis&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;Elasticsearch uses &lt;em&gt;Apache Lucene&lt;/em&gt; under the hood as it&#x27;s underlying search engine.&lt;/p&gt;\n    &lt;h2&gt;Documents &amp;#x26; Indices&lt;/h2&gt;\n    &lt;p&gt;Technically, and I quote - \&quot;Elasticsearch is a distributed documented store\&quot;. It serializes any kind of structure into JSON documents. In an Elasticsearch cluster, the documents are distributed accross multiple nodes and are accessible from any (really?) of the nodes.&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;TODO&lt;/strong&gt; Are the documents distributed accross all nodes, or a part of the cluster? It actually seems unreasonable to me that it is a full replication.&lt;/p&gt;\n    &lt;p&gt;Elasticsearch maintains an &lt;em&gt;Inverted Index&lt;/em&gt; to provide fast search capabilities on all of a document&#x27;s fields (They say that a document is fully searchable within 1 sec). Practically, the index is the data structure the collects multiple documents and provides fast access to them according to search criteria.&lt;/p&gt;\n    &lt;p&gt;We can think of the model as such&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-properties\&quot;&gt;&lt;span class=\&quot;hljs-attr\&quot;&gt;Field&lt;/span&gt;\n    &lt;span class=\&quot;hljs-attr\&quot;&gt;key&lt;/span&gt;: &lt;span class=\&quot;hljs-string\&quot;&gt;String&lt;/span&gt;\n    &lt;span class=\&quot;hljs-attr\&quot;&gt;value&lt;/span&gt;: &lt;span class=\&quot;hljs-string\&quot;&gt;Any&lt;/span&gt;\n\n&lt;span class=\&quot;hljs-attr\&quot;&gt;Document&lt;/span&gt;\n    &lt;span class=\&quot;hljs-attr\&quot;&gt;fields&lt;/span&gt;: &lt;span class=\&quot;hljs-string\&quot;&gt;Field[]&lt;/span&gt;\n\n&lt;span class=\&quot;hljs-attr\&quot;&gt;Index&lt;/span&gt;\n    &lt;span class=\&quot;hljs-attr\&quot;&gt;documents&lt;/span&gt;: &lt;span class=\&quot;hljs-string\&quot;&gt;Document[]&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;By default, Elasticsearch index all fields in all documents, and it has different mehcanisms to index different data types.&lt;/p&gt;\n    &lt;h2&gt;Search &amp;#x26; Aggregations&lt;/h2&gt;\n    &lt;p&gt;Elasticsearch provides a REST API that supports&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Structured queries - Queries that are structurely similiar to SQL queries&lt;/li&gt;\n      &lt;li&gt;Full text queries - Queries that return all documents that match the query, sorted by relevance&lt;/li&gt;\n      &lt;li&gt;Complex queries - A combination of the above&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;Elasticsearch can maintain aggregations of the data enabling us to analyze summaries providing insights regarding metrics, patterns and trends. For example, we can query for&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Total numbers that match X&lt;/li&gt;\n      &lt;li&gt;The average that match Y&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;Furthermore, aggregations can operate alongside the search requests, meaning that we can gain aggregative information for all search results.&lt;/p&gt;\n    &lt;h3&gt;cat API&lt;/h3&gt;\n    &lt;p&gt;The &lt;code&gt;_cat&lt;/code&gt; endpoint provides \&quot;Compact and Aligned Text\&quot; meaning, a general, &lt;strong&gt;consumed by humans&lt;/strong&gt; information.&lt;/p&gt;\n    &lt;p&gt;Get the list of cat APIs&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-bash\&quot;&gt;/_cat\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;Get information about indices using &lt;a href=\&quot;https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-indices.html\&quot;&gt;cat indices API&lt;/a&gt;&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-awk\&quot;&gt;&lt;span class=\&quot;hljs-regexp\&quot;&gt;/_cat/i&lt;/span&gt;ndices\n&lt;span class=\&quot;hljs-regexp\&quot;&gt;/_cat/i&lt;/span&gt;ndices/_all\n&lt;span class=\&quot;hljs-regexp\&quot;&gt;/_cat/i&lt;/span&gt;ndices?format=json\n&lt;span class=\&quot;hljs-regexp\&quot;&gt;/_cat/i&lt;/span&gt;ndices/my-index\n&lt;span class=\&quot;hljs-regexp\&quot;&gt;/_cat/i&lt;/span&gt;ndices/my-index?format=json\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;h3&gt;search API&lt;/h3&gt;\n    &lt;p&gt;The &lt;code&gt;&amp;#x3C;target&gt;/_search&lt;/code&gt; endpoint provides searching functionality.&lt;/p&gt;\n    &lt;p&gt;The &lt;code&gt;&amp;#x3C;target&gt;&lt;/code&gt; parameter is optional and is a comma seperated list indicating data streams, indices and aliases to search. It supports wildcards (*).&lt;/p&gt;\n    &lt;p&gt;There are many query parameters that affects how the search is being performed and how the results are being returned. We won&#x27;t list them all here but here is a small list that give a vibe on the kind of possible parameters&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;q(string) - The query. Note that it can alternetively be provided through the body&lt;/li&gt;\n      &lt;li&gt;explain(boolean) - If true, returns a defailed information about the score computation&lt;/li&gt;\n      &lt;li&gt;timeout(time units) - Sets the timeout for the request&lt;/li&gt;\n      &lt;li&gt;from(integer) - Starting document offset&lt;/li&gt;\n      &lt;li&gt;size(integer) - Number of hits to return&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;The body also contains important information the important one is the \&quot;query\&quot; field.&lt;/p&gt;\n    &lt;h3&gt;shards&lt;/h3&gt;\n    &lt;p&gt;Show shards&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-awk\&quot;&gt;&lt;span class=\&quot;hljs-regexp\&quot;&gt;/_cat/&lt;/span&gt;shards\n&lt;span class=\&quot;hljs-regexp\&quot;&gt;/_cat/&lt;/span&gt;shards?h=index,shard\n&lt;/code&gt;&lt;/pre&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;posts/emacs-cheetsheet&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/posts/emacs-cheetsheet.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Emacs Cheatsheet&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [
                    &quot;Emacs&quot;
                ],
                &quot;categories&quot;: [
                    &quot;Cheatsheets&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\n# Overview \n\nSome tasks or workflows are very mature through emacs and less on other enviornments, for example using clojure repl, editing latex files, scheme work etc...\n\nCreated this file because I use vim in my day to I keep forgetting basic emacs stuff\n\n# emacs\n\n#### open emacs in the terminal emulator\n\n    emacs -nw\n\n#### configuration path\n\n    ~/emacs.d/\n\n# help\n\nIt&#x27;s important to know how to get help and use the documentations within emacs\n\nEnter tutorial\n\n    C-h t\n\nEnter package documentation\n\n    C-h i d m {package}\n\n# basic keys\n\n## core\n\n    C-x C-x        exit\n    C-g            abort command\n\n## navigation\n\n    C-l     Center text around the cursor\n    \n    C-v     Scroll to next screenful\n    M-v     Scroll to previous screenful\n\n    M-f     Move forward a word\n    M-b     Move backward a word\n    \n    C-n     Move to next line\n    C-p     Move to previous line\n    \n    C-a     Move to beginning of line\n    C-e     Move to end of line\n    \n    M-a     Move back to beginning of sentence\n    M-e     Move forward to end of sentence\n\n    M-&lt;     Move to beginning of the document\n    M-&gt;     Move to end of the document\n\n    C-s     Initiate search mode\n\n## windowing\n\n    C-x 1   Delete all windows except focused\n    C-x 2   Split current window horizontally\n    C-x 3   Split current window verticall\n    C-x o   Move to other window\n\n## editing\n\n    C-k     Delete from cursor to end of line\n    M-k     Delete from cursor to end of sentence\n    C-_     Undo\n    C-w     Cut selected region\n    M-w     Copy selected region\n    C-y     Paste\n\n## files and buffers\n\n    C-s         Save current file\n    C-x k       Kill buffer\n    C-x b       Switch to buffer\n    C-x C-b     Lits buffers&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Overview&lt;/h1&gt;\n    &lt;p&gt;Some tasks or workflows are very mature through emacs and less on other enviornments, for example using clojure repl, editing latex files, scheme work etc...&lt;/p&gt;\n    &lt;p&gt;Created this file because I use vim in my day to I keep forgetting basic emacs stuff&lt;/p&gt;\n    &lt;h1&gt;emacs&lt;/h1&gt;\n    &lt;h4&gt;open emacs in the terminal emulator&lt;/h4&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-properties\&quot;&gt;&lt;span class=\&quot;hljs-attr\&quot;&gt;emacs&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;-nw&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;h4&gt;configuration path&lt;/h4&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-arcade\&quot;&gt;~&lt;span class=\&quot;hljs-regexp\&quot;&gt;/emacs.d/&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;h1&gt;help&lt;/h1&gt;\n    &lt;p&gt;It&#x27;s important to know how to get help and use the documentations within emacs&lt;/p&gt;\n    &lt;p&gt;Enter tutorial&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-properties\&quot;&gt;&lt;span class=\&quot;hljs-attr\&quot;&gt;C-h&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;t&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;Enter package documentation&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-properties\&quot;&gt;&lt;span class=\&quot;hljs-attr\&quot;&gt;C-h&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;i d m {package}&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;h1&gt;basic keys&lt;/h1&gt;\n    &lt;h2&gt;core&lt;/h2&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-awk\&quot;&gt;C-x C-x        &lt;span class=\&quot;hljs-keyword\&quot;&gt;exit&lt;/span&gt;\nC-g            abort command\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;h2&gt;navigation&lt;/h2&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-properties\&quot;&gt;&lt;span class=\&quot;hljs-attr\&quot;&gt;C-l&lt;/span&gt;     &lt;span class=\&quot;hljs-string\&quot;&gt;Center text around the cursor&lt;/span&gt;\n\n&lt;span class=\&quot;hljs-attr\&quot;&gt;C-v&lt;/span&gt;     &lt;span class=\&quot;hljs-string\&quot;&gt;Scroll to next screenful&lt;/span&gt;\n&lt;span class=\&quot;hljs-attr\&quot;&gt;M-v&lt;/span&gt;     &lt;span class=\&quot;hljs-string\&quot;&gt;Scroll to previous screenful&lt;/span&gt;\n\n&lt;span class=\&quot;hljs-attr\&quot;&gt;M-f&lt;/span&gt;     &lt;span class=\&quot;hljs-string\&quot;&gt;Move forward a word&lt;/span&gt;\n&lt;span class=\&quot;hljs-attr\&quot;&gt;M-b&lt;/span&gt;     &lt;span class=\&quot;hljs-string\&quot;&gt;Move backward a word&lt;/span&gt;\n\n&lt;span class=\&quot;hljs-attr\&quot;&gt;C-n&lt;/span&gt;     &lt;span class=\&quot;hljs-string\&quot;&gt;Move to next line&lt;/span&gt;\n&lt;span class=\&quot;hljs-attr\&quot;&gt;C-p&lt;/span&gt;     &lt;span class=\&quot;hljs-string\&quot;&gt;Move to previous line&lt;/span&gt;\n\n&lt;span class=\&quot;hljs-attr\&quot;&gt;C-a&lt;/span&gt;     &lt;span class=\&quot;hljs-string\&quot;&gt;Move to beginning of line&lt;/span&gt;\n&lt;span class=\&quot;hljs-attr\&quot;&gt;C-e&lt;/span&gt;     &lt;span class=\&quot;hljs-string\&quot;&gt;Move to end of line&lt;/span&gt;\n\n&lt;span class=\&quot;hljs-attr\&quot;&gt;M-a&lt;/span&gt;     &lt;span class=\&quot;hljs-string\&quot;&gt;Move back to beginning of sentence&lt;/span&gt;\n&lt;span class=\&quot;hljs-attr\&quot;&gt;M-e&lt;/span&gt;     &lt;span class=\&quot;hljs-string\&quot;&gt;Move forward to end of sentence&lt;/span&gt;\n\n&lt;span class=\&quot;hljs-attr\&quot;&gt;M-&amp;#x3C;&lt;/span&gt;     &lt;span class=\&quot;hljs-string\&quot;&gt;Move to beginning of the document&lt;/span&gt;\n&lt;span class=\&quot;hljs-attr\&quot;&gt;M-&gt;&lt;/span&gt;     &lt;span class=\&quot;hljs-string\&quot;&gt;Move to end of the document&lt;/span&gt;\n\n&lt;span class=\&quot;hljs-attr\&quot;&gt;C-s&lt;/span&gt;     &lt;span class=\&quot;hljs-string\&quot;&gt;Initiate search mode&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;h2&gt;windowing&lt;/h2&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-pgsql\&quot;&gt;C-x &lt;span class=\&quot;hljs-number\&quot;&gt;1&lt;/span&gt;   &lt;span class=\&quot;hljs-keyword\&quot;&gt;Delete&lt;/span&gt; &lt;span class=\&quot;hljs-keyword\&quot;&gt;all&lt;/span&gt; windows &lt;span class=\&quot;hljs-keyword\&quot;&gt;except&lt;/span&gt; focused\nC-x &lt;span class=\&quot;hljs-number\&quot;&gt;2&lt;/span&gt;   Split &lt;span class=\&quot;hljs-keyword\&quot;&gt;current&lt;/span&gt; &lt;span class=\&quot;hljs-keyword\&quot;&gt;window&lt;/span&gt; horizontally\nC-x &lt;span class=\&quot;hljs-number\&quot;&gt;3&lt;/span&gt;   Split &lt;span class=\&quot;hljs-keyword\&quot;&gt;current&lt;/span&gt; &lt;span class=\&quot;hljs-keyword\&quot;&gt;window&lt;/span&gt; verticall\nC-x o   &lt;span class=\&quot;hljs-keyword\&quot;&gt;Move&lt;/span&gt; &lt;span class=\&quot;hljs-keyword\&quot;&gt;to&lt;/span&gt; other &lt;span class=\&quot;hljs-keyword\&quot;&gt;window&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;h2&gt;editing&lt;/h2&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-pgsql\&quot;&gt;C-k     &lt;span class=\&quot;hljs-keyword\&quot;&gt;Delete&lt;/span&gt; &lt;span class=\&quot;hljs-keyword\&quot;&gt;from&lt;/span&gt; &lt;span class=\&quot;hljs-keyword\&quot;&gt;cursor&lt;/span&gt; &lt;span class=\&quot;hljs-keyword\&quot;&gt;to&lt;/span&gt; &lt;span class=\&quot;hljs-keyword\&quot;&gt;end&lt;/span&gt; &lt;span class=\&quot;hljs-keyword\&quot;&gt;of&lt;/span&gt; &lt;span class=\&quot;hljs-type\&quot;&gt;line&lt;/span&gt;\nM-k     &lt;span class=\&quot;hljs-keyword\&quot;&gt;Delete&lt;/span&gt; &lt;span class=\&quot;hljs-keyword\&quot;&gt;from&lt;/span&gt; &lt;span class=\&quot;hljs-keyword\&quot;&gt;cursor&lt;/span&gt; &lt;span class=\&quot;hljs-keyword\&quot;&gt;to&lt;/span&gt; &lt;span class=\&quot;hljs-keyword\&quot;&gt;end&lt;/span&gt; &lt;span class=\&quot;hljs-keyword\&quot;&gt;of&lt;/span&gt; sentence\nC-_     Undo\nC-w     Cut selected region\nM-w     &lt;span class=\&quot;hljs-keyword\&quot;&gt;Copy&lt;/span&gt; selected region\nC-y     Paste\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;h2&gt;files and buffers&lt;/h2&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-properties\&quot;&gt;&lt;span class=\&quot;hljs-attr\&quot;&gt;C-s&lt;/span&gt;         &lt;span class=\&quot;hljs-string\&quot;&gt;Save current file&lt;/span&gt;\n&lt;span class=\&quot;hljs-attr\&quot;&gt;C-x&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;k       Kill buffer&lt;/span&gt;\n&lt;span class=\&quot;hljs-attr\&quot;&gt;C-x&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;b       Switch to buffer&lt;/span&gt;\n&lt;span class=\&quot;hljs-attr\&quot;&gt;C-x&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;C-b     Lits buffers&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;posts/kafka&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/posts/kafka.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Kafka&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [],
                &quot;categories&quot;: []
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\n# Overview\n\n## What is kafka?\n\n&gt; Kafka is a streamsing platform for ingesting, storing, accessing and processing streams of data\n\n# High level concepts\n\n## Communication model\n\nOn the contrary of a **Directed Communication** where every service is aware of the other process to send messages to, kafka provides a centralized hub in which processes can send and receieve messages without being aware of each other. Such communication model is known as the **Publish/Subscribe** model which enhance the decoupling between different processes.\n\n&gt; Of course those are not termed by kafka\n\n&gt; Recall the different communication formalization according the sync/async and persistent/transient dimensions. Kafka fills conforms to the asynchronous and persisted model. This is unintuitive since this model is usually used for classical **Queues** rather than **Pubsubs**.\n\n## Core components\n\nA **Topic** is a named stream of data (/ channel - CSP?).\n\n**Producer**s are processes that publish data to a Topic.\n\n**Consumer**s are processes that subscribe to data in one or more Topics.\n\nA **Consumer Group** is a set of Consumers that work together as a group.\n\n## Storage\n\n### Commit Log\n\nA **Commit Log** is an append only data structure which contain an *Ordered Sequence* of events (/records)\n\n- Records in the log are immutable\n- Records are ordered and their ordinal is known as their **Offset**\n\n&gt; It is very similiar to existing transaction logs (Redo Log especially) which can be found in classical Relational Databases.\n\n&gt; To gain perspectives we can find similiarities to STM concurrency mode, Redux stores etc...\n\n### Partitions\n\nIn order to provide distribution capabilities, there is no 1 to 1 correlation of Topic to Log since then each Topic would need to be stored on a single machine. Rather, a Topic is broken into smaller units called **Partitions**.\n\nEffectively, every Partition is a single Log. Such model allows kafka to distribute a Topic with N partitions to at most N workers and we can have K Consumers under the same Consumer Group sharing the load of consumption as long as LEQ(K, N). If K&gt;N than it means that some consumers will be idle.\n\nIt makes sense from a technical POV. Kafka simply assigns a Partition to a Consumer.\n\n- If the number of Consumers is less than the number of Partitions, by the pigenhole principle there is at least a single Consumers that is assigned with more than one partition\n- If the number of Consumers is greather than the number of Partitions, there cannot be a surjective mapping from the Partitions the Consumers\n\n### Event\n\nA unit of data in a Topic has many interchangeable names - Event, Message, Record. We favour the term Event.\n\n&gt; Out of the kafka scope, events in the fotware world are *Entities* that desribe something that *happened in the past*. Therefore, event are past tense verbse. Well, it is the same in kafka\n\nAn **Event** is a timestamped key-value pair the records something that happened.\n\nEvent is composed of **Headers**, **Keys**, a **Timestamp** and a **Value**. We will cover some of those at a later time but for a very brief overview\n\n**Headers** contain optional metadata about the Event. One can use them to annotate, monitor and audit and Event. This is very like HTTP headers.\n\n**Keys** are optional values that affect how data is distributed accross partitions (Again, this is known concept, think of Partition Keys and partitioning schemes in different distributed systems). We will explore this concept independently.\n\n**Timestamp** holds information about when the thing that happened hapenned. There are multiple semantics of time we can speak of - When the Event created? When was it ingested? When was it processed? This is a whole topic of it&#x27;s own which we will explore independantly.\n\n**Value** is the content of the message. Practically this is just a byte array and should be serialized according to the application.\n\n## The Cluster\n\nTODO: cover concepts below\n\n**Broker** TODO\n\n**Replication** TODO\n\n**Leader** TODO\n\n**Follower** TODO\n&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Overview&lt;/h1&gt;\n    &lt;h2&gt;What is kafka?&lt;/h2&gt;\n    &lt;blockquote&gt;\n      &lt;p&gt;Kafka is a streamsing platform for ingesting, storing, accessing and processing streams of data&lt;/p&gt;\n    &lt;/blockquote&gt;\n    &lt;h1&gt;High level concepts&lt;/h1&gt;\n    &lt;h2&gt;Communication model&lt;/h2&gt;\n    &lt;p&gt;On the contrary of a &lt;strong&gt;Directed Communication&lt;/strong&gt; where every service is aware of the other process to send messages to, kafka provides a centralized hub in which processes can send and receieve messages without being aware of each other. Such communication model is known as the &lt;strong&gt;Publish/Subscribe&lt;/strong&gt; model which enhance the decoupling between different processes.&lt;/p&gt;\n    &lt;blockquote&gt;\n      &lt;p&gt;Of course those are not termed by kafka&lt;/p&gt;\n    &lt;/blockquote&gt;\n    &lt;blockquote&gt;\n      &lt;p&gt;Recall the different communication formalization according the sync/async and persistent/transient dimensions. Kafka fills conforms to the asynchronous and persisted model. This is unintuitive since this model is usually used for classical &lt;strong&gt;Queues&lt;/strong&gt; rather than &lt;strong&gt;Pubsubs&lt;/strong&gt;.&lt;/p&gt;\n    &lt;/blockquote&gt;\n    &lt;h2&gt;Core components&lt;/h2&gt;\n    &lt;p&gt;A &lt;strong&gt;Topic&lt;/strong&gt; is a named stream of data (/ channel - CSP?).&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Producer&lt;/strong&gt;s are processes that publish data to a Topic.&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Consumer&lt;/strong&gt;s are processes that subscribe to data in one or more Topics.&lt;/p&gt;\n    &lt;p&gt;A &lt;strong&gt;Consumer Group&lt;/strong&gt; is a set of Consumers that work together as a group.&lt;/p&gt;\n    &lt;h2&gt;Storage&lt;/h2&gt;\n    &lt;h3&gt;Commit Log&lt;/h3&gt;\n    &lt;p&gt;A &lt;strong&gt;Commit Log&lt;/strong&gt; is an append only data structure which contain an &lt;em&gt;Ordered Sequence&lt;/em&gt; of events (/records)&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Records in the log are immutable&lt;/li&gt;\n      &lt;li&gt;Records are ordered and their ordinal is known as their &lt;strong&gt;Offset&lt;/strong&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;blockquote&gt;\n      &lt;p&gt;It is very similiar to existing transaction logs (Redo Log especially) which can be found in classical Relational Databases.&lt;/p&gt;\n    &lt;/blockquote&gt;\n    &lt;blockquote&gt;\n      &lt;p&gt;To gain perspectives we can find similiarities to STM concurrency mode, Redux stores etc...&lt;/p&gt;\n    &lt;/blockquote&gt;\n    &lt;h3&gt;Partitions&lt;/h3&gt;\n    &lt;p&gt;In order to provide distribution capabilities, there is no 1 to 1 correlation of Topic to Log since then each Topic would need to be stored on a single machine. Rather, a Topic is broken into smaller units called &lt;strong&gt;Partitions&lt;/strong&gt;.&lt;/p&gt;\n    &lt;p&gt;Effectively, every Partition is a single Log. Such model allows kafka to distribute a Topic with N partitions to at most N workers and we can have K Consumers under the same Consumer Group sharing the load of consumption as long as LEQ(K, N). If K&gt;N than it means that some consumers will be idle.&lt;/p&gt;\n    &lt;p&gt;It makes sense from a technical POV. Kafka simply assigns a Partition to a Consumer.&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;If the number of Consumers is less than the number of Partitions, by the pigenhole principle there is at least a single Consumers that is assigned with more than one partition&lt;/li&gt;\n      &lt;li&gt;If the number of Consumers is greather than the number of Partitions, there cannot be a surjective mapping from the Partitions the Consumers&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;h3&gt;Event&lt;/h3&gt;\n    &lt;p&gt;A unit of data in a Topic has many interchangeable names - Event, Message, Record. We favour the term Event.&lt;/p&gt;\n    &lt;blockquote&gt;\n      &lt;p&gt;Out of the kafka scope, events in the fotware world are &lt;em&gt;Entities&lt;/em&gt; that desribe something that &lt;em&gt;happened in the past&lt;/em&gt;. Therefore, event are past tense verbse. Well, it is the same in kafka&lt;/p&gt;\n    &lt;/blockquote&gt;\n    &lt;p&gt;An &lt;strong&gt;Event&lt;/strong&gt; is a timestamped key-value pair the records something that happened.&lt;/p&gt;\n    &lt;p&gt;Event is composed of &lt;strong&gt;Headers&lt;/strong&gt;, &lt;strong&gt;Keys&lt;/strong&gt;, a &lt;strong&gt;Timestamp&lt;/strong&gt; and a &lt;strong&gt;Value&lt;/strong&gt;. We will cover some of those at a later time but for a very brief overview&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Headers&lt;/strong&gt; contain optional metadata about the Event. One can use them to annotate, monitor and audit and Event. This is very like HTTP headers.&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Keys&lt;/strong&gt; are optional values that affect how data is distributed accross partitions (Again, this is known concept, think of Partition Keys and partitioning schemes in different distributed systems). We will explore this concept independently.&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Timestamp&lt;/strong&gt; holds information about when the thing that happened hapenned. There are multiple semantics of time we can speak of - When the Event created? When was it ingested? When was it processed? This is a whole topic of it&#x27;s own which we will explore independantly.&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Value&lt;/strong&gt; is the content of the message. Practically this is just a byte array and should be serialized according to the application.&lt;/p&gt;\n    &lt;h2&gt;The Cluster&lt;/h2&gt;\n    &lt;p&gt;TODO: cover concepts below&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Broker&lt;/strong&gt; TODO&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Replication&lt;/strong&gt; TODO&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Leader&lt;/strong&gt; TODO&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Follower&lt;/strong&gt; TODO&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;posts/mariadb-notes&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/posts/mariadb-notes.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;MariaDB Notes&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [
                    &quot;MariaDB&quot;
                ],
                &quot;categories&quot;: [
                    &quot;Databases&quot;,
                    &quot;Notes&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\nEverything is well documented in the MySQL/MariaDB documentations, but i&#x27;ll gather some stuff here which I frequently encounter.\n\n## variables inspection\n\nrun ```mysqladmin variables```\n\n## notable variables/configurations\n\n__data_dir__ - the data directory\n\n&gt; usually ```/var/lib/mysql```\n\n__lower_case_table_names__ - as the name suggests, make all tables lower case.\n\n__local_infile__ - enable loading tables from local files.\n\n&gt; for commands such as: ```mysqlimport --local --fields-terminated-by=\&quot;|\&quot; -h localhost some-file.csv```\n\n## configuration files\n\nat least on the machine i am currently at, the root configuration is at ```/etc/my.cnf```. in turn it includes ```/etc/my.cnf.d```.\n\nin there, ill edit configurations in ```/etc/my.cnf.d/server.cnf```.\n\nfor example; to have all tables names with lower case and enable local in files, ill add the following under ```[mysqld]```\n- lower_case_table_names=1\n- local_infile=1\n\n## datadir notes\n\nwe can check where it is configured to be at using the variables inspection above, but usually it is at ```/var/lib/mysql```.\n\nit should be owned by the user/group running the process; those are usually mysql/mysql, but we can check this as well. assuming we start the process using ```systemd```, open the service file, and check what are the ```User``` and ```Group``` configured to be. the service file on my machine is at ```/usr/lib/systemd/system/mariadb.service```\n\n## complete reboot\n\nstop mariadb service - ```systemctl stop mariadb```\n\ndelete contents of datadir -```rm -rf /var/lib/mysql/*```\n\nremake infrastructure files - ```mysql_install_db --user=mysql --ldata=/var/lib/mysql```\n\nstart mariadb service - ```systemctl start mariadb```\n\n&gt; when we need such a thing?\n&gt; for example; there are configurations that only apply on installation, such as ```lower_case_table_names```\n&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;p&gt;Everything is well documented in the MySQL/MariaDB documentations, but i&#x27;ll gather some stuff here which I frequently encounter.&lt;/p&gt;\n    &lt;h2&gt;variables inspection&lt;/h2&gt;\n    &lt;p&gt;run &lt;code&gt;mysqladmin variables&lt;/code&gt;&lt;/p&gt;\n    &lt;h2&gt;notable variables/configurations&lt;/h2&gt;\n    &lt;p&gt;&lt;strong&gt;data_dir&lt;/strong&gt; - the data directory&lt;/p&gt;\n    &lt;blockquote&gt;\n      &lt;p&gt;usually &lt;code&gt;/var/lib/mysql&lt;/code&gt;&lt;/p&gt;\n    &lt;/blockquote&gt;\n    &lt;p&gt;&lt;strong&gt;lower_case_table_names&lt;/strong&gt; - as the name suggests, make all tables lower case.&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;local_infile&lt;/strong&gt; - enable loading tables from local files.&lt;/p&gt;\n    &lt;blockquote&gt;\n      &lt;p&gt;for commands such as: &lt;code&gt;mysqlimport --local --fields-terminated-by=\&quot;|\&quot; -h localhost some-file.csv&lt;/code&gt;&lt;/p&gt;\n    &lt;/blockquote&gt;\n    &lt;h2&gt;configuration files&lt;/h2&gt;\n    &lt;p&gt;at least on the machine i am currently at, the root configuration is at &lt;code&gt;/etc/my.cnf&lt;/code&gt;. in turn it includes &lt;code&gt;/etc/my.cnf.d&lt;/code&gt;.&lt;/p&gt;\n    &lt;p&gt;in there, ill edit configurations in &lt;code&gt;/etc/my.cnf.d/server.cnf&lt;/code&gt;.&lt;/p&gt;\n    &lt;p&gt;for example; to have all tables names with lower case and enable local in files, ill add the following under &lt;code&gt;[mysqld]&lt;/code&gt;&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;lower_case_table_names=1&lt;/li&gt;\n      &lt;li&gt;local_infile=1&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;h2&gt;datadir notes&lt;/h2&gt;\n    &lt;p&gt;we can check where it is configured to be at using the variables inspection above, but usually it is at &lt;code&gt;/var/lib/mysql&lt;/code&gt;.&lt;/p&gt;\n    &lt;p&gt;it should be owned by the user/group running the process; those are usually mysql/mysql, but we can check this as well. assuming we start the process using &lt;code&gt;systemd&lt;/code&gt;, open the service file, and check what are the &lt;code&gt;User&lt;/code&gt; and &lt;code&gt;Group&lt;/code&gt; configured to be. the service file on my machine is at &lt;code&gt;/usr/lib/systemd/system/mariadb.service&lt;/code&gt;&lt;/p&gt;\n    &lt;h2&gt;complete reboot&lt;/h2&gt;\n    &lt;p&gt;stop mariadb service - &lt;code&gt;systemctl stop mariadb&lt;/code&gt;&lt;/p&gt;\n    &lt;p&gt;delete contents of datadir -&lt;code&gt;rm -rf /var/lib/mysql/*&lt;/code&gt;&lt;/p&gt;\n    &lt;p&gt;remake infrastructure files - &lt;code&gt;mysql_install_db --user=mysql --ldata=/var/lib/mysql&lt;/code&gt;&lt;/p&gt;\n    &lt;p&gt;start mariadb service - &lt;code&gt;systemctl start mariadb&lt;/code&gt;&lt;/p&gt;\n    &lt;blockquote&gt;\n      &lt;p&gt;\n        when we need such a thing?\n        for example; there are configurations that only apply on installation, such as &lt;code&gt;lower_case_table_names&lt;/code&gt;\n      &lt;/p&gt;\n    &lt;/blockquote&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;posts/pacman&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/posts/pacman.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Pacman&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [],
                &quot;categories&quot;: []
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\n# Pacman\n\nFocues on esoteric usages I sometimes do, instead of re-searching\n\n&gt; Perhaps do a single packages for all (apk, pacman, apt)?\n\n__Purge Package__\n\n    pacman -Rns {package}\n\nThe most common use case is to just delete a package {package}, removing all it&#x27;s configurations and dependencies.\n\n- R is the _Remove_ operation\n- n flag indicates to remove the package&#x27;s configuration\n- s flag indicates to remove unnecessary dependencies\n\n\n__List Orphans__\n\n    pacman -Qdt\n\n- Q is the _Query_ action\n- d filters only depnedencies\n- t filters only those who are unrequired\n\n__Delete orphans__\n\n    pacman -Rns $(pacman -Qdtq)\n\nNotice that we added a _q_ option to the _List Orphans_ command, it prints less information. Specifically, it makes it so the _Query_ operation will only return the package name. That way, we can easily pass it as an argument to the _Remove_ operation\n&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Pacman&lt;/h1&gt;\n    &lt;p&gt;Focues on esoteric usages I sometimes do, instead of re-searching&lt;/p&gt;\n    &lt;blockquote&gt;\n      &lt;p&gt;Perhaps do a single packages for all (apk, pacman, apt)?&lt;/p&gt;\n    &lt;/blockquote&gt;\n    &lt;p&gt;&lt;strong&gt;Purge Package&lt;/strong&gt;&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-properties\&quot;&gt;&lt;span class=\&quot;hljs-attr\&quot;&gt;pacman&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;-Rns {package}&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;The most common use case is to just delete a package {package}, removing all it&#x27;s configurations and dependencies.&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;R is the &lt;em&gt;Remove&lt;/em&gt; operation&lt;/li&gt;\n      &lt;li&gt;n flag indicates to remove the package&#x27;s configuration&lt;/li&gt;\n      &lt;li&gt;s flag indicates to remove unnecessary dependencies&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;&lt;strong&gt;List Orphans&lt;/strong&gt;&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-properties\&quot;&gt;&lt;span class=\&quot;hljs-attr\&quot;&gt;pacman&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;-Qdt&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Q is the &lt;em&gt;Query&lt;/em&gt; action&lt;/li&gt;\n      &lt;li&gt;d filters only depnedencies&lt;/li&gt;\n      &lt;li&gt;t filters only those who are unrequired&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;&lt;strong&gt;Delete orphans&lt;/strong&gt;&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-properties\&quot;&gt;&lt;span class=\&quot;hljs-attr\&quot;&gt;pacman&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;-Rns $(pacman -Qdtq)&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;Notice that we added a &lt;em&gt;q&lt;/em&gt; option to the &lt;em&gt;List Orphans&lt;/em&gt; command, it prints less information. Specifically, it makes it so the &lt;em&gt;Query&lt;/em&gt; operation will only return the package name. That way, we can easily pass it as an argument to the &lt;em&gt;Remove&lt;/em&gt; operation&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;posts/postgres-notes&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/posts/postgres-notes.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Postgres Notes&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [
                    &quot;PostgreSQL&quot;
                ],
                &quot;categories&quot;: [
                    &quot;Notes&quot;,
                    &quot;Databases&quot;
                ]
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\nWithin the REPL, all buit-in commands are prefixed with ```\\```. For example;\n\n```\n\\?              Displays help page\n\\q              Quits the REPL\n\\l              Lists all databases\n\\c {database}   Connect to a different database\n```\n\n### Roles\n\nA Role is the single entity for authentication/authorization. According to how it is set up, it can act as a user, a group or both.\n\nCreate a role\n\n```sql\nCREATE ROLE \&quot;somerole\&quot;\n```\n\nGrant login to the role\n\n```sql\nALTER ROLE \&quot;somerole\&quot; WITH LOGIN;\n```\n\n### Serial (Auto Increment)\n\n```sql\nCREATE TABLE some_table (\n  id SERIAL PRIMARY KEY,\n  other varchar(20)\n)\n```\n&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;p&gt;Within the REPL, all buit-in commands are prefixed with &lt;code&gt;\\&lt;/code&gt;. For example;&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-properties\&quot;&gt;&lt;span class=\&quot;hljs-attr\&quot;&gt;\\?&lt;/span&gt;              &lt;span class=\&quot;hljs-string\&quot;&gt;Displays help page&lt;/span&gt;\n&lt;span class=\&quot;hljs-attr\&quot;&gt;\\q&lt;/span&gt;              &lt;span class=\&quot;hljs-string\&quot;&gt;Quits the REPL&lt;/span&gt;\n&lt;span class=\&quot;hljs-attr\&quot;&gt;\\l&lt;/span&gt;              &lt;span class=\&quot;hljs-string\&quot;&gt;Lists all databases&lt;/span&gt;\n&lt;span class=\&quot;hljs-attr\&quot;&gt;\\c&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;{database}   Connect to a different database&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;h3&gt;Roles&lt;/h3&gt;\n    &lt;p&gt;A Role is the single entity for authentication/authorization. According to how it is set up, it can act as a user, a group or both.&lt;/p&gt;\n    &lt;p&gt;Create a role&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-sql\&quot;&gt;&lt;span class=\&quot;hljs-keyword\&quot;&gt;CREATE&lt;/span&gt; ROLE \&quot;somerole\&quot;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;Grant login to the role&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-sql\&quot;&gt;&lt;span class=\&quot;hljs-keyword\&quot;&gt;ALTER&lt;/span&gt; ROLE \&quot;somerole\&quot; &lt;span class=\&quot;hljs-keyword\&quot;&gt;WITH&lt;/span&gt; LOGIN;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;h3&gt;Serial (Auto Increment)&lt;/h3&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-sql\&quot;&gt;&lt;span class=\&quot;hljs-keyword\&quot;&gt;CREATE&lt;/span&gt; &lt;span class=\&quot;hljs-keyword\&quot;&gt;TABLE&lt;/span&gt; some_table (\n  id SERIAL &lt;span class=\&quot;hljs-keyword\&quot;&gt;PRIMARY&lt;/span&gt; KEY,\n  other &lt;span class=\&quot;hljs-type\&quot;&gt;varchar&lt;/span&gt;(&lt;span class=\&quot;hljs-number\&quot;&gt;20&lt;/span&gt;)\n)\n&lt;/code&gt;&lt;/pre&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;posts/system&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/posts/system.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Scattered stuff to bring together&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [],
                &quot;categories&quot;: []
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\n# Cache\n\n**read-through** strategy is when the server checks if X is in the cache, if it does it returns it, if it does not, invoke the appropriate database query, cache the result as X, and return back.\n\n# Framework\n\n## 1. Undestand the problem and establish a scope for the design\n\n- Take your time\n- Ask clarifications\n- Write down assumptions\n\n## 2. High level proposition\n\n## 3. Design deep dive\n\n## 4. Deep dive\n&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Cache&lt;/h1&gt;\n    &lt;p&gt;&lt;strong&gt;read-through&lt;/strong&gt; strategy is when the server checks if X is in the cache, if it does it returns it, if it does not, invoke the appropriate database query, cache the result as X, and return back.&lt;/p&gt;\n    &lt;h1&gt;Framework&lt;/h1&gt;\n    &lt;h2&gt;1. Undestand the problem and establish a scope for the design&lt;/h2&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Take your time&lt;/li&gt;\n      &lt;li&gt;Ask clarifications&lt;/li&gt;\n      &lt;li&gt;Write down assumptions&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;h2&gt;2. High level proposition&lt;/h2&gt;\n    &lt;h2&gt;3. Design deep dive&lt;/h2&gt;\n    &lt;h2&gt;4. Deep dive&lt;/h2&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        },
        {
            &quot;id&quot;: &quot;posts/vagrant-over-libvirt-arch&quot;,
            &quot;filePath&quot;: &quot;/home/runner/work/tglanz.github.io/tglanz.github.io/content/posts/vagrant-over-libvirt-arch.md&quot;,
            &quot;metadata&quot;: {
                &quot;title&quot;: &quot;Vagrant over libvirt on Archlinux&quot;,
                &quot;description&quot;: null,
                &quot;permalink&quot;: null,
                &quot;priority&quot;: 0,
                &quot;tags&quot;: [],
                &quot;categories&quot;: []
            },
            &quot;content&quot;: {
                &quot;raw&quot;: &quot;\n# The tools\n\n[Vagrant](https://www.vagrantup.com/) is an open source software to virtualize development environments.\n\n[libvirt](https://libvit.org/) is an open source virtualization API. I think the [archwiki page](https://wiki.archlinux.org/title/Libvit) actually describes it better, as a collection of software that provides a way to manage virtualization functionality.\n\n[kvm](https://www.linux-kvm.org/page/Main_Page/) a short for **K**ernel-based **V**irtual **M**achine, is a virtualization infrastructure provided by the kernel.\n\n[QEMU](https://www.qemu.org/) is an open source machine emulator and virtualizer.\n\n# The stack\n\nVagrant will use the libvirt APIs which will be drived by QEMU utilizing kvm.\n\n```\n+-----------+\n|  Vagrant  |\n+-----------+\n|  libvirt  |\n+-----------+\n|  QEMU     |\n+-----------+\n|  KVM      |\n+-----------\n```\n\nTo install the stack we will need the following packages  \n- [vagrant](https://archlinux.org/packages/?name=vagrant)\n- [libvirt](https://archlinux.org/packages/?name=libvirt)\n- [iptables-nft](https://archlinux.org/packages/?name=iptables-nft)\n- [dnsmasq](https://archlinux.org/packages/?name=dnsmasq)\n- [qemu-headless](https://archlinux.org/packages/?name=qemu-headless)\n\nRun the command\n\n    pacman -Suy vagrant libvirt iptables-nft dnsmasq qemu-headless\n\n# Startup\n\nIn order to run the stack we need the following services running\n- libvirtd.service \n- virtlogd.service\n\nYou can either start them by running\n\n    systemctl start libvirtd virtlogd\n\nOr you can enable them by running\n\n    systemctl enable libvirtd virtlogd\n\nYou can check that everything is running by running\n\n    virsh -c qemu:///system\n\n&gt; [virsh](https://linux.die.net/man/1/virsh) is a cli to interact with guest domains (virtual machines).\n\n# Starting out first Vagrant Box\n\nIn Vagrant&#x27;s domain, a Box is a package format, a bit like an ISO or a docker image. We will start a ```debian/bullseye64``` Boxed OS.\n\nInside your working directory, there should be a configuration file that Vagrant can read and function by - It is the ```Vagrantfile```. The Vagrantfile contains configuration about what Box to run, how many resources to allocate and more.\n\nIn a new environment, we can either create a file manually or use Vagrants ```init``` command to do this for us. \n\n    vagrant init --minimal\n\nNow, lets edit the newly created Vagrantfile and set the parameter ```config.vm.box``` to \&quot;debian/bullseye64\&quot;. Lets also add the following parameters  \n\nWe can always validate the Vagrantfile by running\n\n    vagrant validate\n\nAt this point, during validation, you might get an error like ```No usable default provider could be found for your system```. Thats fine, see next.\n\nVagrant uses a notion of Providers. A Provider is Vagrant&#x27;s abstraction over the underlying hypervisor that vagrant will operate. In our case, as illustrated above, we use libvirt as the hypervisor so we will need to install the relevant Provider. To install the Provider we will use Vagrant&#x27;s Plugins mechanism and run\n\n    vagrant plugin install vagrant-libvirt\n\nNow we are finally ready to run the Box by running the command\n\n    vagrant up --provider=libvirt\n\nSome errors you might encounter\n\n**Some error about polkit indicates permission issues**. Read about it for best scenario solution, as a quick and easy solution, add your user to the libvirt group\n\n**Some error about your machine not supporting NFS.** Just install nfs-utils by running\n\n    pacman -Syu nfs-utils\n\n**Forevr waiting on IP acquisition.** I still havn&#x27;t completely figured this out. It looks somethong about DNS and DHCP. What I did find out that some Boxes do not get this issue, try changing.\n\nTo ssh into the OS run\n\n    vagrant ssh\n\nTo clean up run\n\n    vagrant destroy\n\n# Other tools\n\n__virt-manager__ is a graphical tool to list and manage the guest domains.\n&quot;,
                &quot;html&quot;: &quot;&lt;!doctype html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n    &lt;meta charset=\&quot;utf-8\&quot;&gt;\n    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;The tools&lt;/h1&gt;\n    &lt;p&gt;&lt;a href=\&quot;https://www.vagrantup.com/\&quot;&gt;Vagrant&lt;/a&gt; is an open source software to virtualize development environments.&lt;/p&gt;\n    &lt;p&gt;&lt;a href=\&quot;https://libvit.org/\&quot;&gt;libvirt&lt;/a&gt; is an open source virtualization API. I think the &lt;a href=\&quot;https://wiki.archlinux.org/title/Libvit\&quot;&gt;archwiki page&lt;/a&gt; actually describes it better, as a collection of software that provides a way to manage virtualization functionality.&lt;/p&gt;\n    &lt;p&gt;&lt;a href=\&quot;https://www.linux-kvm.org/page/Main_Page/\&quot;&gt;kvm&lt;/a&gt; a short for &lt;strong&gt;K&lt;/strong&gt;ernel-based &lt;strong&gt;V&lt;/strong&gt;irtual &lt;strong&gt;M&lt;/strong&gt;achine, is a virtualization infrastructure provided by the kernel.&lt;/p&gt;\n    &lt;p&gt;&lt;a href=\&quot;https://www.qemu.org/\&quot;&gt;QEMU&lt;/a&gt; is an open source machine emulator and virtualizer.&lt;/p&gt;\n    &lt;h1&gt;The stack&lt;/h1&gt;\n    &lt;p&gt;Vagrant will use the libvirt APIs which will be drived by QEMU utilizing kvm.&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-asciidoc\&quot;&gt;&lt;span class=\&quot;hljs-code\&quot;&gt;+-----------+&lt;/span&gt;\n&lt;span class=\&quot;hljs-section\&quot;&gt;|  Vagrant  |\n+-----------+&lt;/span&gt;\n&lt;span class=\&quot;hljs-section\&quot;&gt;|  libvirt  |\n+-----------+&lt;/span&gt;\n&lt;span class=\&quot;hljs-section\&quot;&gt;|  QEMU     |\n+-----------+&lt;/span&gt;\n&lt;span class=\&quot;hljs-section\&quot;&gt;|  KVM      |\n+-----------&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;To install the stack we will need the following packages&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;&lt;a href=\&quot;https://archlinux.org/packages/?name=vagrant\&quot;&gt;vagrant&lt;/a&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;a href=\&quot;https://archlinux.org/packages/?name=libvirt\&quot;&gt;libvirt&lt;/a&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;a href=\&quot;https://archlinux.org/packages/?name=iptables-nft\&quot;&gt;iptables-nft&lt;/a&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;a href=\&quot;https://archlinux.org/packages/?name=dnsmasq\&quot;&gt;dnsmasq&lt;/a&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;a href=\&quot;https://archlinux.org/packages/?name=qemu-headless\&quot;&gt;qemu-headless&lt;/a&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;Run the command&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-properties\&quot;&gt;&lt;span class=\&quot;hljs-attr\&quot;&gt;pacman&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;-Suy vagrant libvirt iptables-nft dnsmasq qemu-headless&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;h1&gt;Startup&lt;/h1&gt;\n    &lt;p&gt;In order to run the stack we need the following services running&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;libvirtd.service&lt;/li&gt;\n      &lt;li&gt;virtlogd.service&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;p&gt;You can either start them by running&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-properties\&quot;&gt;&lt;span class=\&quot;hljs-attr\&quot;&gt;systemctl&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;start libvirtd virtlogd&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;Or you can enable them by running&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-properties\&quot;&gt;&lt;span class=\&quot;hljs-attr\&quot;&gt;systemctl&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;enable libvirtd virtlogd&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;You can check that everything is running by running&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-perl\&quot;&gt;virsh -c qemu:&lt;span class=\&quot;hljs-regexp\&quot;&gt;//&lt;/span&gt;/&lt;span class=\&quot;hljs-keyword\&quot;&gt;system&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;blockquote&gt;\n      &lt;p&gt;&lt;a href=\&quot;https://linux.die.net/man/1/virsh\&quot;&gt;virsh&lt;/a&gt; is a cli to interact with guest domains (virtual machines).&lt;/p&gt;\n    &lt;/blockquote&gt;\n    &lt;h1&gt;Starting out first Vagrant Box&lt;/h1&gt;\n    &lt;p&gt;In Vagrant&#x27;s domain, a Box is a package format, a bit like an ISO or a docker image. We will start a &lt;code&gt;debian/bullseye64&lt;/code&gt; Boxed OS.&lt;/p&gt;\n    &lt;p&gt;Inside your working directory, there should be a configuration file that Vagrant can read and function by - It is the &lt;code&gt;Vagrantfile&lt;/code&gt;. The Vagrantfile contains configuration about what Box to run, how many resources to allocate and more.&lt;/p&gt;\n    &lt;p&gt;In a new environment, we can either create a file manually or use Vagrants &lt;code&gt;init&lt;/code&gt; command to do this for us.&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-properties\&quot;&gt;&lt;span class=\&quot;hljs-attr\&quot;&gt;vagrant&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;init --minimal&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;Now, lets edit the newly created Vagrantfile and set the parameter &lt;code&gt;config.vm.box&lt;/code&gt; to \&quot;debian/bullseye64\&quot;. Lets also add the following parameters&lt;/p&gt;\n    &lt;p&gt;We can always validate the Vagrantfile by running&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-properties\&quot;&gt;&lt;span class=\&quot;hljs-attr\&quot;&gt;vagrant&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;validate&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;At this point, during validation, you might get an error like &lt;code&gt;No usable default provider could be found for your system&lt;/code&gt;. Thats fine, see next.&lt;/p&gt;\n    &lt;p&gt;Vagrant uses a notion of Providers. A Provider is Vagrant&#x27;s abstraction over the underlying hypervisor that vagrant will operate. In our case, as illustrated above, we use libvirt as the hypervisor so we will need to install the relevant Provider. To install the Provider we will use Vagrant&#x27;s Plugins mechanism and run&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-properties\&quot;&gt;&lt;span class=\&quot;hljs-attr\&quot;&gt;vagrant&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;plugin install vagrant-libvirt&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;Now we are finally ready to run the Box by running the command&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-ini\&quot;&gt;vagrant up &lt;span class=\&quot;hljs-attr\&quot;&gt;--provider&lt;/span&gt;=libvirt\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;Some errors you might encounter&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Some error about polkit indicates permission issues&lt;/strong&gt;. Read about it for best scenario solution, as a quick and easy solution, add your user to the libvirt group&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Some error about your machine not supporting NFS.&lt;/strong&gt; Just install nfs-utils by running&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-properties\&quot;&gt;&lt;span class=\&quot;hljs-attr\&quot;&gt;pacman&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;-Syu nfs-utils&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;&lt;strong&gt;Forevr waiting on IP acquisition.&lt;/strong&gt; I still havn&#x27;t completely figured this out. It looks somethong about DNS and DHCP. What I did find out that some Boxes do not get this issue, try changing.&lt;/p&gt;\n    &lt;p&gt;To ssh into the OS run&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-properties\&quot;&gt;&lt;span class=\&quot;hljs-attr\&quot;&gt;vagrant&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;ssh&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;p&gt;To clean up run&lt;/p&gt;\n    &lt;pre&gt;&lt;code class=\&quot;hljs language-properties\&quot;&gt;&lt;span class=\&quot;hljs-attr\&quot;&gt;vagrant&lt;/span&gt; &lt;span class=\&quot;hljs-string\&quot;&gt;destroy&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;\n    &lt;h1&gt;Other tools&lt;/h1&gt;\n    &lt;p&gt;&lt;strong&gt;virt-manager&lt;/strong&gt; is a graphical tool to list and manage the guest domains.&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;
            }
        }
    ],
    &quot;metadataAggregation&quot;: {
        &quot;categories&quot;: {
            &quot;Algorithms&quot;: {
                &quot;count&quot;: 4
            },
            &quot;Aws&quot;: {
                &quot;count&quot;: 5
            },
            &quot;Domain Driven Design&quot;: {
                &quot;count&quot;: 4
            },
            &quot;Kubernetes&quot;: {
                &quot;count&quot;: 6
            },
            &quot;Guides&quot;: {
                &quot;count&quot;: 1
            },
            &quot;Cheatsheets&quot;: {
                &quot;count&quot;: 1
            },
            &quot;Databases&quot;: {
                &quot;count&quot;: 2
            },
            &quot;Notes&quot;: {
                &quot;count&quot;: 2
            }
        },
        &quot;tags&quot;: {
            &quot;Dynamic Programming&quot;: {
                &quot;count&quot;: 3
            },
            &quot;Palindrom&quot;: {
                &quot;count&quot;: 1
            },
            &quot;Kubernetes&quot;: {
                &quot;count&quot;: 6
            },
            &quot;Archlinux&quot;: {
                &quot;count&quot;: 1
            },
            &quot;Linux&quot;: {
                &quot;count&quot;: 1
            },
            &quot;Emacs&quot;: {
                &quot;count&quot;: 1
            },
            &quot;MariaDB&quot;: {
                &quot;count&quot;: 1
            },
            &quot;PostgreSQL&quot;: {
                &quot;count&quot;: 1
            }
        }
    }
}</pre></div><footer><div class=" border-t-2 p-4 flex flex-row justify-center items-center space-x-4"><a href="https://github.com/tglanz"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://il.linkedin.com/in/tal-glanzman"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"content":{"articles":[{"id":"about","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/about.md","metadata":{"title":null,"description":null,"permalink":null,"priority":0,"tags":[],"categories":[]},"content":{"raw":"# About\n\nBla bla\n\nAdditional Blabla","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch1\u003eAbout\u003c/h1\u003e\n    \u003cp\u003eBla bla\u003c/p\u003e\n    \u003cp\u003eAdditional Blabla\u003c/p\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"algorithms/bloom","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/algorithms/bloom.md","metadata":{"title":"Bloom Filter","description":"Illustrate the Bloomfilter data structure","permalink":null,"priority":0,"tags":[],"categories":["Algorithms"]},"content":{"raw":"\n## Summary\n\nA _Bloom Filter_ is an data structure that given a set of elements $S$ and a subset $X \\subset S$, answers the question whether some element $s \\in S$ is in $X$.\n\nThe main difference between this and standard data structures using sets and lists is in the fact that this is a probabilistic data structure. Specifically, it can return False Positives but never False Negatives.\n\n|           | Positive      | Negative\n|-----------|---------------|---------\n| **True**  | Always        | Always\n| **False** | Probabilistic | Never\n\nA standard implementation of _Bloom Filters_ support adding elements to $X$ but does not support removal requiring the application to reconstruct the data structure accordingly.\n\n_Bloom Filter_ provides a more efficient memory usage than most other constructs such as lists and sets in the cost of a chance for False Positives.\n\nTo illustrate, consider the following interface\n\n```c#\ninterface BloomFilter\u003cS\u003e {\n    // Add {element} to the container\n    void add(S element);\n\n    // Determines whether {element} is in the container\n    bool contains(S element);\n}\n```\n\n## Implementation\n\n- Set __A__ to be an $m$ bits bit array.\n- Set __H__ to be a set of $k$ functions mapping $S$ onto $\\\\{1, 2, ..., m \\\\}$\n\n__add(x)__\n\nApply all hash functions of $H$ on $x$ and set the corresponding bit in $A$ to 1.\n\n- $\\forall h \\in H$  \n  - $A[h(x)] \\leftarrow 1$\n\n__contains(x)__\n\nApply all hash functions of $H$ on $x$ and return true if and only if all of the corresponding bits in $A$ are set to 1.\n\n- $\\forall h \\in H$\n  - $A[h(x)] = 1 \\Rightarrow True$\n\nNow we can easily understand where the False Positives comes from.\n\n## False Positives\n\nAssume \n\n- $S = \\\\{x_1, x_2, x_3\\\\}$\n- $m=5$\n- $k=2$\n\nWith the hash functions  \n\n- $h_1(x_1)=1 ~~ h_1(x_2)=3 ~~ h_1(x_3)=3$\n- $h_2(x_1)=4 ~~ h_2(x_2)=1 ~~ h_2(x_3)=4$\n\nAnd the scenario of $X = \\\\{x_1, x_2\\\\}$ as shown below\n\n| 1 | 2 | 3 | 4 | 5 |\n|:-:|:-:|:-:|:-:|:-:|\n|$x_1, x_2$||$x_2$|$x_1$||\n\nApplying __contains($x_3$)__ will yield a False Positive.\n\nWhat was the chance of that happening?\n\nAssuming equal distribution of the hash functions, the probability that a specific bit is unset after applying $k$ functions on an element is $(1 - \\frac {1}{m})^k \\approx e^{-\\frac{k}{m}}$ for large $m$.\n\nNow we can conclude that after the addition of $n$ elements (using __add(x)__) to the data structure the probability for a specific bit to be unset is $e^{-\\frac{kn}{m}}$ and for it to be set is $1 - e^{-\\frac{kn}{m}}$.\n\nFor the containment check to return true it is enough for $k$ specific bits to be set. From the above, the probability for this is $(1 - e^{-\\frac{kn}{m}})^k$.\n\n__To summarize, the probability for a _False Positive_ is $(1 - e^{-\\frac{kn}{m}})^k$.__\n\n## Picking the hash functions\n\n[This paper](https://www.eecs.harvard.edu/~michaelm/postscripts/tr-02-05.pdf) describes a technique of constructing a set of $k$ hash functions based upon exactly 2 hash functions $h_1$ and $h_2$.\n\nThe usefulness of this approach relies in the fact that not all scenarios require the same number of hash functions $k$. Thus, allowing us to provide only 2 base functions $h_1$ and $h_2$ and construct a set of hash functions for any arbitrary $k$.\n\nThere they describe a partitioning of the bit array, each partition of a primary size $p$, such that each hash function maps to $\\\\{ 1, 2, ..., p \\\\}$ within it's own unique partition. (Note that this is just a restatement of the original view).\n\nNow, forall $i$ we can have\n$$\n    g_i(x) = h_1(x) + ih_2(x) \\mod p\n$$\n\n\u003e The paper discusses a lot more and more in-depth","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch2\u003eSummary\u003c/h2\u003e\n    \u003cp\u003eA \u003cem\u003eBloom Filter\u003c/em\u003e is an data structure that given a set of elements $S$ and a subset $X \\subset S$, answers the question whether some element $s \\in S$ is in $X$.\u003c/p\u003e\n    \u003cp\u003eThe main difference between this and standard data structures using sets and lists is in the fact that this is a probabilistic data structure. Specifically, it can return False Positives but never False Negatives.\u003c/p\u003e\n    \u003ctable\u003e\n      \u003cthead\u003e\n        \u003ctr\u003e\n          \u003cth\u003e\u003c/th\u003e\n          \u003cth\u003ePositive\u003c/th\u003e\n          \u003cth\u003eNegative\u003c/th\u003e\n        \u003c/tr\u003e\n      \u003c/thead\u003e\n      \u003ctbody\u003e\n        \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eTrue\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eAlways\u003c/td\u003e\n          \u003ctd\u003eAlways\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eFalse\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eProbabilistic\u003c/td\u003e\n          \u003ctd\u003eNever\u003c/td\u003e\n        \u003c/tr\u003e\n      \u003c/tbody\u003e\n    \u003c/table\u003e\n    \u003cp\u003eA standard implementation of \u003cem\u003eBloom Filters\u003c/em\u003e support adding elements to $X$ but does not support removal requiring the application to reconstruct the data structure accordingly.\u003c/p\u003e\n    \u003cp\u003e\u003cem\u003eBloom Filter\u003c/em\u003e provides a more efficient memory usage than most other constructs such as lists and sets in the cost of a chance for False Positives.\u003c/p\u003e\n    \u003cp\u003eTo illustrate, consider the following interface\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-c#\"\u003e\u003cspan class=\"hljs-keyword\"\u003einterface\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003eBloomFilter\u003c/span\u003e\u0026#x3C;\u003cspan class=\"hljs-title\"\u003eS\u003c/span\u003e\u003e {\n    \u003cspan class=\"hljs-comment\"\u003e// Add {element} to the container\u003c/span\u003e\n    \u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003evoid\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003eadd\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eS element\u003c/span\u003e)\u003c/span\u003e;\n\n    \u003cspan class=\"hljs-comment\"\u003e// Determines whether {element} is in the container\u003c/span\u003e\n    \u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-built_in\"\u003ebool\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003econtains\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eS element\u003c/span\u003e)\u003c/span\u003e;\n}\n\u003c/code\u003e\u003c/pre\u003e\n    \u003ch2\u003eImplementation\u003c/h2\u003e\n    \u003cul\u003e\n      \u003cli\u003eSet \u003cstrong\u003eA\u003c/strong\u003e to be an $m$ bits bit array.\u003c/li\u003e\n      \u003cli\u003eSet \u003cstrong\u003eH\u003c/strong\u003e to be a set of $k$ functions mapping $S$ onto $\\{1, 2, ..., m \\}$\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003e\u003cstrong\u003eadd(x)\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003eApply all hash functions of $H$ on $x$ and set the corresponding bit in $A$ to 1.\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e$\\forall h \\in H$\n        \u003cul\u003e\n          \u003cli\u003e$A[h(x)] \\leftarrow 1$\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003e\u003cstrong\u003econtains(x)\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003eApply all hash functions of $H$ on $x$ and return true if and only if all of the corresponding bits in $A$ are set to 1.\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e$\\forall h \\in H$\n        \u003cul\u003e\n          \u003cli\u003e$A[h(x)] = 1 \\Rightarrow True$\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eNow we can easily understand where the False Positives comes from.\u003c/p\u003e\n    \u003ch2\u003eFalse Positives\u003c/h2\u003e\n    \u003cp\u003eAssume\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e$S = \\{x_1, x_2, x_3\\}$\u003c/li\u003e\n      \u003cli\u003e$m=5$\u003c/li\u003e\n      \u003cli\u003e$k=2$\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eWith the hash functions\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e$h_1(x_1)=1 ~~ h_1(x_2)=3 ~~ h_1(x_3)=3$\u003c/li\u003e\n      \u003cli\u003e$h_2(x_1)=4 ~~ h_2(x_2)=1 ~~ h_2(x_3)=4$\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eAnd the scenario of $X = \\{x_1, x_2\\}$ as shown below\u003c/p\u003e\n    \u003ctable\u003e\n      \u003cthead\u003e\n        \u003ctr\u003e\n          \u003cth align=\"center\"\u003e1\u003c/th\u003e\n          \u003cth align=\"center\"\u003e2\u003c/th\u003e\n          \u003cth align=\"center\"\u003e3\u003c/th\u003e\n          \u003cth align=\"center\"\u003e4\u003c/th\u003e\n          \u003cth align=\"center\"\u003e5\u003c/th\u003e\n        \u003c/tr\u003e\n      \u003c/thead\u003e\n      \u003ctbody\u003e\n        \u003ctr\u003e\n          \u003ctd align=\"center\"\u003e$x_1, x_2$\u003c/td\u003e\n          \u003ctd align=\"center\"\u003e\u003c/td\u003e\n          \u003ctd align=\"center\"\u003e$x_2$\u003c/td\u003e\n          \u003ctd align=\"center\"\u003e$x_1$\u003c/td\u003e\n          \u003ctd align=\"center\"\u003e\u003c/td\u003e\n        \u003c/tr\u003e\n      \u003c/tbody\u003e\n    \u003c/table\u003e\n    \u003cp\u003eApplying \u003cstrong\u003econtains($x_3$)\u003c/strong\u003e will yield a False Positive.\u003c/p\u003e\n    \u003cp\u003eWhat was the chance of that happening?\u003c/p\u003e\n    \u003cp\u003eAssuming equal distribution of the hash functions, the probability that a specific bit is unset after applying $k$ functions on an element is $(1 - \\frac {1}{m})^k \\approx e^{-\\frac{k}{m}}$ for large $m$.\u003c/p\u003e\n    \u003cp\u003eNow we can conclude that after the addition of $n$ elements (using \u003cstrong\u003eadd(x)\u003c/strong\u003e) to the data structure the probability for a specific bit to be unset is $e^{-\\frac{kn}{m}}$ and for it to be set is $1 - e^{-\\frac{kn}{m}}$.\u003c/p\u003e\n    \u003cp\u003eFor the containment check to return true it is enough for $k$ specific bits to be set. From the above, the probability for this is $(1 - e^{-\\frac{kn}{m}})^k$.\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eTo summarize, the probability for a \u003cem\u003eFalse Positive\u003c/em\u003e is $(1 - e^{-\\frac{kn}{m}})^k$.\u003c/strong\u003e\u003c/p\u003e\n    \u003ch2\u003ePicking the hash functions\u003c/h2\u003e\n    \u003cp\u003e\u003ca href=\"https://www.eecs.harvard.edu/~michaelm/postscripts/tr-02-05.pdf\"\u003eThis paper\u003c/a\u003e describes a technique of constructing a set of $k$ hash functions based upon exactly 2 hash functions $h_1$ and $h_2$.\u003c/p\u003e\n    \u003cp\u003eThe usefulness of this approach relies in the fact that not all scenarios require the same number of hash functions $k$. Thus, allowing us to provide only 2 base functions $h_1$ and $h_2$ and construct a set of hash functions for any arbitrary $k$.\u003c/p\u003e\n    \u003cp\u003eThere they describe a partitioning of the bit array, each partition of a primary size $p$, such that each hash function maps to $\\{ 1, 2, ..., p \\}$ within it's own unique partition. (Note that this is just a restatement of the original view).\u003c/p\u003e\n    \u003cp\u003e\n      Now, forall $i$ we can have\n      $$\n      g_i(x) = h_1(x) + ih_2(x) \\mod p\n      $$\n    \u003c/p\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003eThe paper discusses a lot more and more in-depth\u003c/p\u003e\n    \u003c/blockquote\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"algorithms/dynamic-programming/longest-increasing-subsequence","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/algorithms/dynamic-programming/longest-increasing-subsequence.md","metadata":{"title":"Longest increasing subsequence","description":null,"permalink":null,"priority":0,"tags":["Dynamic Programming"],"categories":["Algorithms"]},"content":{"raw":"\n\n**The problem**\n\nGiven a sequence of numbers $N$, find the length of longest subsequence of numbers that is increasing\n\n**Illustration**\n\n- [1, 2, 3, 4] $\\rightarrow$ [1, 2, 3, 4]\n- [1, 4, 2, 6, 3, 7] $\\rightarrow$ [1, 4, 6, 7]\n\n**Solution**\n\nlet $(a_i)_{i=1}^n = N$.\n\n$a_i$ can contribute to a subsequence of previous elements only if it is greater than all those elements.\n\nWe can also realize the the longest increasing subsequence that ends with $a_i$ is the previous longest subsequence containing elements less that $a_i$, adding $a_i$ to it.\n\nWe shall define opt(i) to indicate the length of the longest increasing subsequence that ends with $a_i$.\n\nTrivially,\n$$\nopt(0) = 0\n$$\n\nRecursively,\n$$\n\\forall i \u003e 0; ~ opt(i) = 1 + \\max \\\\{ \\\\{0\\\\} \\cup \\\\{ opt(j) | j \u003c i \\land a_j \u003c a_i \\\\} \\\\}\n$$\n\n**Psuedo**\n\n- Initialize opt\n  - $opt \\leftarrow array(n+1)$\n  - $opt[0] \\leftarrow 0$\n\n- Build opt in a bottom up fashion\n  - $for ~ i = 1, 2, ... n$\n    - $opt[i] = 1 + \\max_{j \u003c i \\land a_j \u003c a_i}{opt[j]}$\n- Return the final answer\n  - $ans \\leftarrow 0$\n  - $for ~ i = 1, 2, ... n$\n    - $ans \\leftarrow max \\\\{ ans, opt[i] \\\\}$\n  - Return $ans$\n\n\n**Time Complexity**\n\n- Initialization of opt is $O(1)$ (single access to opt)\n  - We exclude the actual array creation\n- Building opt takes $O(n)$ iterations\n  - Each iteration takes $O(n)$ accesses at the worst case\n\nSo in total, the time complexity is $O(n^2)$\n\n**Finding the subsequence using a Journal**\n\nWe will keep another data structure $S$ that will act as the journal.\n\nFor each $i$, $S$ will contain the index $j$ that contains the previous element in the subsequence.\n\nWe can fill it during the algorithm by modifying \"Build opt in a bottom up fashion\" to be\n\n- Build opt in a bottom up fashion\n  - $for ~ i = 1, 2, ... n$\n    - $opt[i] = 1 + \\max_{j \u003c i \\land a_j \u003c a_i}{opt[j]}$\n    - $S[i] = $ the $j$ that achieved the max\n\nFinally, to print we shall go back from the position of the result using the indices at the journal. \n\n**Finding the subsequence using Traceback**\n\nLets review an example.\n\nAssume $N=[3, 4, 2, 7, 5]$\n\nThe final tabulation of opt will be\n\ni|0|1|2|3|4|5\n-|-|-|-|-|-|-\n$a_i$| |3|4|2|7|5\nopt(i)|0|1|2|1|3|2\n\nHere, we need to find $7 \\leftarrow 4 \\leftarrow 3$. Specifically\n- $opt(\"7\") = 3, opt(\"4\") = 2, opt(\"3\") = 1$\n\nGenerally, to traceback, we shall find the index i that contains the maximum of opt as we did finding the answer. We notice that the previous element in the relevant subsequence is ending a subsequence of length $opt(i) - 1$. So we traverse back in the opt array until we find an element $a_j$ such that $a_j \u003c a_i$ and $opt(j) = opt(i) - 1$. This $j$ is the index of the previous element in the answer. We will keep going in this manner until index 0. \n\n**Actual Code**\n\n{{\u003ccodepen RwVdYyO\u003e}}","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003cp\u003e\u003cstrong\u003eThe problem\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003eGiven a sequence of numbers $N$, find the length of longest subsequence of numbers that is increasing\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eIllustration\u003c/strong\u003e\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e[1, 2, 3, 4] $\\rightarrow$ [1, 2, 3, 4]\u003c/li\u003e\n      \u003cli\u003e[1, 4, 2, 6, 3, 7] $\\rightarrow$ [1, 4, 6, 7]\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003e\u003cstrong\u003eSolution\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003elet $(a_i)_{i=1}^n = N$.\u003c/p\u003e\n    \u003cp\u003e$a_i$ can contribute to a subsequence of previous elements only if it is greater than all those elements.\u003c/p\u003e\n    \u003cp\u003eWe can also realize the the longest increasing subsequence that ends with $a_i$ is the previous longest subsequence containing elements less that $a_i$, adding $a_i$ to it.\u003c/p\u003e\n    \u003cp\u003eWe shall define opt(i) to indicate the length of the longest increasing subsequence that ends with $a_i$.\u003c/p\u003e\n    \u003cp\u003e\n      Trivially,\n      $$\n      opt(0) = 0\n      $$\n    \u003c/p\u003e\n    \u003cp\u003e\n      Recursively,\n      $$\n      \\forall i \u003e 0; ~ opt(i) = 1 + \\max \\{ \\{0\\} \\cup \\{ opt(j) | j \u0026#x3C; i \\land a_j \u0026#x3C; a_i \\} \\}\n      $$\n    \u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003ePsuedo\u003c/strong\u003e\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\n        \u003cp\u003eInitialize opt\u003c/p\u003e\n        \u003cul\u003e\n          \u003cli\u003e$opt \\leftarrow array(n+1)$\u003c/li\u003e\n          \u003cli\u003e$opt[0] \\leftarrow 0$\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\n        \u003cp\u003eBuild opt in a bottom up fashion\u003c/p\u003e\n        \u003cul\u003e\n          \u003cli\u003e$for ~ i = 1, 2, ... n$\n            \u003cul\u003e\n              \u003cli\u003e$opt[i] = 1 + \\max_{j \u0026#x3C; i \\land a_j \u0026#x3C; a_i}{opt[j]}$\u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\n        \u003cp\u003eReturn the final answer\u003c/p\u003e\n        \u003cul\u003e\n          \u003cli\u003e$ans \\leftarrow 0$\u003c/li\u003e\n          \u003cli\u003e$for ~ i = 1, 2, ... n$\n            \u003cul\u003e\n              \u003cli\u003e$ans \\leftarrow max \\{ ans, opt[i] \\}$\u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n          \u003cli\u003eReturn $ans$\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003e\u003cstrong\u003eTime Complexity\u003c/strong\u003e\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eInitialization of opt is $O(1)$ (single access to opt)\n        \u003cul\u003e\n          \u003cli\u003eWe exclude the actual array creation\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003eBuilding opt takes $O(n)$ iterations\n        \u003cul\u003e\n          \u003cli\u003eEach iteration takes $O(n)$ accesses at the worst case\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eSo in total, the time complexity is $O(n^2)$\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eFinding the subsequence using a Journal\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003eWe will keep another data structure $S$ that will act as the journal.\u003c/p\u003e\n    \u003cp\u003eFor each $i$, $S$ will contain the index $j$ that contains the previous element in the subsequence.\u003c/p\u003e\n    \u003cp\u003eWe can fill it during the algorithm by modifying \"Build opt in a bottom up fashion\" to be\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eBuild opt in a bottom up fashion\n        \u003cul\u003e\n          \u003cli\u003e$for ~ i = 1, 2, ... n$\n            \u003cul\u003e\n              \u003cli\u003e$opt[i] = 1 + \\max_{j \u0026#x3C; i \\land a_j \u0026#x3C; a_i}{opt[j]}$\u003c/li\u003e\n              \u003cli\u003e$S[i] = $ the $j$ that achieved the max\u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eFinally, to print we shall go back from the position of the result using the indices at the journal.\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eFinding the subsequence using Traceback\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003eLets review an example.\u003c/p\u003e\n    \u003cp\u003eAssume $N=[3, 4, 2, 7, 5]$\u003c/p\u003e\n    \u003cp\u003eThe final tabulation of opt will be\u003c/p\u003e\n    \u003ctable\u003e\n      \u003cthead\u003e\n        \u003ctr\u003e\n          \u003cth\u003ei\u003c/th\u003e\n          \u003cth\u003e0\u003c/th\u003e\n          \u003cth\u003e1\u003c/th\u003e\n          \u003cth\u003e2\u003c/th\u003e\n          \u003cth\u003e3\u003c/th\u003e\n          \u003cth\u003e4\u003c/th\u003e\n          \u003cth\u003e5\u003c/th\u003e\n        \u003c/tr\u003e\n      \u003c/thead\u003e\n      \u003ctbody\u003e\n        \u003ctr\u003e\n          \u003ctd\u003e$a_i$\u003c/td\u003e\n          \u003ctd\u003e\u003c/td\u003e\n          \u003ctd\u003e3\u003c/td\u003e\n          \u003ctd\u003e4\u003c/td\u003e\n          \u003ctd\u003e2\u003c/td\u003e\n          \u003ctd\u003e7\u003c/td\u003e\n          \u003ctd\u003e5\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n          \u003ctd\u003eopt(i)\u003c/td\u003e\n          \u003ctd\u003e0\u003c/td\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003e2\u003c/td\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003e3\u003c/td\u003e\n          \u003ctd\u003e2\u003c/td\u003e\n        \u003c/tr\u003e\n      \u003c/tbody\u003e\n    \u003c/table\u003e\n    \u003cp\u003eHere, we need to find $7 \\leftarrow 4 \\leftarrow 3$. Specifically\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e$opt(\"7\") = 3, opt(\"4\") = 2, opt(\"3\") = 1$\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eGenerally, to traceback, we shall find the index i that contains the maximum of opt as we did finding the answer. We notice that the previous element in the relevant subsequence is ending a subsequence of length $opt(i) - 1$. So we traverse back in the opt array until we find an element $a_j$ such that $a_j \u0026#x3C; a_i$ and $opt(j) = opt(i) - 1$. This $j$ is the index of the previous element in the answer. We will keep going in this manner until index 0.\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eActual Code\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003e{{}}\u003c/p\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"algorithms/dynamic-programming/longest-path-in-ordered-graph","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/algorithms/dynamic-programming/longest-path-in-ordered-graph.md","metadata":{"title":"Longest Path in Ordered Graph","description":null,"permalink":null,"priority":0,"tags":["Dynamic Programming"],"categories":["Algorithms"]},"content":{"raw":"\nA directed graph $G=(V, E)$ is **ordererd**  \n\nif\n$$\n  \\forall (v_i, v_j) \\in E \\Rightarrow i \u003c j\n$$\n\nand\n$$\n  \\forall v_i \\in V / \\\\{ v_n \\\\} ~;~ \\exists j\u003ei, e=(v_i, v_j) \\in E\n$$\n\n**The problem**\n\nGiven such graph, find the length of the longest path from $v_1$ to $v_n$ where $n = |V|$.\n\n**Illustration**\n\n{{\u003c mermaid \u003e}}\n  graph LR\n    v1 --\u003e v2\n    v3 --\u003e v4\n    v4 --\u003e v5\n    v1 --\u003e v4\n    v2 --\u003e v4\n    v2 --\u003e v5\n{{\u003c/ mermaid \u003e}}\n\nFor this example, the longest path is $(v_1, v_2) \\rightarrow (v_2, v_4) \\rightarrow (v_4, v_5)$.\n\n**Solution**\n\nWe shall define $opt(i)$ to indicate the length of the longest path from $v_1$ to $v_i$ by\n\n$$\n  opt(0) = 0\n$$\n\n$$\n  opt(i)_{1 \u003e 0} = 1 + \\max \\\\{ opt(k) | (v_k, v_i) \\in E \\\\}\n$$\n\nThe answer we are looking for is given by $opt(n)$.\n\nThe intuition here is breaking the problem into smaller subproblems. The most straight forward way of doing so is by starting from $v_1$ and incrementally adding more vertices into consideration.\n\n**Psuedo**\n\nAs always, pseudo is very important in dynamic programming algorithms in order to illustrate pattenr of building the data structure (bottom up).\n\n- Initialize opt\n  - $opt \\leftarrow array(n)$\n  - $opt[0] \\leftarrow 0$\n  - $\\forall i \\in \\\\{ 1, 2, ..., n \\\\}$\n    - $opt[i] \\leftarrow nil$\n- Build opt in a bottom up fashion\n  - $for ~ i \\leftarrow 1 ~ to ~ n$\n    - $opt[i] \\leftarrow 1 + \\max \\\\{ opt(k) | (v_k, v_i) \\in E \\\\}$\n- Return $opt[n]$\n\n**Time Complexity**\n\n- Initialization of opt is $O(n)$\n- Building opt takes $O(n)$ iterations\n  - Each iteration takes $O(n)$ accesses to opt\n\nSo in total, the time complexity is $O(n^2)$","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003cp\u003eA directed graph $G=(V, E)$ is \u003cstrong\u003eordererd\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003e\n      if\n      $$\n      \\forall (v_i, v_j) \\in E \\Rightarrow i \u0026#x3C; j\n      $$\n    \u003c/p\u003e\n    \u003cp\u003e\n      and\n      $$\n      \\forall v_i \\in V / \\{ v_n \\} \u003cdel\u003e;\u003c/del\u003e \\exists j\u003ei, e=(v_i, v_j) \\in E\n      $$\n    \u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eThe problem\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003eGiven such graph, find the length of the longest path from $v_1$ to $v_n$ where $n = |V|$.\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eIllustration\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003e\n      {{\u0026#x3C; mermaid \u003e}}\n      graph LR\n      v1 --\u003e v2\n      v3 --\u003e v4\n      v4 --\u003e v5\n      v1 --\u003e v4\n      v2 --\u003e v4\n      v2 --\u003e v5\n      {{\u0026#x3C;/ mermaid \u003e}}\n    \u003c/p\u003e\n    \u003cp\u003eFor this example, the longest path is $(v_1, v_2) \\rightarrow (v_2, v_4) \\rightarrow (v_4, v_5)$.\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eSolution\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003eWe shall define $opt(i)$ to indicate the length of the longest path from $v_1$ to $v_i$ by\u003c/p\u003e\n    \u003cp\u003e\n      $$\n      opt(0) = 0\n      $$\n    \u003c/p\u003e\n    \u003cp\u003e\n      $$\n      opt(i)_{1 \u003e 0} = 1 + \\max \\{ opt(k) | (v_k, v_i) \\in E \\}\n      $$\n    \u003c/p\u003e\n    \u003cp\u003eThe answer we are looking for is given by $opt(n)$.\u003c/p\u003e\n    \u003cp\u003eThe intuition here is breaking the problem into smaller subproblems. The most straight forward way of doing so is by starting from $v_1$ and incrementally adding more vertices into consideration.\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003ePsuedo\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003eAs always, pseudo is very important in dynamic programming algorithms in order to illustrate pattenr of building the data structure (bottom up).\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eInitialize opt\n        \u003cul\u003e\n          \u003cli\u003e$opt \\leftarrow array(n)$\u003c/li\u003e\n          \u003cli\u003e$opt[0] \\leftarrow 0$\u003c/li\u003e\n          \u003cli\u003e$\\forall i \\in \\{ 1, 2, ..., n \\}$\n            \u003cul\u003e\n              \u003cli\u003e$opt[i] \\leftarrow nil$\u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003eBuild opt in a bottom up fashion\n        \u003cul\u003e\n          \u003cli\u003e$for ~ i \\leftarrow 1 ~ to ~ n$\n            \u003cul\u003e\n              \u003cli\u003e$opt[i] \\leftarrow 1 + \\max \\{ opt(k) | (v_k, v_i) \\in E \\}$\u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003eReturn $opt[n]$\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003e\u003cstrong\u003eTime Complexity\u003c/strong\u003e\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eInitialization of opt is $O(n)$\u003c/li\u003e\n      \u003cli\u003eBuilding opt takes $O(n)$ iterations\n        \u003cul\u003e\n          \u003cli\u003eEach iteration takes $O(n)$ accesses to opt\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eSo in total, the time complexity is $O(n^2)$\u003c/p\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"algorithms/dynamic-programming/longest-substring-that-is-a-palindrom","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/algorithms/dynamic-programming/longest-substring-that-is-a-palindrom.md","metadata":{"title":"Longest substring that is a Palindrom","description":null,"permalink":null,"priority":0,"tags":["Dynamic Programming","Palindrom"],"categories":["Algorithms"]},"content":{"raw":"\n# Overview\n\nA string $S$ is a palindrom iff $S = reverse(S)$\n\n**The Problem**\n\nGiven a string S, find the longest *substring* of S that is also a palindrom\n\n\u003e Remember that substrings are consequtive\n\n**To illustrate**\n\n- a**bcb**ea $\\rightarrow$ bcb (not abcba)\n- **abbcbba**dad $\\rightarrow$ abbcbba\n\n# Solution\n\nWe shall define $opt(i, j)$ to indicate the length of the longest palindrom substring between the i'th character to the j'th character by the recurrence relation\n\nfor all i, opt(i, i) = true since a single character is a palindrom of itself.\n\nfor all i and $j \u003e i$, opt(i, j) true if S[i] = S[j] and also opt(i + 1, j - 1) = true.\n\nFinally, to figure out the length of the longest substring that is a palindrom we need to find i and j that maximizes $j - i$ and such that opt(i, j) = true.\n\n**Psuedo**\n\n- Initialize opt\n  - $opt \\leftarrow matrix(n, n)$\n    - initialize all values false by default\n    - $\\forall i$\n      - $opt(i, i) = true$\n      - $opt(i, i + 1) \\leftarrow S[i]=S[i+1]$\n\n- Build opt in a bottom up fashion\n  - $for ~ l = 2, 3, ... n - 1$\n    - $for ~ i = 1, ..., n - l$\n      - $j \\leftarrow i +  - 1$\n      - $opt(i, j) \\leftarrow S[i] = S[j] \\land opt(i + 1, j - 1)$\n- Find the length of the longest substring that is a palindrom\n  - $for~ l = n-1, n-2, ..., 1$\n    - $for~ i = 1, 2, ..., n - l$\n      - $if~ opt(i, i + l)$\n        - return l\n\n**Time Complexity**\n\n- Initialization of opt is $O(n^2)$\n- Building opt takes $O(n^2)$ iterations\n  - Each iteration takes $O(1)$ accesses to opt\n\nSo in total, the time complexity is $O(n^2)$","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch1\u003eOverview\u003c/h1\u003e\n    \u003cp\u003eA string $S$ is a palindrom iff $S = reverse(S)$\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eThe Problem\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003eGiven a string S, find the longest \u003cem\u003esubstring\u003c/em\u003e of S that is also a palindrom\u003c/p\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003eRemember that substrings are consequtive\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003cp\u003e\u003cstrong\u003eTo illustrate\u003c/strong\u003e\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003ea\u003cstrong\u003ebcb\u003c/strong\u003eea $\\rightarrow$ bcb (not abcba)\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003eabbcbba\u003c/strong\u003edad $\\rightarrow$ abbcbba\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch1\u003eSolution\u003c/h1\u003e\n    \u003cp\u003eWe shall define $opt(i, j)$ to indicate the length of the longest palindrom substring between the i'th character to the j'th character by the recurrence relation\u003c/p\u003e\n    \u003cp\u003efor all i, opt(i, i) = true since a single character is a palindrom of itself.\u003c/p\u003e\n    \u003cp\u003efor all i and $j \u003e i$, opt(i, j) true if S[i] = S[j] and also opt(i + 1, j - 1) = true.\u003c/p\u003e\n    \u003cp\u003eFinally, to figure out the length of the longest substring that is a palindrom we need to find i and j that maximizes $j - i$ and such that opt(i, j) = true.\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003ePsuedo\u003c/strong\u003e\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\n        \u003cp\u003eInitialize opt\u003c/p\u003e\n        \u003cul\u003e\n          \u003cli\u003e$opt \\leftarrow matrix(n, n)$\n            \u003cul\u003e\n              \u003cli\u003einitialize all values false by default\u003c/li\u003e\n              \u003cli\u003e$\\forall i$\n                \u003cul\u003e\n                  \u003cli\u003e$opt(i, i) = true$\u003c/li\u003e\n                  \u003cli\u003e$opt(i, i + 1) \\leftarrow S[i]=S[i+1]$\u003c/li\u003e\n                \u003c/ul\u003e\n              \u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\n        \u003cp\u003eBuild opt in a bottom up fashion\u003c/p\u003e\n        \u003cul\u003e\n          \u003cli\u003e$for ~ l = 2, 3, ... n - 1$\n            \u003cul\u003e\n              \u003cli\u003e$for ~ i = 1, ..., n - l$\n                \u003cul\u003e\n                  \u003cli\u003e$j \\leftarrow i + - 1$\u003c/li\u003e\n                  \u003cli\u003e$opt(i, j) \\leftarrow S[i] = S[j] \\land opt(i + 1, j - 1)$\u003c/li\u003e\n                \u003c/ul\u003e\n              \u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\n        \u003cp\u003eFind the length of the longest substring that is a palindrom\u003c/p\u003e\n        \u003cul\u003e\n          \u003cli\u003e$for~ l = n-1, n-2, ..., 1$\n            \u003cul\u003e\n              \u003cli\u003e$for~ i = 1, 2, ..., n - l$\n                \u003cul\u003e\n                  \u003cli\u003e$if~ opt(i, i + l)$\n                    \u003cul\u003e\n                      \u003cli\u003ereturn l\u003c/li\u003e\n                    \u003c/ul\u003e\n                  \u003c/li\u003e\n                \u003c/ul\u003e\n              \u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003e\u003cstrong\u003eTime Complexity\u003c/strong\u003e\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eInitialization of opt is $O(n^2)$\u003c/li\u003e\n      \u003cli\u003eBuilding opt takes $O(n^2)$ iterations\n        \u003cul\u003e\n          \u003cli\u003eEach iteration takes $O(1)$ accesses to opt\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eSo in total, the time complexity is $O(n^2)$\u003c/p\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"aws/best-practices","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/aws/best-practices.md","metadata":{"title":"Aws, Best practices","description":null,"permalink":null,"priority":0,"tags":[],"categories":["Aws"]},"content":{"raw":"\n### Do not use the root acount for any task that is not required to be performed by a root user\n\nWhat else can we do then? Create a different user and specifically control permissions.\n\nEven ceating an Administrator user is good - The point is, the use the account owner.","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch3\u003eDo not use the root acount for any task that is not required to be performed by a root user\u003c/h3\u003e\n    \u003cp\u003eWhat else can we do then? Create a different user and specifically control permissions.\u003c/p\u003e\n    \u003cp\u003eEven ceating an Administrator user is good - The point is, the use the account owner.\u003c/p\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"aws/bullets","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/aws/bullets.md","metadata":{"title":"Aws, Bullets","description":null,"permalink":null,"priority":0,"tags":[],"categories":["Aws"]},"content":{"raw":"\n# Available Storage Types\n\n- Block\n    - EBS\n- File\n    - EFS\n    - FSx Lustre\n    - FSx Windows\n- Block\n    - S3\n    - Glacier\n\n# EBS Volume Types\n\n**EBS** is a block storage.\n\nIt provides one of the following volume types, with the following categories\n\n- Solid state (SSD)\n    - General Purpose SSD - Provides a sane cost/performance balance\n    - Provisioned IOPS SSD - High performance, mission-critical, low latency and high-throughput\n- Hard disk drives (HDD)\n    - Throughput Optimized HDD - Low cost HDD for throughput intensive workloads\n    - Cold HDD - Lowest cost HDD for less frequently accessed workloads\n- Previous generation\n\n# ELB\n\n- Application Load Balancer (ALB) - Works at application layer (layer 7 osi) HTTP/HTTPS\n    - Host based routing - Balance using the host and port portion of the url (scheme://host:port/path)\n    - Path based routing - Balance using the path portion of the url (scheme://host:port/path)\n\n- Network Load Balancer (NLB) - Works at the network layer (layer 4 osi) IP\n\n# Route53 routing policies\n\n- Simple Round Robin - Route the user in a round robin fashion accross servers\n- Weighted - Weight precentage of routes for each server\n- Geolocation - Route the user to the geographically nearest server\n\n# EC2 Instance Types\n\n- General Purpose (m4)\n- Compute Optimized\n- Memory Optimized\n- Accelerated Computing - Using hardware accelerators\n- Storage Optimized\n- High Memory - Acquired only using special request\n","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch1\u003eAvailable Storage Types\u003c/h1\u003e\n    \u003cul\u003e\n      \u003cli\u003eBlock\n        \u003cul\u003e\n          \u003cli\u003eEBS\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003eFile\n        \u003cul\u003e\n          \u003cli\u003eEFS\u003c/li\u003e\n          \u003cli\u003eFSx Lustre\u003c/li\u003e\n          \u003cli\u003eFSx Windows\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003eBlock\n        \u003cul\u003e\n          \u003cli\u003eS3\u003c/li\u003e\n          \u003cli\u003eGlacier\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch1\u003eEBS Volume Types\u003c/h1\u003e\n    \u003cp\u003e\u003cstrong\u003eEBS\u003c/strong\u003e is a block storage.\u003c/p\u003e\n    \u003cp\u003eIt provides one of the following volume types, with the following categories\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eSolid state (SSD)\n        \u003cul\u003e\n          \u003cli\u003eGeneral Purpose SSD - Provides a sane cost/performance balance\u003c/li\u003e\n          \u003cli\u003eProvisioned IOPS SSD - High performance, mission-critical, low latency and high-throughput\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003eHard disk drives (HDD)\n        \u003cul\u003e\n          \u003cli\u003eThroughput Optimized HDD - Low cost HDD for throughput intensive workloads\u003c/li\u003e\n          \u003cli\u003eCold HDD - Lowest cost HDD for less frequently accessed workloads\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003ePrevious generation\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch1\u003eELB\u003c/h1\u003e\n    \u003cul\u003e\n      \u003cli\u003e\n        \u003cp\u003eApplication Load Balancer (ALB) - Works at application layer (layer 7 osi) HTTP/HTTPS\u003c/p\u003e\n        \u003cul\u003e\n          \u003cli\u003eHost based routing - Balance using the host and port portion of the url (scheme://host:port/path)\u003c/li\u003e\n          \u003cli\u003ePath based routing - Balance using the path portion of the url (scheme://host:port/path)\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\n        \u003cp\u003eNetwork Load Balancer (NLB) - Works at the network layer (layer 4 osi) IP\u003c/p\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch1\u003eRoute53 routing policies\u003c/h1\u003e\n    \u003cul\u003e\n      \u003cli\u003eSimple Round Robin - Route the user in a round robin fashion accross servers\u003c/li\u003e\n      \u003cli\u003eWeighted - Weight precentage of routes for each server\u003c/li\u003e\n      \u003cli\u003eGeolocation - Route the user to the geographically nearest server\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch1\u003eEC2 Instance Types\u003c/h1\u003e\n    \u003cul\u003e\n      \u003cli\u003eGeneral Purpose (m4)\u003c/li\u003e\n      \u003cli\u003eCompute Optimized\u003c/li\u003e\n      \u003cli\u003eMemory Optimized\u003c/li\u003e\n      \u003cli\u003eAccelerated Computing - Using hardware accelerators\u003c/li\u003e\n      \u003cli\u003eStorage Optimized\u003c/li\u003e\n      \u003cli\u003eHigh Memory - Acquired only using special request\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"aws/cli","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/aws/cli.md","metadata":{"title":"Aws, The cli","description":null,"permalink":null,"priority":0,"tags":[],"categories":["Aws"]},"content":{"raw":"\nThe _aws cli_ is a utility that captures all of the administration capabilities with aws.\n\n## Configuration\n\nWe can apply initial configuration using the `configure` command which will create a default configuration. This configuration is called a __profile__. For any action, if no profile is specified, the default one will be used.\n\n\nAlternatively, we can create named profiles and use each separately and specifically. The selected profile is determined using a cli argument or by an environment variable (AWS_PROFILE).\n\n{{\u003c alert \"Perhaps providing a test environment as the default is the safest option!\" \u003e}}\n\n## Cli vs Console\n\nThis is probably subjective, but I highly advocate the use of the cli.","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003cp\u003eThe \u003cem\u003eaws cli\u003c/em\u003e is a utility that captures all of the administration capabilities with aws.\u003c/p\u003e\n    \u003ch2\u003eConfiguration\u003c/h2\u003e\n    \u003cp\u003eWe can apply initial configuration using the \u003ccode\u003econfigure\u003c/code\u003e command which will create a default configuration. This configuration is called a \u003cstrong\u003eprofile\u003c/strong\u003e. For any action, if no profile is specified, the default one will be used.\u003c/p\u003e\n    \u003cp\u003eAlternatively, we can create named profiles and use each separately and specifically. The selected profile is determined using a cli argument or by an environment variable (AWS_PROFILE).\u003c/p\u003e\n    \u003cp\u003e{{\u0026#x3C; alert \"Perhaps providing a test environment as the default is the safest option!\" \u003e}}\u003c/p\u003e\n    \u003ch2\u003eCli vs Console\u003c/h2\u003e\n    \u003cp\u003eThis is probably subjective, but I highly advocate the use of the cli.\u003c/p\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"aws/kinesis","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/aws/kinesis.md","metadata":{"title":null,"description":null,"permalink":null,"priority":0,"tags":[],"categories":["Aws"]},"content":{"raw":"\nList streams\n\n    aws kinesis list-streams\n\nDescribe a _Stream_, listing it's _Shards_, _Stream's_ ARN, current _SequenceNumbers_ etc...\n\n    aws kinesis list-streams --stream-name {stream-name}\n\nTo list the consumers/producers of a given _Stream_\n\n    aws kinesis list-stream-consumers --stream-arn {stream-arn}\n\n## Getting records\n\n_ShardIterator_ is an object used to iterate _Records_ within a specific _Shard_. So, in order to get a _Shard's_ _Records_ we need to acquire a reference to a specific _ShardIterator_.\n\n    aws kines get-shard-iterator --stream-name {stream-name} --shard-id {shard-id} --shard-iterator-type {shard-iterator-type}\n\nThe ```shard-iterator-type``` has multiple choices, advise the documentation for those.\n\nThe ```get-shard-iterator``` command provided us with an identifier of the _ShardIterator_, we can use it to get _Records_ using\n\n    aws kinesis get-records --shard-iterator {shard-iterator}\n","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003cp\u003eList streams\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-dsconfig\"\u003e\u003cspan class=\"hljs-string\"\u003eaws\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003ekinesis\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003elist-streams\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eDescribe a \u003cem\u003eStream\u003c/em\u003e, listing it's \u003cem\u003eShards\u003c/em\u003e, \u003cem\u003eStream's\u003c/em\u003e ARN, current \u003cem\u003eSequenceNumbers\u003c/em\u003e etc...\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-dsconfig\"\u003e\u003cspan class=\"hljs-string\"\u003eaws\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003ekinesis\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003elist-streams\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003e--stream-name\u003c/span\u003e {\u003cspan class=\"hljs-string\"\u003estream-name\u003c/span\u003e}\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eTo list the consumers/producers of a given \u003cem\u003eStream\u003c/em\u003e\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-dsconfig\"\u003e\u003cspan class=\"hljs-string\"\u003eaws\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003ekinesis\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003elist-stream-consumers\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003e--stream-arn\u003c/span\u003e {\u003cspan class=\"hljs-string\"\u003estream-arn\u003c/span\u003e}\n\u003c/code\u003e\u003c/pre\u003e\n    \u003ch2\u003eGetting records\u003c/h2\u003e\n    \u003cp\u003e\u003cem\u003eShardIterator\u003c/em\u003e is an object used to iterate \u003cem\u003eRecords\u003c/em\u003e within a specific \u003cem\u003eShard\u003c/em\u003e. So, in order to get a \u003cem\u003eShard's\u003c/em\u003e \u003cem\u003eRecords\u003c/em\u003e we need to acquire a reference to a specific \u003cem\u003eShardIterator\u003c/em\u003e.\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-dsconfig\"\u003e\u003cspan class=\"hljs-string\"\u003eaws\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003ekines\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eget-shard-iterator\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003e--stream-name\u003c/span\u003e {\u003cspan class=\"hljs-string\"\u003estream-name\u003c/span\u003e} \u003cspan class=\"hljs-built_in\"\u003e--shard-id\u003c/span\u003e {\u003cspan class=\"hljs-string\"\u003eshard-id\u003c/span\u003e} \u003cspan class=\"hljs-built_in\"\u003e--shard-iterator-type\u003c/span\u003e {\u003cspan class=\"hljs-string\"\u003eshard-iterator-type\u003c/span\u003e}\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eThe \u003ccode\u003eshard-iterator-type\u003c/code\u003e has multiple choices, advise the documentation for those.\u003c/p\u003e\n    \u003cp\u003eThe \u003ccode\u003eget-shard-iterator\u003c/code\u003e command provided us with an identifier of the \u003cem\u003eShardIterator\u003c/em\u003e, we can use it to get \u003cem\u003eRecords\u003c/em\u003e using\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-dsconfig\"\u003e\u003cspan class=\"hljs-string\"\u003eaws\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003ekinesis\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eget-records\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003e--shard-iterator\u003c/span\u003e {\u003cspan class=\"hljs-string\"\u003eshard-iterator\u003c/span\u003e}\n\u003c/code\u003e\u003c/pre\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"aws/services","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/aws/services.md","metadata":{"title":"Services","description":null,"permalink":null,"priority":0,"tags":[],"categories":["Aws"]},"content":{"raw":"\n# S3\n\n- Object-Based, Serverless, Unlimited storage service\n- Data is replicated across at least 3 AZs which ensures 99.99% __Availability__ and 11' 9s of __Durability__\n- Objects contain data and can have size for 0 Byts to 5 Terabytes\n- Buckets names are unique globally and contain either Objects or Folders (which in turn contain Objects)\n\n- __Lifecycle Management__ is a mechanism to delete/move objects between __Storage Classes__ based on schedule or some criteria\n\n[Exampro Cheatsheet](https://youtu.be/Ia-UEYYR44s?t=3524)\n\nTypes of Replication\n- Cross-Region Replication (CRR) - Bucket is **asynchronously** replicated to another region\n- Same-Region Replication (SRR) - Bucket is **asynchronously** replicated to the same region\n\n# Snowball\n\n- Snowball\n- Snoball Edge\n- Snowmobile\n\n# VPC\n\n- VPC Peering\n- Route Tables\n- Internet Gateway\n- Bastion / Jumpbox\n- Direct Connect\n\n## VPC Endpoints\n\n- Interface Endpoints\n- Gateway Endpoints\n\n## VPC Flow Logs\n\n# NACL\n\n# Security Groups \n\n# NAT\n\n# IAM\n\n# COGNITO\n\n# DNS\n\n# Route 53\n\n# EC2\n\n## EC2 Pricing\n\n## AMI\n\n## Auto Scaling Groups\n\n## ELB\n\n# EFS\n\n# EBS\n\n# Cloud Front\n\n# Aurora\n\n# Redshift\n\n# DynamoDB\n\n# CloudFormation\n\n# CloudWatch\n\n# CloudTrail\n\n# Lambda\n\n# SQS\n\n# SNS\n\n# ElasticCache\n\n# High Availability\n\n# Elastic Beanstalk\n\n# Kinesis\n\nRealtime processing platform.\n\n# Storage Gateway\n\nProvides on-premise storage access to cloud storage.\n\nPractically you install a VM on the on-premise host which will can be connected as NFS/SMB.\n\nStorage Types\n- S3 File Gateway\n- FSx File Gateway\n- Tape Gateway\n- Volume Gateway\n\nModes\n- Gateway Stored - Access data in the VM and synchronously get data from remote\n- Cached Stored - Frequently accessed data is cached at the VM and data is invalidated asynchronously\n","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch1\u003eS3\u003c/h1\u003e\n    \u003cul\u003e\n      \u003cli\u003e\n        \u003cp\u003eObject-Based, Serverless, Unlimited storage service\u003c/p\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\n        \u003cp\u003eData is replicated across at least 3 AZs which ensures 99.99% \u003cstrong\u003eAvailability\u003c/strong\u003e and 11' 9s of \u003cstrong\u003eDurability\u003c/strong\u003e\u003c/p\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\n        \u003cp\u003eObjects contain data and can have size for 0 Byts to 5 Terabytes\u003c/p\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\n        \u003cp\u003eBuckets names are unique globally and contain either Objects or Folders (which in turn contain Objects)\u003c/p\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\n        \u003cp\u003e\u003cstrong\u003eLifecycle Management\u003c/strong\u003e is a mechanism to delete/move objects between \u003cstrong\u003eStorage Classes\u003c/strong\u003e based on schedule or some criteria\u003c/p\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003e\u003ca href=\"https://youtu.be/Ia-UEYYR44s?t=3524\"\u003eExampro Cheatsheet\u003c/a\u003e\u003c/p\u003e\n    \u003cp\u003eTypes of Replication\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eCross-Region Replication (CRR) - Bucket is \u003cstrong\u003easynchronously\u003c/strong\u003e replicated to another region\u003c/li\u003e\n      \u003cli\u003eSame-Region Replication (SRR) - Bucket is \u003cstrong\u003easynchronously\u003c/strong\u003e replicated to the same region\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch1\u003eSnowball\u003c/h1\u003e\n    \u003cul\u003e\n      \u003cli\u003eSnowball\u003c/li\u003e\n      \u003cli\u003eSnoball Edge\u003c/li\u003e\n      \u003cli\u003eSnowmobile\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch1\u003eVPC\u003c/h1\u003e\n    \u003cul\u003e\n      \u003cli\u003eVPC Peering\u003c/li\u003e\n      \u003cli\u003eRoute Tables\u003c/li\u003e\n      \u003cli\u003eInternet Gateway\u003c/li\u003e\n      \u003cli\u003eBastion / Jumpbox\u003c/li\u003e\n      \u003cli\u003eDirect Connect\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch2\u003eVPC Endpoints\u003c/h2\u003e\n    \u003cul\u003e\n      \u003cli\u003eInterface Endpoints\u003c/li\u003e\n      \u003cli\u003eGateway Endpoints\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch2\u003eVPC Flow Logs\u003c/h2\u003e\n    \u003ch1\u003eNACL\u003c/h1\u003e\n    \u003ch1\u003eSecurity Groups\u003c/h1\u003e\n    \u003ch1\u003eNAT\u003c/h1\u003e\n    \u003ch1\u003eIAM\u003c/h1\u003e\n    \u003ch1\u003eCOGNITO\u003c/h1\u003e\n    \u003ch1\u003eDNS\u003c/h1\u003e\n    \u003ch1\u003eRoute 53\u003c/h1\u003e\n    \u003ch1\u003eEC2\u003c/h1\u003e\n    \u003ch2\u003eEC2 Pricing\u003c/h2\u003e\n    \u003ch2\u003eAMI\u003c/h2\u003e\n    \u003ch2\u003eAuto Scaling Groups\u003c/h2\u003e\n    \u003ch2\u003eELB\u003c/h2\u003e\n    \u003ch1\u003eEFS\u003c/h1\u003e\n    \u003ch1\u003eEBS\u003c/h1\u003e\n    \u003ch1\u003eCloud Front\u003c/h1\u003e\n    \u003ch1\u003eAurora\u003c/h1\u003e\n    \u003ch1\u003eRedshift\u003c/h1\u003e\n    \u003ch1\u003eDynamoDB\u003c/h1\u003e\n    \u003ch1\u003eCloudFormation\u003c/h1\u003e\n    \u003ch1\u003eCloudWatch\u003c/h1\u003e\n    \u003ch1\u003eCloudTrail\u003c/h1\u003e\n    \u003ch1\u003eLambda\u003c/h1\u003e\n    \u003ch1\u003eSQS\u003c/h1\u003e\n    \u003ch1\u003eSNS\u003c/h1\u003e\n    \u003ch1\u003eElasticCache\u003c/h1\u003e\n    \u003ch1\u003eHigh Availability\u003c/h1\u003e\n    \u003ch1\u003eElastic Beanstalk\u003c/h1\u003e\n    \u003ch1\u003eKinesis\u003c/h1\u003e\n    \u003cp\u003eRealtime processing platform.\u003c/p\u003e\n    \u003ch1\u003eStorage Gateway\u003c/h1\u003e\n    \u003cp\u003eProvides on-premise storage access to cloud storage.\u003c/p\u003e\n    \u003cp\u003ePractically you install a VM on the on-premise host which will can be connected as NFS/SMB.\u003c/p\u003e\n    \u003cp\u003eStorage Types\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eS3 File Gateway\u003c/li\u003e\n      \u003cli\u003eFSx File Gateway\u003c/li\u003e\n      \u003cli\u003eTape Gateway\u003c/li\u003e\n      \u003cli\u003eVolume Gateway\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eModes\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eGateway Stored - Access data in the VM and synchronously get data from remote\u003c/li\u003e\n      \u003cli\u003eCached Stored - Frequently accessed data is cached at the VM and data is invalidated asynchronously\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"ddd/applying","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/ddd/applying.md","metadata":{"title":"Applying","description":null,"permalink":null,"priority":0,"tags":[],"categories":["Domain Driven Design"]},"content":{"raw":"\n## Identify the possible Subdomains\n\n1. Core\n2. Support\n3. Generic\n\n## Split the Domain into Subdomains\n\nIf possible apply a single _Bounded Context_ for each _Subdomain_","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch2\u003eIdentify the possible Subdomains\u003c/h2\u003e\n    \u003col\u003e\n      \u003cli\u003eCore\u003c/li\u003e\n      \u003cli\u003eSupport\u003c/li\u003e\n      \u003cli\u003eGeneric\u003c/li\u003e\n    \u003c/ol\u003e\n    \u003ch2\u003eSplit the Domain into Subdomains\u003c/h2\u003e\n    \u003cp\u003eIf possible apply a single \u003cem\u003eBounded Context\u003c/em\u003e for each \u003cem\u003eSubdomain\u003c/em\u003e\u003c/p\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"ddd/model-refinement-steps","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/ddd/model-refinement-steps.md","metadata":{"title":"Model Refinement Steps","description":null,"permalink":null,"priority":0,"tags":[],"categories":["Domain Driven Design"]},"content":{"raw":"\n# Distinguishing Entities and Value Objects\n\nConsider each object in turn and try to identify an identity.\n\nConsider\n- How to track the entity?\n- Are to instances with same values are the same?\n- Can it exist without some parent object?\n\n# Designing Associations\n\nSpecify traversal directions.\n\nConsider\n- How the is application used?\n\nAvoid\n- Bi-Directional associations\n\n# Identifying Aggregate Boundaries\n\n# Selecting Repositories\n\n# Walking through scenarios\n\n# Iterate\n","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch1\u003eDistinguishing Entities and Value Objects\u003c/h1\u003e\n    \u003cp\u003eConsider each object in turn and try to identify an identity.\u003c/p\u003e\n    \u003cp\u003eConsider\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eHow to track the entity?\u003c/li\u003e\n      \u003cli\u003eAre to instances with same values are the same?\u003c/li\u003e\n      \u003cli\u003eCan it exist without some parent object?\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch1\u003eDesigning Associations\u003c/h1\u003e\n    \u003cp\u003eSpecify traversal directions.\u003c/p\u003e\n    \u003cp\u003eConsider\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eHow the is application used?\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eAvoid\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eBi-Directional associations\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch1\u003eIdentifying Aggregate Boundaries\u003c/h1\u003e\n    \u003ch1\u003eSelecting Repositories\u003c/h1\u003e\n    \u003ch1\u003eWalking through scenarios\u003c/h1\u003e\n    \u003ch1\u003eIterate\u003c/h1\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"ddd/prologue","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/ddd/prologue.md","metadata":{"title":"Domain Driven Design, Prologue","description":null,"permalink":null,"priority":0,"tags":[],"categories":["Domain Driven Design"]},"content":{"raw":"","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"ddd/tactical-design","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/ddd/tactical-design.md","metadata":{"title":"Tactical Design","description":null,"permalink":null,"priority":0,"tags":[],"categories":["Domain Driven Design"]},"content":{"raw":"\n# Entities\n\nEntities are the building blocks of the model.\n\nEntities are differentiated by their Id, not by their attributes.\n\n# Value Objects\n\nValue objects are differentiated by their attributes and contain no Id.\n\nValue objects should be able to implemented in an immutable fashion.\n\n# Services\n\nIn the exact same words\n\n\u003e A SERVICE is an operation offered as an interface that stands alone in the model, without encapsulating state, as ENTITIES and VALUE OBJECTS do.\n\nCharacterisitcs of a good SERVICE\n1. The operation relates to a domain concept that is not a natural part of an ENTITY or VALUE OBJECT\n1. The interface is defined in terms of other elements of the domain model\n1. The operation is stateless\n\n# Aggregates\n\nIn the exact same words\n\n\u003e An AGGREGATE is a cluster of associated objects that we treat as a unit for the purpose of data changes\n\nEach AGGREGATE has a **root** and a **boundary**.\n\nThe **boundary** delineate objects within the AGGREGATE.\n\nThe **root** is a *single*, *specific* ENTITY that the outside can hold references to.\n\n# Factories\n\nIn the exact same words\n\n\u003e When creation of an object, or an entire AGGREGATE, becomes complicated or reveals too much of the internal structure, FACTORIES provide encapsulation\n\nWhile every ENTITY has a constructor receiving it's idetity, FACTORIES will provide mechanisms to create complex ENTITIES and AGGREGATES.\n\nFACTORIES are responsible for ensuring that all invariants are met for the AGGREGATE's objects.\n\n# Repositories\n\nIn the exact same words\n\n\u003e A REPOSITORY represents all objects of a certain type as a conceptual set (usually emulated).\n\nThe REPOSITORIES act like collections but they often provide additional query mechanisms/options.\n","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch1\u003eEntities\u003c/h1\u003e\n    \u003cp\u003eEntities are the building blocks of the model.\u003c/p\u003e\n    \u003cp\u003eEntities are differentiated by their Id, not by their attributes.\u003c/p\u003e\n    \u003ch1\u003eValue Objects\u003c/h1\u003e\n    \u003cp\u003eValue objects are differentiated by their attributes and contain no Id.\u003c/p\u003e\n    \u003cp\u003eValue objects should be able to implemented in an immutable fashion.\u003c/p\u003e\n    \u003ch1\u003eServices\u003c/h1\u003e\n    \u003cp\u003eIn the exact same words\u003c/p\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003eA SERVICE is an operation offered as an interface that stands alone in the model, without encapsulating state, as ENTITIES and VALUE OBJECTS do.\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003cp\u003eCharacterisitcs of a good SERVICE\u003c/p\u003e\n    \u003col\u003e\n      \u003cli\u003eThe operation relates to a domain concept that is not a natural part of an ENTITY or VALUE OBJECT\u003c/li\u003e\n      \u003cli\u003eThe interface is defined in terms of other elements of the domain model\u003c/li\u003e\n      \u003cli\u003eThe operation is stateless\u003c/li\u003e\n    \u003c/ol\u003e\n    \u003ch1\u003eAggregates\u003c/h1\u003e\n    \u003cp\u003eIn the exact same words\u003c/p\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003eAn AGGREGATE is a cluster of associated objects that we treat as a unit for the purpose of data changes\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003cp\u003eEach AGGREGATE has a \u003cstrong\u003eroot\u003c/strong\u003e and a \u003cstrong\u003eboundary\u003c/strong\u003e.\u003c/p\u003e\n    \u003cp\u003eThe \u003cstrong\u003eboundary\u003c/strong\u003e delineate objects within the AGGREGATE.\u003c/p\u003e\n    \u003cp\u003eThe \u003cstrong\u003eroot\u003c/strong\u003e is a \u003cem\u003esingle\u003c/em\u003e, \u003cem\u003especific\u003c/em\u003e ENTITY that the outside can hold references to.\u003c/p\u003e\n    \u003ch1\u003eFactories\u003c/h1\u003e\n    \u003cp\u003eIn the exact same words\u003c/p\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003eWhen creation of an object, or an entire AGGREGATE, becomes complicated or reveals too much of the internal structure, FACTORIES provide encapsulation\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003cp\u003eWhile every ENTITY has a constructor receiving it's idetity, FACTORIES will provide mechanisms to create complex ENTITIES and AGGREGATES.\u003c/p\u003e\n    \u003cp\u003eFACTORIES are responsible for ensuring that all invariants are met for the AGGREGATE's objects.\u003c/p\u003e\n    \u003ch1\u003eRepositories\u003c/h1\u003e\n    \u003cp\u003eIn the exact same words\u003c/p\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003eA REPOSITORY represents all objects of a certain type as a conceptual set (usually emulated).\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003cp\u003eThe REPOSITORIES act like collections but they often provide additional query mechanisms/options.\u003c/p\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"kubernetes/deployments","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/kubernetes/deployments.md","metadata":{"title":"Kubernetes Deployments","description":null,"permalink":null,"priority":0,"tags":["Kubernetes"],"categories":["Kubernetes"]},"content":{"raw":"\n# Deployments\n\nA _Deployment_ manages _ReplicaSets_ and _ReplicaSets_ manage _Pods_.\n\n_ReplicaSet_ manage _Pods_ and bring self-healing and scaling capabilities while _Deployments_ manage _ReplicaSets_ and add rollout and rollback capabilities.\n\n### Self-healing and scalability\n\nIf _Pods_ managed by a _Deployment_ fail, they will be replaced - this is known as _self healing_.\n\nIf _Pods_ managed by a _Deployment_ see increased/decreased load, they will be _scaled_.\n\nIn Kubernetes there are 3 related concepts\n\n- _desired state_\n- _observerd state_\n- _reconciliation_\n\n_ReplicaSets_ are implemented as a controller running background process comparing the _desired state_ vs the _observed state_. If they are different it contacts the cluster to perform _reconciliation_.\n\n### Rolling updates\n\nZero downtime, rolling updates (a.k.a rollouts) can be supported when a service has backward and forward compatibility. One by one, the _ReplicaSet_ bring a replica down and introduces a new one with the designated version until all of the _Pods_ are updated with the desired version.\n\nIt is crucial that the services be stateless and backward/forward compatible for this to work.\n\n### Rollbacks\n\n### Commands\n\nTo scale a _Deployment_\n\n    kubectl scale deployment {deployment-name} --replicas {number-of-replicas}\n\nAfter changing image versions, initiate rollouts simply by reaplying a manifest\n\n    kubectl apply -f {manifest-path}\n\nWe can monitor the rollout progress by\n\n    kubectl rollout status deployment {deployment-name}\n\nTo pause a rollout\n\n    kubectl rollout pause deployment {deployment-name}\n\nTo resume a rollout\n\n    kubectl rollout resume deployment {deployment-name}\n\nIn the manifests we can specify ```revisionHistoryLimit``` for containers. \n\nTo show rollout history\n\n    kubectl rollout history deployment {deployment-name}\n\nTo rollback to a revision\n\n    kubectl rollout undo deployment {deployment-name} --to-revision={revision-number}\n\n\n","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch1\u003eDeployments\u003c/h1\u003e\n    \u003cp\u003eA \u003cem\u003eDeployment\u003c/em\u003e manages \u003cem\u003eReplicaSets\u003c/em\u003e and \u003cem\u003eReplicaSets\u003c/em\u003e manage \u003cem\u003ePods\u003c/em\u003e.\u003c/p\u003e\n    \u003cp\u003e\u003cem\u003eReplicaSet\u003c/em\u003e manage \u003cem\u003ePods\u003c/em\u003e and bring self-healing and scaling capabilities while \u003cem\u003eDeployments\u003c/em\u003e manage \u003cem\u003eReplicaSets\u003c/em\u003e and add rollout and rollback capabilities.\u003c/p\u003e\n    \u003ch3\u003eSelf-healing and scalability\u003c/h3\u003e\n    \u003cp\u003eIf \u003cem\u003ePods\u003c/em\u003e managed by a \u003cem\u003eDeployment\u003c/em\u003e fail, they will be replaced - this is known as \u003cem\u003eself healing\u003c/em\u003e.\u003c/p\u003e\n    \u003cp\u003eIf \u003cem\u003ePods\u003c/em\u003e managed by a \u003cem\u003eDeployment\u003c/em\u003e see increased/decreased load, they will be \u003cem\u003escaled\u003c/em\u003e.\u003c/p\u003e\n    \u003cp\u003eIn Kubernetes there are 3 related concepts\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\u003cem\u003edesired state\u003c/em\u003e\u003c/li\u003e\n      \u003cli\u003e\u003cem\u003eobserverd state\u003c/em\u003e\u003c/li\u003e\n      \u003cli\u003e\u003cem\u003ereconciliation\u003c/em\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003e\u003cem\u003eReplicaSets\u003c/em\u003e are implemented as a controller running background process comparing the \u003cem\u003edesired state\u003c/em\u003e vs the \u003cem\u003eobserved state\u003c/em\u003e. If they are different it contacts the cluster to perform \u003cem\u003ereconciliation\u003c/em\u003e.\u003c/p\u003e\n    \u003ch3\u003eRolling updates\u003c/h3\u003e\n    \u003cp\u003eZero downtime, rolling updates (a.k.a rollouts) can be supported when a service has backward and forward compatibility. One by one, the \u003cem\u003eReplicaSet\u003c/em\u003e bring a replica down and introduces a new one with the designated version until all of the \u003cem\u003ePods\u003c/em\u003e are updated with the desired version.\u003c/p\u003e\n    \u003cp\u003eIt is crucial that the services be stateless and backward/forward compatible for this to work.\u003c/p\u003e\n    \u003ch3\u003eRollbacks\u003c/h3\u003e\n    \u003ch3\u003eCommands\u003c/h3\u003e\n    \u003cp\u003eTo scale a \u003cem\u003eDeployment\u003c/em\u003e\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-fortran\"\u003ekubectl \u003cspan class=\"hljs-built_in\"\u003escale\u003c/span\u003e deployment {deployment-\u003cspan class=\"hljs-keyword\"\u003ename\u003c/span\u003e} --replicas {\u003cspan class=\"hljs-keyword\"\u003enumber\u003c/span\u003e-of-replicas}\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eAfter changing image versions, initiate rollouts simply by reaplying a manifest\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-puppet\"\u003ekubectl apply -\u003cspan class=\"hljs-keyword\"\u003ef\u003c/span\u003e {\u003cspan class=\"hljs-literal\"\u003emanifest\u003c/span\u003e-\u003cspan class=\"hljs-built_in\"\u003epath\u003c/span\u003e}\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eWe can monitor the rollout progress by\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-fortran\"\u003ekubectl rollout \u003cspan class=\"hljs-keyword\"\u003estatus\u003c/span\u003e deployment {deployment-\u003cspan class=\"hljs-keyword\"\u003ename\u003c/span\u003e}\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eTo pause a rollout\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-fortran\"\u003ekubectl rollout \u003cspan class=\"hljs-keyword\"\u003epause\u003c/span\u003e deployment {deployment-\u003cspan class=\"hljs-keyword\"\u003ename\u003c/span\u003e}\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eTo resume a rollout\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-basic\"\u003ekubectl rollout \u003cspan class=\"hljs-keyword\"\u003eresume\u003c/span\u003e deployment {deployment-\u003cspan class=\"hljs-keyword\"\u003ename\u003c/span\u003e}\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eIn the manifests we can specify \u003ccode\u003erevisionHistoryLimit\u003c/code\u003e for containers.\u003c/p\u003e\n    \u003cp\u003eTo show rollout history\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-properties\"\u003e\u003cspan class=\"hljs-attr\"\u003ekubectl\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003erollout history deployment {deployment-name}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eTo rollback to a revision\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-vim\"\u003ekubectl rollout \u003cspan class=\"hljs-keyword\"\u003eundo\u003c/span\u003e deployment {deployment-name} --\u003cspan class=\"hljs-keyword\"\u003eto\u003c/span\u003e-revision={revision-\u003cspan class=\"hljs-keyword\"\u003enumber\u003c/span\u003e}\n\u003c/code\u003e\u003c/pre\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"kubernetes/service-discovery","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/kubernetes/service-discovery.md","metadata":{"title":"Service Discovery","description":null,"permalink":null,"priority":0,"tags":["Kubernetes"],"categories":["Kubernetes"]},"content":{"raw":"\n# Service Discovery\n\nService discovery is a mean for applications to find one another in the cluster.\n\nThere are two major components to service discovery\n\n- Registration\n- Discovery\n\nService registration is when an application registers itself in a _service registry_.\n\nKubernetes uses its internal DNS as a _service registry_, and as we know, _Services_ are automatically registered with DNS.\n\nFor discovery to work, Kubernetes provides a well-known internal DNS services that are called the __cluster DNS__, in the namespace __kube-system__. Every _Pod_ in the cluster is automatically configured to where this service is. The relevant _Pods_ are managed by a _Deployment_ called __coredns__ and frontend by a _Service_ called __kube-dns__. \n\nFor illustration\n\n```\nkubectl get pods --namespace kube-system --selector k8s-app=kube-dns\n\nNAME                       READY   STATUS    RESTARTS   AGE\ncoredns-6d4b75cb6d-4lpv9   1/1     Running   0          139m\ncoredns-6d4b75cb6d-vmkfz   1/1     Running   0          139m\n\n```\n\nWe specify a _Service_ DNS using it's name (in the metadata)\n","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch1\u003eService Discovery\u003c/h1\u003e\n    \u003cp\u003eService discovery is a mean for applications to find one another in the cluster.\u003c/p\u003e\n    \u003cp\u003eThere are two major components to service discovery\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eRegistration\u003c/li\u003e\n      \u003cli\u003eDiscovery\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eService registration is when an application registers itself in a \u003cem\u003eservice registry\u003c/em\u003e.\u003c/p\u003e\n    \u003cp\u003eKubernetes uses its internal DNS as a \u003cem\u003eservice registry\u003c/em\u003e, and as we know, \u003cem\u003eServices\u003c/em\u003e are automatically registered with DNS.\u003c/p\u003e\n    \u003cp\u003eFor discovery to work, Kubernetes provides a well-known internal DNS services that are called the \u003cstrong\u003ecluster DNS\u003c/strong\u003e, in the namespace \u003cstrong\u003ekube-system\u003c/strong\u003e. Every \u003cem\u003ePod\u003c/em\u003e in the cluster is automatically configured to where this service is. The relevant \u003cem\u003ePods\u003c/em\u003e are managed by a \u003cem\u003eDeployment\u003c/em\u003e called \u003cstrong\u003ecoredns\u003c/strong\u003e and frontend by a \u003cem\u003eService\u003c/em\u003e called \u003cstrong\u003ekube-dns\u003c/strong\u003e.\u003c/p\u003e\n    \u003cp\u003eFor illustration\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-angelscript\"\u003ekubectl \u003cspan class=\"hljs-keyword\"\u003eget\u003c/span\u003e pods --\u003cspan class=\"hljs-keyword\"\u003enamespace\u003c/span\u003e \u003cspan class=\"hljs-symbol\"\u003ekube\u003c/span\u003e-\u003cspan class=\"hljs-symbol\"\u003esystem\u003c/span\u003e --\u003cspan class=\"hljs-symbol\"\u003eselector\u003c/span\u003e \u003cspan class=\"hljs-symbol\"\u003ek8s\u003c/span\u003e-\u003cspan class=\"hljs-symbol\"\u003eapp\u003c/span\u003e=\u003cspan class=\"hljs-symbol\"\u003ekube\u003c/span\u003e-\u003cspan class=\"hljs-symbol\"\u003edns\u003c/span\u003e\n\n\u003cspan class=\"hljs-symbol\"\u003eNAME\u003c/span\u003e                       \u003cspan class=\"hljs-symbol\"\u003eREADY\u003c/span\u003e   \u003cspan class=\"hljs-symbol\"\u003eSTATUS\u003c/span\u003e    \u003cspan class=\"hljs-symbol\"\u003eRESTARTS\u003c/span\u003e   \u003cspan class=\"hljs-symbol\"\u003eAGE\u003c/span\u003e\n\u003cspan class=\"hljs-symbol\"\u003ecoredns\u003c/span\u003e-\u003cspan class=\"hljs-symbol\"\u003e6d4b75cb6d\u003c/span\u003e-\u003cspan class=\"hljs-symbol\"\u003e4lpv9\u003c/span\u003e   \u003cspan class=\"hljs-symbol\"\u003e1\u003c/span\u003e/\u003cspan class=\"hljs-symbol\"\u003e1\u003c/span\u003e     \u003cspan class=\"hljs-symbol\"\u003eRunning\u003c/span\u003e   \u003cspan class=\"hljs-symbol\"\u003e0\u003c/span\u003e          \u003cspan class=\"hljs-symbol\"\u003e139m\u003c/span\u003e\n\u003cspan class=\"hljs-symbol\"\u003ecoredns\u003c/span\u003e-\u003cspan class=\"hljs-symbol\"\u003e6d4b75cb6d\u003c/span\u003e-\u003cspan class=\"hljs-symbol\"\u003evmkfz\u003c/span\u003e   \u003cspan class=\"hljs-symbol\"\u003e1\u003c/span\u003e/\u003cspan class=\"hljs-symbol\"\u003e1\u003c/span\u003e     \u003cspan class=\"hljs-symbol\"\u003eRunning\u003c/span\u003e   \u003cspan class=\"hljs-symbol\"\u003e0\u003c/span\u003e          \u003cspan class=\"hljs-symbol\"\u003e139m\u003c/span\u003e\n\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eWe specify a \u003cem\u003eService\u003c/em\u003e DNS using it's name (in the metadata)\u003c/p\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"kubernetes/services","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/kubernetes/services.md","metadata":{"title":"Kubernetes Services","description":null,"permalink":null,"priority":0,"tags":["Kubernetes"],"categories":["Kubernetes"]},"content":{"raw":"\n# Services \n\n_Service_ provides reliable access to _Pods_.\n\nMain _Service_ concepts\n\n- _Services_ are REST objects in the API that we define in a manifest file or post to the API server.\n- Every service gets it's own __stable IP address__, it's own __stable DNS name__ and it's own __stable port__.\n- _Services_ use __labels__ and __selectors__ to dynamically select the _Pods_ they send traffic to.\n\n_Services_ get a list of healthy pods that match the relevant selctors using a Kubernetes object called an _Endpoint_. Kubernetes is continuously monitoring the state of the _Pods_ and updates the relevant _Endpoints'_ lists.\n\n```bash\n              +----------------+     +------------+     +--------------------------+\n{request} --\u003e | DNS resolution | --\u003e | Service IP | --\u003e | Pod in the Endpoint List |\n              +----------------+     +------------+     +--------------------------+\n```\n\nKubernetes native applications can query the API and directly find the _Service_ IP, bypassing DNS resolution.\n\n","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch1\u003eServices\u003c/h1\u003e\n    \u003cp\u003e\u003cem\u003eService\u003c/em\u003e provides reliable access to \u003cem\u003ePods\u003c/em\u003e.\u003c/p\u003e\n    \u003cp\u003eMain \u003cem\u003eService\u003c/em\u003e concepts\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\u003cem\u003eServices\u003c/em\u003e are REST objects in the API that we define in a manifest file or post to the API server.\u003c/li\u003e\n      \u003cli\u003eEvery service gets it's own \u003cstrong\u003estable IP address\u003c/strong\u003e, it's own \u003cstrong\u003estable DNS name\u003c/strong\u003e and it's own \u003cstrong\u003estable port\u003c/strong\u003e.\u003c/li\u003e\n      \u003cli\u003e\u003cem\u003eServices\u003c/em\u003e use \u003cstrong\u003elabels\u003c/strong\u003e and \u003cstrong\u003eselectors\u003c/strong\u003e to dynamically select the \u003cem\u003ePods\u003c/em\u003e they send traffic to.\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003e\u003cem\u003eServices\u003c/em\u003e get a list of healthy pods that match the relevant selctors using a Kubernetes object called an \u003cem\u003eEndpoint\u003c/em\u003e. Kubernetes is continuously monitoring the state of the \u003cem\u003ePods\u003c/em\u003e and updates the relevant \u003cem\u003eEndpoints'\u003c/em\u003e lists.\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e              +----------------+     +------------+     +--------------------------+\n{request} --\u003e | DNS resolution | --\u003e | Service IP | --\u003e | Pod \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e the Endpoint List |\n              +----------------+     +------------+     +--------------------------+\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eKubernetes native applications can query the API and directly find the \u003cem\u003eService\u003c/em\u003e IP, bypassing DNS resolution.\u003c/p\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"kubernetes/simple-on-prem-cluster","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/kubernetes/simple-on-prem-cluster.md","metadata":{"title":"Simple on-prem Kuberenetes cluster","description":null,"permalink":null,"priority":0,"tags":["Kubernetes"],"categories":["Kubernetes"]},"content":{"raw":"\n# Kubes\n\nDeploy a kubernetes cluster.\n\nWe will setup a simple kubernetes cluster will describe the concepts and process.\n\nThe OS on all nodes is debian bullseye - I specifically executing this using vagrant's box ```debian/bullseye64```.\n\nTo document the steps we will provide bash script snippets for now. But, the goal is to provide configuration files content for something like ansible or chef.\n\n## Container runtime\n\nKubernetes is an orchestration infrastructure and does not provide any containerization - It relies on a different containarization platform, a.k.a the [Container Runtime](https://kubernetes.io/docs/setup/production-environment/container-runtimes/).\n\nWe will use [docker](https://www.docker.com/). Let's install it by following the documentation [Here](https://docs.docker.com/engine/install/debian/).\n\n```\nsudo apt-get remove docker docker-engine docker.io containerd runc\n\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg lsb-release\n\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\\n  $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null\n\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io\n```\n\nAnother thing to do is docker to utilize systemd for cgroup management (You can read more about it in the container runtimes documentation). To apply this setting we need to edit docker's configuration. There are multiple to do so, for example - edit systemd's service that initiates docker. Another approach is to edit docker's global configuration file which is at ```/etc/docker/daemon.json```. Hence, edit (create if missing) the mentioned file and add\n\n```json\n{\n    ... other configurations\n    \"exec-opts\": [\"native.cgroupdriver=systemd\", ... more exec opts if exists]\n}\n```\n\nOnce done, reboot docker by running ```sudo systemctl restart docker```\n\n## Kube Components\n\nWe will not rely on the package manager to install the components.\n\nDefine the relevant variables\n\n\u003e Note that the cni directory does not include the version! Later we will install a network plugin, it'll be in the same directory\n\n```\nARCH=\"amd64\"\nCNI_VERSION=\"v0.8.2\"\nCNI_DIR=\"/opt/cni/bin\"\nCRICTL_VERSION=\"v1.23.0\"\nCRICTL_DIR=\"/opt/cri/$CRICTL_VERSION/bin\"\nKUBERNETES_VERSION=\"v1.23.3\"\nKUBERNETES_DIR=\"/opt/kubernetes/$KUBERNETES_VERSION\"\n\n# Install [CNI](https://www.cni.dev/)\n\nsudo mkdir -p $CNI_DIR\ncurl -L \"https://github.com/containernetworking/plugins/releases/download/${CNI_VERSION}/cni-plugins-linux-${ARCH}-${CNI_VERSION}.tgz\" | sudo tar -C $CNI_DIR -xz\n\n# Install [CRI](https://kubernetes.io/docs/concepts/architecture/cri/)\n\nsudo mkdir -p $CRICTL_DIR\ncurl -L \"https://github.com/kubernetes-sigs/cri-tools/releases/download/${CRICTL_VERSION}/crictl-${CRICTL_VERSION}-linux-${ARCH}.tar.gz\" | sudo tar -C $CRICTL_DIR -xz\n\n# Install Kube components\n\nsudo mkdir -p $KUBERNETES_DIR\ncd $KUBERNETES_DIR\nfor component in kubeadm kubectl kubelet; do\n  sudo curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/$KUBERNETES_VERSION/bin/linux/$ARCH/$component\n  sudo chmod +x $component\ndone\n\n# and services\n\nRELEASE_VERSION=\"v0.4.0\"\ncurl -sSL \"https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service\" | sed \"s:/usr/bin:${KUBERNETES_DIR}:g\" | sudo tee /etc/systemd/system/kubelet.service\nsudo mkdir -p /etc/systemd/system/kubelet.service.d\ncurl -sSL \"https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf\" | sed \"s:/usr/bin:${KUBERNETES_DIR}:g\" | sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n```\n\nenable, and start kubelet\n\n```\nsudo systemctl enable --now kubelet\n```\n\n## Initialization\n\nInstall prerequesites for kubeadm\n\n```\nsudo apt-get update \nsudo apt install ethtool socat conntrack\n```\n\nCreate an update alternative\n\n```\nsudo update-alternatives --install /usr/bin/kubeadm kubeadm $KUBERNETES_DIR/kubeadm 100\nsudo update-alternatives --install /usr/bin/kubelet kubelet $KUBERNETES_DIR/kubelet 100\nsudo update-alternatives --install /usr/bin/kubectl kubectl $KUBERNETES_DIR/kubectl 100\n```\n\nRun @controlplane\n\n\u003e TODO: load balancer, hostnames\n\nInitialize configuration such that the network is 10.10.0.0/16\n\n```\nsudo kubeadm init --pod-network-cidr 10.10.0.0/16 --apiserver-advertise-address {ip}\n```\n\nFor documentation, you should see something like\n\n```\nYour Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 192.168.121.210:6443 --token clns4a.b29f6anjipygy0e2 \\\n\t--discovery-token-ca-cert-hash sha256:833f599cc9ab27eb5010c499e9c77e8e3263fb991d8e9e78ef187ba97e1efb59\n```\n\nDo as it says, run\n\n```\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n```\n\nWe will use [Weave Net](https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/weave-network-policy/) as a network plugin\n\n```\nkubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\"\n```\n","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch1\u003eKubes\u003c/h1\u003e\n    \u003cp\u003eDeploy a kubernetes cluster.\u003c/p\u003e\n    \u003cp\u003eWe will setup a simple kubernetes cluster will describe the concepts and process.\u003c/p\u003e\n    \u003cp\u003eThe OS on all nodes is debian bullseye - I specifically executing this using vagrant's box \u003ccode\u003edebian/bullseye64\u003c/code\u003e.\u003c/p\u003e\n    \u003cp\u003eTo document the steps we will provide bash script snippets for now. But, the goal is to provide configuration files content for something like ansible or chef.\u003c/p\u003e\n    \u003ch2\u003eContainer runtime\u003c/h2\u003e\n    \u003cp\u003eKubernetes is an orchestration infrastructure and does not provide any containerization - It relies on a different containarization platform, a.k.a the \u003ca href=\"https://kubernetes.io/docs/setup/production-environment/container-runtimes/\"\u003eContainer Runtime\u003c/a\u003e.\u003c/p\u003e\n    \u003cp\u003eWe will use \u003ca href=\"https://www.docker.com/\"\u003edocker\u003c/a\u003e. Let's install it by following the documentation \u003ca href=\"https://docs.docker.com/engine/install/debian/\"\u003eHere\u003c/a\u003e.\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-basic\"\u003esudo apt-\u003cspan class=\"hljs-keyword\"\u003eget\u003c/span\u003e \u003cspan class=\"hljs-comment\"\u003eremove docker docker-engine docker.io containerd runc\u003c/span\u003e\n\nsudo apt-\u003cspan class=\"hljs-keyword\"\u003eget\u003c/span\u003e update\nsudo apt-\u003cspan class=\"hljs-keyword\"\u003eget\u003c/span\u003e install ca-certificates curl gnupg lsb-release\n\ncurl -fsSL https://download.docker.\u003cspan class=\"hljs-keyword\"\u003ecom\u003c/span\u003e/linux/debian/gpg | sudo gpg --dearmor -o /\u003cspan class=\"hljs-keyword\"\u003eusr\u003c/span\u003e/share/keyrings/docker-archive-keyring.gpg\n\necho \\\n  \u003cspan class=\"hljs-string\"\u003e\"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\\n  $(lsb_release -cs) stable\"\u003c/span\u003e | sudo tee /etc/apt/sources.\u003cspan class=\"hljs-keyword\"\u003elist\u003c/span\u003e.d/docker.\u003cspan class=\"hljs-keyword\"\u003elist\u003c/span\u003e \u003e /dev/null\n\nsudo apt-\u003cspan class=\"hljs-keyword\"\u003eget\u003c/span\u003e update\nsudo apt-\u003cspan class=\"hljs-keyword\"\u003eget\u003c/span\u003e install docker-ce docker-ce-cli containerd.io\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eAnother thing to do is docker to utilize systemd for cgroup management (You can read more about it in the container runtimes documentation). To apply this setting we need to edit docker's configuration. There are multiple to do so, for example - edit systemd's service that initiates docker. Another approach is to edit docker's global configuration file which is at \u003ccode\u003e/etc/docker/daemon.json\u003c/code\u003e. Hence, edit (create if missing) the mentioned file and add\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-json\"\u003e\u003cspan class=\"hljs-punctuation\"\u003e{\u003c/span\u003e\n    ... other configurations\n    \u003cspan class=\"hljs-attr\"\u003e\"exec-opts\"\u003c/span\u003e\u003cspan class=\"hljs-punctuation\"\u003e:\u003c/span\u003e \u003cspan class=\"hljs-punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"native.cgroupdriver=systemd\"\u003c/span\u003e\u003cspan class=\"hljs-punctuation\"\u003e,\u003c/span\u003e ... more exec opts if exists\u003cspan class=\"hljs-punctuation\"\u003e]\u003c/span\u003e\n\u003cspan class=\"hljs-punctuation\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eOnce done, reboot docker by running \u003ccode\u003esudo systemctl restart docker\u003c/code\u003e\u003c/p\u003e\n    \u003ch2\u003eKube Components\u003c/h2\u003e\n    \u003cp\u003eWe will not rely on the package manager to install the components.\u003c/p\u003e\n    \u003cp\u003eDefine the relevant variables\u003c/p\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003eNote that the cni directory does not include the version! Later we will install a network plugin, it'll be in the same directory\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003eARCH=\u003cspan class=\"hljs-string\"\u003e\"amd64\"\u003c/span\u003e\nCNI_VERSION=\u003cspan class=\"hljs-string\"\u003e\"v0.8.2\"\u003c/span\u003e\nCNI_DIR=\u003cspan class=\"hljs-string\"\u003e\"/opt/cni/bin\"\u003c/span\u003e\nCRICTL_VERSION=\u003cspan class=\"hljs-string\"\u003e\"v1.23.0\"\u003c/span\u003e\nCRICTL_DIR=\u003cspan class=\"hljs-string\"\u003e\"/opt/cri/\u003cspan class=\"hljs-variable\"\u003e$CRICTL_VERSION\u003c/span\u003e/bin\"\u003c/span\u003e\nKUBERNETES_VERSION=\u003cspan class=\"hljs-string\"\u003e\"v1.23.3\"\u003c/span\u003e\nKUBERNETES_DIR=\u003cspan class=\"hljs-string\"\u003e\"/opt/kubernetes/\u003cspan class=\"hljs-variable\"\u003e$KUBERNETES_VERSION\u003c/span\u003e\"\u003c/span\u003e\n\n\u003cspan class=\"hljs-comment\"\u003e# Install [CNI](https://www.cni.dev/)\u003c/span\u003e\n\nsudo \u003cspan class=\"hljs-built_in\"\u003emkdir\u003c/span\u003e -p \u003cspan class=\"hljs-variable\"\u003e$CNI_DIR\u003c/span\u003e\ncurl -L \u003cspan class=\"hljs-string\"\u003e\"https://github.com/containernetworking/plugins/releases/download/\u003cspan class=\"hljs-variable\"\u003e${CNI_VERSION}\u003c/span\u003e/cni-plugins-linux-\u003cspan class=\"hljs-variable\"\u003e${ARCH}\u003c/span\u003e-\u003cspan class=\"hljs-variable\"\u003e${CNI_VERSION}\u003c/span\u003e.tgz\"\u003c/span\u003e | sudo tar -C \u003cspan class=\"hljs-variable\"\u003e$CNI_DIR\u003c/span\u003e -xz\n\n\u003cspan class=\"hljs-comment\"\u003e# Install [CRI](https://kubernetes.io/docs/concepts/architecture/cri/)\u003c/span\u003e\n\nsudo \u003cspan class=\"hljs-built_in\"\u003emkdir\u003c/span\u003e -p \u003cspan class=\"hljs-variable\"\u003e$CRICTL_DIR\u003c/span\u003e\ncurl -L \u003cspan class=\"hljs-string\"\u003e\"https://github.com/kubernetes-sigs/cri-tools/releases/download/\u003cspan class=\"hljs-variable\"\u003e${CRICTL_VERSION}\u003c/span\u003e/crictl-\u003cspan class=\"hljs-variable\"\u003e${CRICTL_VERSION}\u003c/span\u003e-linux-\u003cspan class=\"hljs-variable\"\u003e${ARCH}\u003c/span\u003e.tar.gz\"\u003c/span\u003e | sudo tar -C \u003cspan class=\"hljs-variable\"\u003e$CRICTL_DIR\u003c/span\u003e -xz\n\n\u003cspan class=\"hljs-comment\"\u003e# Install Kube components\u003c/span\u003e\n\nsudo \u003cspan class=\"hljs-built_in\"\u003emkdir\u003c/span\u003e -p \u003cspan class=\"hljs-variable\"\u003e$KUBERNETES_DIR\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003ecd\u003c/span\u003e \u003cspan class=\"hljs-variable\"\u003e$KUBERNETES_DIR\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e component \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e kubeadm kubectl kubelet; \u003cspan class=\"hljs-keyword\"\u003edo\u003c/span\u003e\n  sudo curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/\u003cspan class=\"hljs-variable\"\u003e$KUBERNETES_VERSION\u003c/span\u003e/bin/linux/\u003cspan class=\"hljs-variable\"\u003e$ARCH\u003c/span\u003e/\u003cspan class=\"hljs-variable\"\u003e$component\u003c/span\u003e\n  sudo \u003cspan class=\"hljs-built_in\"\u003echmod\u003c/span\u003e +x \u003cspan class=\"hljs-variable\"\u003e$component\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edone\u003c/span\u003e\n\n\u003cspan class=\"hljs-comment\"\u003e# and services\u003c/span\u003e\n\nRELEASE_VERSION=\u003cspan class=\"hljs-string\"\u003e\"v0.4.0\"\u003c/span\u003e\ncurl -sSL \u003cspan class=\"hljs-string\"\u003e\"https://raw.githubusercontent.com/kubernetes/release/\u003cspan class=\"hljs-variable\"\u003e${RELEASE_VERSION}\u003c/span\u003e/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service\"\u003c/span\u003e | sed \u003cspan class=\"hljs-string\"\u003e\"s:/usr/bin:\u003cspan class=\"hljs-variable\"\u003e${KUBERNETES_DIR}\u003c/span\u003e:g\"\u003c/span\u003e | sudo \u003cspan class=\"hljs-built_in\"\u003etee\u003c/span\u003e /etc/systemd/system/kubelet.service\nsudo \u003cspan class=\"hljs-built_in\"\u003emkdir\u003c/span\u003e -p /etc/systemd/system/kubelet.service.d\ncurl -sSL \u003cspan class=\"hljs-string\"\u003e\"https://raw.githubusercontent.com/kubernetes/release/\u003cspan class=\"hljs-variable\"\u003e${RELEASE_VERSION}\u003c/span\u003e/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf\"\u003c/span\u003e | sed \u003cspan class=\"hljs-string\"\u003e\"s:/usr/bin:\u003cspan class=\"hljs-variable\"\u003e${KUBERNETES_DIR}\u003c/span\u003e:g\"\u003c/span\u003e | sudo \u003cspan class=\"hljs-built_in\"\u003etee\u003c/span\u003e /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eenable, and start kubelet\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-pgsql\"\u003esudo systemctl \u003cspan class=\"hljs-keyword\"\u003eenable\u003c/span\u003e \u003cspan class=\"hljs-comment\"\u003e--now kubelet\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003ch2\u003eInitialization\u003c/h2\u003e\n    \u003cp\u003eInstall prerequesites for kubeadm\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-properties\"\u003e\u003cspan class=\"hljs-attr\"\u003esudo\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eapt-get update \u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003esudo\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eapt install ethtool socat conntrack\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eCreate an update alternative\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-awk\"\u003esudo update-alternatives --install \u003cspan class=\"hljs-regexp\"\u003e/usr/\u003c/span\u003ebin\u003cspan class=\"hljs-regexp\"\u003e/kubeadm kubeadm $KUBERNETES_DIR/\u003c/span\u003ekubeadm \u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e\nsudo update-alternatives --install \u003cspan class=\"hljs-regexp\"\u003e/usr/\u003c/span\u003ebin\u003cspan class=\"hljs-regexp\"\u003e/kubelet kubelet $KUBERNETES_DIR/\u003c/span\u003ekubelet \u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e\nsudo update-alternatives --install \u003cspan class=\"hljs-regexp\"\u003e/usr/\u003c/span\u003ebin\u003cspan class=\"hljs-regexp\"\u003e/kubectl kubectl $KUBERNETES_DIR/\u003c/span\u003ekubectl \u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eRun @controlplane\u003c/p\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003eTODO: load balancer, hostnames\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003cp\u003eInitialize configuration such that the network is 10.10.0.0/16\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-apache\"\u003e\u003cspan class=\"hljs-attribute\"\u003esudo\u003c/span\u003e kubeadm init --pod-network-cidr \u003cspan class=\"hljs-number\"\u003e10.10.0.0\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e --apiserver-advertise-address {ip}\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eFor documentation, you should see something like\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-pgsql\"\u003eYour Kubernetes control-plane has initialized successfully!\n\n\u003cspan class=\"hljs-keyword\"\u003eTo\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003estart\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eusing\u003c/span\u003e your \u003cspan class=\"hljs-keyword\"\u003ecluster\u003c/span\u003e, you need \u003cspan class=\"hljs-keyword\"\u003eto\u003c/span\u003e run the \u003cspan class=\"hljs-keyword\"\u003efollowing\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e a regular \u003cspan class=\"hljs-keyword\"\u003euser\u003c/span\u003e:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/\u003cspan class=\"hljs-keyword\"\u003eadmin\u003c/span\u003e.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e you are the root \u003cspan class=\"hljs-keyword\"\u003euser\u003c/span\u003e, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/\u003cspan class=\"hljs-keyword\"\u003eadmin\u003c/span\u003e.conf\n\nYou should now deploy a pod network \u003cspan class=\"hljs-keyword\"\u003eto\u003c/span\u003e the \u003cspan class=\"hljs-keyword\"\u003ecluster\u003c/span\u003e.\nRun \"kubectl apply -f [podnetwork].yaml\" \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e one \u003cspan class=\"hljs-keyword\"\u003eof\u003c/span\u003e the \u003cspan class=\"hljs-keyword\"\u003eoptions\u003c/span\u003e listed at:\n  https://kubernetes.io/docs/concepts/\u003cspan class=\"hljs-keyword\"\u003ecluster\u003c/span\u003e-administration/addons/\n\n\u003cspan class=\"hljs-keyword\"\u003eThen\u003c/span\u003e you can \u003cspan class=\"hljs-keyword\"\u003ejoin\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eany\u003c/span\u003e number \u003cspan class=\"hljs-keyword\"\u003eof\u003c/span\u003e worker nodes \u003cspan class=\"hljs-keyword\"\u003eby\u003c/span\u003e running the \u003cspan class=\"hljs-keyword\"\u003efollowing\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eon\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eeach\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e root:\n\nkubeadm \u003cspan class=\"hljs-keyword\"\u003ejoin\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e192.168\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.121\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.210\u003c/span\u003e:\u003cspan class=\"hljs-number\"\u003e6443\u003c/span\u003e \u003cspan class=\"hljs-comment\"\u003e--token clns4a.b29f6anjipygy0e2 \\\u003c/span\u003e\n\t\u003cspan class=\"hljs-comment\"\u003e--discovery-token-ca-cert-hash sha256:833f599cc9ab27eb5010c499e9c77e8e3263fb991d8e9e78ef187ba97e1efb59\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eDo as it says, run\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-built_in\"\u003emkdir\u003c/span\u003e -p \u003cspan class=\"hljs-variable\"\u003e$HOME\u003c/span\u003e/.kube\nsudo \u003cspan class=\"hljs-built_in\"\u003ecp\u003c/span\u003e -i /etc/kubernetes/admin.conf \u003cspan class=\"hljs-variable\"\u003e$HOME\u003c/span\u003e/.kube/config\nsudo \u003cspan class=\"hljs-built_in\"\u003echown\u003c/span\u003e $(\u003cspan class=\"hljs-built_in\"\u003eid\u003c/span\u003e -u):$(\u003cspan class=\"hljs-built_in\"\u003eid\u003c/span\u003e -g) \u003cspan class=\"hljs-variable\"\u003e$HOME\u003c/span\u003e/.kube/config\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eWe will use \u003ca href=\"https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/weave-network-policy/\"\u003eWeave Net\u003c/a\u003e as a network plugin\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-powershell\"\u003ekubectl apply \u003cspan class=\"hljs-operator\"\u003e-f\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"https://cloud.weave.works/k8s/net?k8s-version=\u003cspan class=\"hljs-variable\"\u003e$\u003c/span\u003e(kubectl version | base64 | tr -d '\\n')\"\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"kubernetes/storage","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/kubernetes/storage.md","metadata":{"title":"Storage","description":null,"permalink":null,"priority":0,"tags":["Kubernetes"],"categories":["Kubernetes"]},"content":{"raw":"\nKubernetes abstracts the storage through a __plugin layer__. The plugin layer is the interface that connects external storage with Kubernetes. The layer can mediate between any kind of storage, be it file based, bloc of object based, on-prem or cloud.\n\nMost plugins are based on the __Container Storage Interface (CSI)__ which is an open standard.\n\n\u003e TBD\n","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003cp\u003eKubernetes abstracts the storage through a \u003cstrong\u003eplugin layer\u003c/strong\u003e. The plugin layer is the interface that connects external storage with Kubernetes. The layer can mediate between any kind of storage, be it file based, bloc of object based, on-prem or cloud.\u003c/p\u003e\n    \u003cp\u003eMost plugins are based on the \u003cstrong\u003eContainer Storage Interface (CSI)\u003c/strong\u003e which is an open standard.\u003c/p\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003eTBD\u003c/p\u003e\n    \u003c/blockquote\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"kubernetes/technical-overview","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/kubernetes/technical-overview.md","metadata":{"title":"Kubernetes technical overview","description":null,"permalink":null,"priority":0,"tags":["Kubernetes"],"categories":["Kubernetes"]},"content":{"raw":"\n## Application packaging\n\nAn application should be\n\n1. Packaged as a container\n1. Wrapped in a _Pod_\n1. Deployed via a declerative manifest file\n\n## The declerative model \n\nAccording to the _declerative model_, we only declare about how we want the application to look like. It is Kubernetes' jpb to make sure the cluster behaves as intended.\n\n_Manifests_ simple YAML files and they tell Kubernetes how the application should look like - the _desired state_.\n\n_Controllers_ are constantly running and monitor the application's state, reconciling and difference betweeen the _observerd state_ and the _desired state_.\n\n## Pods\n\nIn Kubernetes, _Pods_ are the atomic unit of scheduling. Kubernetes demands that every container runs inside a pod.\n\nA simple model is to run a sigle container in every pod. \n\nEffectively, a _Pod_ is a construct for running one or more containers.\n\nPods are the most basic unit of scaling. We scale applications by adding or removing pods (not containers).\n\nPods are deployed atomiclly. A pod is ready only when all containers are up and running. A single pod executes on a single Node.\n\nWhen a pod dies, a new one takes it's place. The new pod is a different instance with the same semantics, it has different id, ip etc..\n\nPods are immutable. If we want to change a pod's configuration, we must create a new pod to take it's place.\n\n### Pod theory\n\nThere are 3 main reasons for Pods to exist\n\n1. Pods augment containers 1. Pods assist in scheduling\n1. Pods enable resource sharing\n\nThe augmentation is done in the following ways\n\n- Labels / annotations\n- Restart policies\n- Probes (startup, readiness, liveness etc...)\n- Affinity / anti-affinity rules (Affinities are related to specifying how specific pods behave with other pods)\n- Termination control\n- Security policies\n- Resource requests and limits (min/max values on CPU, memory and I/O)\n\nPods have __Labels__ which lets us group Pods and associate them with other objects. \n\nRegarding resource sharing, Pods provide _shared execution environment_ for one or more containers. It includes\n\n- Filesystem\n- Network stack (IP address, routing, ports)\n- Memory\n- Volumes\n\nPods can be deployed either directly via a Pod manifest or indirectly via a controller. Pods deployed directly are called _static pods_.\n\n## Deployments\n\nA _Deployment_ is a higher-level controller. Usually we will deploy pods indirectly via a deployment.\n\nThe deployment controller monitors the state of a wrapped pod providing extra features such as self-healing, scaling, zero-downtime rollouts and versioned rollbacks.\n\n## Services\n\nA _Service_ is a Kubernetes contstruct which provides reliable networking for a set of pods.\n\nAs we know, pods are immutable and modifications due to auto scaling, rollbacks etc... result in replacements of pods - and the effective IPs.\n\n_Services_ provide reliable names and IPs and provide load balancing capabilities over a set of pods.\n\n## Examples of controllers\n\n- Deployments\n- DaemonSets\n- StatefulSets\n\n## Generall usefull commands\n\nList all possible Pod attributes\n\n    kubectl explain pods --recursive\n\n## Multi container patterns\n\nKubernetes offers several well-defined multi-container Pod patterns\n\n### Sidecar pattern\n\nThis pattern has a _main_ application container and a _sidecar_ container. The _sidecar's_ job is to augment and perform secondary tasks for the _main_ application container.\n\n### Adapter pattern\n\nThis pattern is a specific variation of the _sidecar pattern_ where the _sidecar_ container takes non-standardized output from the _main_ container and standardize it as required by an external system.\n\n### Ambassador pattern\n\nThis is another variation of the _sidecar pattern_ where the _sidecar_ brokers connectivity to an external system.\n\n### Init pattern\n\nThis pattern has an _init_ container that's gauranteed to start and complete before your _main_ application container. It is also gauranteed to run exactly once!\n","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch2\u003eApplication packaging\u003c/h2\u003e\n    \u003cp\u003eAn application should be\u003c/p\u003e\n    \u003col\u003e\n      \u003cli\u003ePackaged as a container\u003c/li\u003e\n      \u003cli\u003eWrapped in a \u003cem\u003ePod\u003c/em\u003e\u003c/li\u003e\n      \u003cli\u003eDeployed via a declerative manifest file\u003c/li\u003e\n    \u003c/ol\u003e\n    \u003ch2\u003eThe declerative model\u003c/h2\u003e\n    \u003cp\u003eAccording to the \u003cem\u003edeclerative model\u003c/em\u003e, we only declare about how we want the application to look like. It is Kubernetes' jpb to make sure the cluster behaves as intended.\u003c/p\u003e\n    \u003cp\u003e\u003cem\u003eManifests\u003c/em\u003e simple YAML files and they tell Kubernetes how the application should look like - the \u003cem\u003edesired state\u003c/em\u003e.\u003c/p\u003e\n    \u003cp\u003e\u003cem\u003eControllers\u003c/em\u003e are constantly running and monitor the application's state, reconciling and difference betweeen the \u003cem\u003eobserverd state\u003c/em\u003e and the \u003cem\u003edesired state\u003c/em\u003e.\u003c/p\u003e\n    \u003ch2\u003ePods\u003c/h2\u003e\n    \u003cp\u003eIn Kubernetes, \u003cem\u003ePods\u003c/em\u003e are the atomic unit of scheduling. Kubernetes demands that every container runs inside a pod.\u003c/p\u003e\n    \u003cp\u003eA simple model is to run a sigle container in every pod.\u003c/p\u003e\n    \u003cp\u003eEffectively, a \u003cem\u003ePod\u003c/em\u003e is a construct for running one or more containers.\u003c/p\u003e\n    \u003cp\u003ePods are the most basic unit of scaling. We scale applications by adding or removing pods (not containers).\u003c/p\u003e\n    \u003cp\u003ePods are deployed atomiclly. A pod is ready only when all containers are up and running. A single pod executes on a single Node.\u003c/p\u003e\n    \u003cp\u003eWhen a pod dies, a new one takes it's place. The new pod is a different instance with the same semantics, it has different id, ip etc..\u003c/p\u003e\n    \u003cp\u003ePods are immutable. If we want to change a pod's configuration, we must create a new pod to take it's place.\u003c/p\u003e\n    \u003ch3\u003ePod theory\u003c/h3\u003e\n    \u003cp\u003eThere are 3 main reasons for Pods to exist\u003c/p\u003e\n    \u003col\u003e\n      \u003cli\u003ePods augment containers 1. Pods assist in scheduling\u003c/li\u003e\n      \u003cli\u003ePods enable resource sharing\u003c/li\u003e\n    \u003c/ol\u003e\n    \u003cp\u003eThe augmentation is done in the following ways\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eLabels / annotations\u003c/li\u003e\n      \u003cli\u003eRestart policies\u003c/li\u003e\n      \u003cli\u003eProbes (startup, readiness, liveness etc...)\u003c/li\u003e\n      \u003cli\u003eAffinity / anti-affinity rules (Affinities are related to specifying how specific pods behave with other pods)\u003c/li\u003e\n      \u003cli\u003eTermination control\u003c/li\u003e\n      \u003cli\u003eSecurity policies\u003c/li\u003e\n      \u003cli\u003eResource requests and limits (min/max values on CPU, memory and I/O)\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003ePods have \u003cstrong\u003eLabels\u003c/strong\u003e which lets us group Pods and associate them with other objects.\u003c/p\u003e\n    \u003cp\u003eRegarding resource sharing, Pods provide \u003cem\u003eshared execution environment\u003c/em\u003e for one or more containers. It includes\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eFilesystem\u003c/li\u003e\n      \u003cli\u003eNetwork stack (IP address, routing, ports)\u003c/li\u003e\n      \u003cli\u003eMemory\u003c/li\u003e\n      \u003cli\u003eVolumes\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003ePods can be deployed either directly via a Pod manifest or indirectly via a controller. Pods deployed directly are called \u003cem\u003estatic pods\u003c/em\u003e.\u003c/p\u003e\n    \u003ch2\u003eDeployments\u003c/h2\u003e\n    \u003cp\u003eA \u003cem\u003eDeployment\u003c/em\u003e is a higher-level controller. Usually we will deploy pods indirectly via a deployment.\u003c/p\u003e\n    \u003cp\u003eThe deployment controller monitors the state of a wrapped pod providing extra features such as self-healing, scaling, zero-downtime rollouts and versioned rollbacks.\u003c/p\u003e\n    \u003ch2\u003eServices\u003c/h2\u003e\n    \u003cp\u003eA \u003cem\u003eService\u003c/em\u003e is a Kubernetes contstruct which provides reliable networking for a set of pods.\u003c/p\u003e\n    \u003cp\u003eAs we know, pods are immutable and modifications due to auto scaling, rollbacks etc... result in replacements of pods - and the effective IPs.\u003c/p\u003e\n    \u003cp\u003e\u003cem\u003eServices\u003c/em\u003e provide reliable names and IPs and provide load balancing capabilities over a set of pods.\u003c/p\u003e\n    \u003ch2\u003eExamples of controllers\u003c/h2\u003e\n    \u003cul\u003e\n      \u003cli\u003eDeployments\u003c/li\u003e\n      \u003cli\u003eDaemonSets\u003c/li\u003e\n      \u003cli\u003eStatefulSets\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch2\u003eGenerall usefull commands\u003c/h2\u003e\n    \u003cp\u003eList all possible Pod attributes\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-pgsql\"\u003ekubectl \u003cspan class=\"hljs-keyword\"\u003eexplain\u003c/span\u003e pods \u003cspan class=\"hljs-comment\"\u003e--recursive\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003ch2\u003eMulti container patterns\u003c/h2\u003e\n    \u003cp\u003eKubernetes offers several well-defined multi-container Pod patterns\u003c/p\u003e\n    \u003ch3\u003eSidecar pattern\u003c/h3\u003e\n    \u003cp\u003eThis pattern has a \u003cem\u003emain\u003c/em\u003e application container and a \u003cem\u003esidecar\u003c/em\u003e container. The \u003cem\u003esidecar's\u003c/em\u003e job is to augment and perform secondary tasks for the \u003cem\u003emain\u003c/em\u003e application container.\u003c/p\u003e\n    \u003ch3\u003eAdapter pattern\u003c/h3\u003e\n    \u003cp\u003eThis pattern is a specific variation of the \u003cem\u003esidecar pattern\u003c/em\u003e where the \u003cem\u003esidecar\u003c/em\u003e container takes non-standardized output from the \u003cem\u003emain\u003c/em\u003e container and standardize it as required by an external system.\u003c/p\u003e\n    \u003ch3\u003eAmbassador pattern\u003c/h3\u003e\n    \u003cp\u003eThis is another variation of the \u003cem\u003esidecar pattern\u003c/em\u003e where the \u003cem\u003esidecar\u003c/em\u003e brokers connectivity to an external system.\u003c/p\u003e\n    \u003ch3\u003eInit pattern\u003c/h3\u003e\n    \u003cp\u003eThis pattern has an \u003cem\u003einit\u003c/em\u003e container that's gauranteed to start and complete before your \u003cem\u003emain\u003c/em\u003e application container. It is also gauranteed to run exactly once!\u003c/p\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"markdown","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/markdown.md","metadata":{"title":"Markdown","description":null,"permalink":null,"priority":0,"tags":[],"categories":[]},"content":{"raw":"\n# Header 1\n## Header 2\n### Header 3\n#### Header 4\n##### Header 5\n###### Header 6\n\nContrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \"de Finibus Bonorum et Malorum\" (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, \"Lorem ipsum dolor sit amet..\", comes from a line in section 1.10.32.\n\nThe standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from \"de Finibus Bonorum et Malorum\" by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham.\n\n\u003e This is some quote\n\n__Some Math__\n\n$\\forall h \\in H$\n\n__Ordered list__\n\n1. First Item  \n1. Second Item  \n1. Third Item  \n\n__Unordered list__\n\n- First  \n- Second  \n- Third  \n\n\n| id | name              | phone            | description      |\n|----|-------------------|------------------|------------------|\n| 1  | Ricky Phelps      | +123 123 5555555 | Some description |\n| 2  | Asha Valdez       | +123 123 5555555 |                  |\n| 3  | Katelyn Dougherty | +123 123 5555555 |                  |\n\n```python\n# Some example python code\nclass BinaryOperation:\n  def __init__(self, operation):\n    self.operation = operation\n  \n  def operate(self, x, y):\n    return self.operation(x, y)\n\n  def __call__(self, x, y):\n    return self.operate(x, y)\n```","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch1\u003eHeader 1\u003c/h1\u003e\n    \u003ch2\u003eHeader 2\u003c/h2\u003e\n    \u003ch3\u003eHeader 3\u003c/h3\u003e\n    \u003ch4\u003eHeader 4\u003c/h4\u003e\n    \u003ch5\u003eHeader 5\u003c/h5\u003e\n    \u003ch6\u003eHeader 6\u003c/h6\u003e\n    \u003cp\u003eContrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \"de Finibus Bonorum et Malorum\" (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, \"Lorem ipsum dolor sit amet..\", comes from a line in section 1.10.32.\u003c/p\u003e\n    \u003cp\u003eThe standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from \"de Finibus Bonorum et Malorum\" by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham.\u003c/p\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003eThis is some quote\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003cp\u003e\u003cstrong\u003eSome Math\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003e$\\forall h \\in H$\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eOrdered list\u003c/strong\u003e\u003c/p\u003e\n    \u003col\u003e\n      \u003cli\u003eFirst Item\u003c/li\u003e\n      \u003cli\u003eSecond Item\u003c/li\u003e\n      \u003cli\u003eThird Item\u003c/li\u003e\n    \u003c/ol\u003e\n    \u003cp\u003e\u003cstrong\u003eUnordered list\u003c/strong\u003e\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eFirst\u003c/li\u003e\n      \u003cli\u003eSecond\u003c/li\u003e\n      \u003cli\u003eThird\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ctable\u003e\n      \u003cthead\u003e\n        \u003ctr\u003e\n          \u003cth\u003eid\u003c/th\u003e\n          \u003cth\u003ename\u003c/th\u003e\n          \u003cth\u003ephone\u003c/th\u003e\n          \u003cth\u003edescription\u003c/th\u003e\n        \u003c/tr\u003e\n      \u003c/thead\u003e\n      \u003ctbody\u003e\n        \u003ctr\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003eRicky Phelps\u003c/td\u003e\n          \u003ctd\u003e+123 123 5555555\u003c/td\u003e\n          \u003ctd\u003eSome description\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n          \u003ctd\u003e2\u003c/td\u003e\n          \u003ctd\u003eAsha Valdez\u003c/td\u003e\n          \u003ctd\u003e+123 123 5555555\u003c/td\u003e\n          \u003ctd\u003e\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n          \u003ctd\u003e3\u003c/td\u003e\n          \u003ctd\u003eKatelyn Dougherty\u003c/td\u003e\n          \u003ctd\u003e+123 123 5555555\u003c/td\u003e\n          \u003ctd\u003e\u003c/td\u003e\n        \u003c/tr\u003e\n      \u003c/tbody\u003e\n    \u003c/table\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-comment\"\u003e# Some example python code\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eBinaryOperation\u003c/span\u003e:\n  \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, operation\u003c/span\u003e):\n    self.operation = operation\n  \n  \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eoperate\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, x, y\u003c/span\u003e):\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self.operation(x, y)\n\n  \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__call__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, x, y\u003c/span\u003e):\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self.operate(x, y)\n\u003c/code\u003e\u003c/pre\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"posts/archlinux-installation-guide","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/posts/archlinux-installation-guide.md","metadata":{"title":"Archlinux Installation Guide","description":null,"permalink":null,"priority":0,"tags":["Archlinux","Linux"],"categories":["Guides"]},"content":{"raw":"\n# arch boot spec\n\nthis is specific to my machine and software of choice\n- efi boot\n- disk at sdb \n- netctl as network manager in boot environment\n- networkmanager as network manager in installation\n\ni edit this file in parallel to the installation and learning process; if you want to do this as well, install git from the boot environment (or use elinks to view the file through github)\n\n## connect to network\n\n```wifi-menu``` and follow instructions\n\ncheck connectivity\n\n```ping 8.8.8.8```\n\nin ```/etc/nsswitch.conf```, at the ```hosts``` entry, make sure ```dns``` is before ```[!UNAVAIL=return]```\n\ncheck dns acquisition\n\n```ping google.com```\n\n## partition, filesystems and mount\n\nbe careful, assume each step to erase all data on partition layout of the disk!\n\nmake sure on which device you want to work on, using ```lsblk```\n\nerase labels ```wipefs -a /dev/sdb```\n\nexample partition scheme below\n\n\u003e you can use ```fdisk``` or ```cfdisk``` for example\n\npartition | size  | type             | desc\n----------|-------|------------------|----------------\nsdb1      | 550MB | EFI System       | boot partition\nsdb2      | 24GB  | Linux swap       | swap partition\nsdb3      | 32GB  | Linux filesystem  | root partition\nsdb4      | rest  | Linux filesystem  | home partition\n\nmake filesystems for the partitions\n\n```bash\nmkfs.fat -F32 /dev/sdb1\n\nmkswap /dev/sdb2\nswapon /dev/sdb2\n\nmkfs.ext4 /dev/sdb3\nmkfs.ext4 /dev/sdb4\n```\n\nmount partitions to filesystem\n\n```bash\nmount /dev/sdb3 /mnt\n\nmkdir /mnt/boot\nmkdir /mnt/boot/efi\nmkdir /mnt/home\n\nmount /dev/sdb4 /mnt/home\nmount /dev/sdb1 /mnt/boot/efi\n```\n\n## archlinux installation\n\n```bash\npacstrap /mnt \\\n    base base-devel \\\n    linux linux-firmware \\\n    grub efibootmgr \\\n    networkmanager \\\n    vim git\n```\n\n## installation setup\n\ngenerate fstab - ```genfstab -U /mnt \u003e\u003e /mnt/etc/fstab```\n\nchange root to installation - ```arch-chroot /mnt```\n\nedit file ```/etc/locale.gen``` and uncomment desired locale\n\ngenerate locale - ```locale-gen```\n\nadd set language - ```echo \"LANG=en_US.UTF-8\" \u003e /etc/locale.conf```\n\nset timezone -\n\n```bash\nln -sf /usr/share/zoneinfo/Asia/Jerusalem /etc/localtime\nhwclock --systohc\n```\n\nset root password for linux installation - ```passwd```\n\n## grub\n\n```bash\ngrub-install --target=x86_64-efi --efi-directory=/boot/efi\ngrub-mkconfig -o /boot/grub/grub.cfg\n```\n\n## finalize\n\nback to bootable environment\n\n```exit```\n\nunmount all partitions\n\n```umount -R /mnt```\n\n```reboot```\n","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch1\u003earch boot spec\u003c/h1\u003e\n    \u003cp\u003ethis is specific to my machine and software of choice\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eefi boot\u003c/li\u003e\n      \u003cli\u003edisk at sdb\u003c/li\u003e\n      \u003cli\u003enetctl as network manager in boot environment\u003c/li\u003e\n      \u003cli\u003enetworkmanager as network manager in installation\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003ei edit this file in parallel to the installation and learning process; if you want to do this as well, install git from the boot environment (or use elinks to view the file through github)\u003c/p\u003e\n    \u003ch2\u003econnect to network\u003c/h2\u003e\n    \u003cp\u003e\u003ccode\u003ewifi-menu\u003c/code\u003e and follow instructions\u003c/p\u003e\n    \u003cp\u003echeck connectivity\u003c/p\u003e\n    \u003cp\u003e\u003ccode\u003eping 8.8.8.8\u003c/code\u003e\u003c/p\u003e\n    \u003cp\u003ein \u003ccode\u003e/etc/nsswitch.conf\u003c/code\u003e, at the \u003ccode\u003ehosts\u003c/code\u003e entry, make sure \u003ccode\u003edns\u003c/code\u003e is before \u003ccode\u003e[!UNAVAIL=return]\u003c/code\u003e\u003c/p\u003e\n    \u003cp\u003echeck dns acquisition\u003c/p\u003e\n    \u003cp\u003e\u003ccode\u003eping google.com\u003c/code\u003e\u003c/p\u003e\n    \u003ch2\u003epartition, filesystems and mount\u003c/h2\u003e\n    \u003cp\u003ebe careful, assume each step to erase all data on partition layout of the disk!\u003c/p\u003e\n    \u003cp\u003emake sure on which device you want to work on, using \u003ccode\u003elsblk\u003c/code\u003e\u003c/p\u003e\n    \u003cp\u003eerase labels \u003ccode\u003ewipefs -a /dev/sdb\u003c/code\u003e\u003c/p\u003e\n    \u003cp\u003eexample partition scheme below\u003c/p\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003eyou can use \u003ccode\u003efdisk\u003c/code\u003e or \u003ccode\u003ecfdisk\u003c/code\u003e for example\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003ctable\u003e\n      \u003cthead\u003e\n        \u003ctr\u003e\n          \u003cth\u003epartition\u003c/th\u003e\n          \u003cth\u003esize\u003c/th\u003e\n          \u003cth\u003etype\u003c/th\u003e\n          \u003cth\u003edesc\u003c/th\u003e\n        \u003c/tr\u003e\n      \u003c/thead\u003e\n      \u003ctbody\u003e\n        \u003ctr\u003e\n          \u003ctd\u003esdb1\u003c/td\u003e\n          \u003ctd\u003e550MB\u003c/td\u003e\n          \u003ctd\u003eEFI System\u003c/td\u003e\n          \u003ctd\u003eboot partition\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n          \u003ctd\u003esdb2\u003c/td\u003e\n          \u003ctd\u003e24GB\u003c/td\u003e\n          \u003ctd\u003eLinux swap\u003c/td\u003e\n          \u003ctd\u003eswap partition\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n          \u003ctd\u003esdb3\u003c/td\u003e\n          \u003ctd\u003e32GB\u003c/td\u003e\n          \u003ctd\u003eLinux filesystem\u003c/td\u003e\n          \u003ctd\u003eroot partition\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n          \u003ctd\u003esdb4\u003c/td\u003e\n          \u003ctd\u003erest\u003c/td\u003e\n          \u003ctd\u003eLinux filesystem\u003c/td\u003e\n          \u003ctd\u003ehome partition\u003c/td\u003e\n        \u003c/tr\u003e\n      \u003c/tbody\u003e\n    \u003c/table\u003e\n    \u003cp\u003emake filesystems for the partitions\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003emkfs.fat -F32 /dev/sdb1\n\nmkswap /dev/sdb2\nswapon /dev/sdb2\n\nmkfs.ext4 /dev/sdb3\nmkfs.ext4 /dev/sdb4\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003emount partitions to filesystem\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003emount /dev/sdb3 /mnt\n\n\u003cspan class=\"hljs-built_in\"\u003emkdir\u003c/span\u003e /mnt/boot\n\u003cspan class=\"hljs-built_in\"\u003emkdir\u003c/span\u003e /mnt/boot/efi\n\u003cspan class=\"hljs-built_in\"\u003emkdir\u003c/span\u003e /mnt/home\n\nmount /dev/sdb4 /mnt/home\nmount /dev/sdb1 /mnt/boot/efi\n\u003c/code\u003e\u003c/pre\u003e\n    \u003ch2\u003earchlinux installation\u003c/h2\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003epacstrap /mnt \\\n    base base-devel \\\n    linux linux-firmware \\\n    grub efibootmgr \\\n    networkmanager \\\n    vim git\n\u003c/code\u003e\u003c/pre\u003e\n    \u003ch2\u003einstallation setup\u003c/h2\u003e\n    \u003cp\u003egenerate fstab - \u003ccode\u003egenfstab -U /mnt \u003e\u003e /mnt/etc/fstab\u003c/code\u003e\u003c/p\u003e\n    \u003cp\u003echange root to installation - \u003ccode\u003earch-chroot /mnt\u003c/code\u003e\u003c/p\u003e\n    \u003cp\u003eedit file \u003ccode\u003e/etc/locale.gen\u003c/code\u003e and uncomment desired locale\u003c/p\u003e\n    \u003cp\u003egenerate locale - \u003ccode\u003elocale-gen\u003c/code\u003e\u003c/p\u003e\n    \u003cp\u003eadd set language - \u003ccode\u003eecho \"LANG=en_US.UTF-8\" \u003e /etc/locale.conf\u003c/code\u003e\u003c/p\u003e\n    \u003cp\u003eset timezone -\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-built_in\"\u003eln\u003c/span\u003e -sf /usr/share/zoneinfo/Asia/Jerusalem /etc/localtime\nhwclock --systohc\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eset root password for linux installation - \u003ccode\u003epasswd\u003c/code\u003e\u003c/p\u003e\n    \u003ch2\u003egrub\u003c/h2\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003egrub-install --target=x86_64-efi --efi-directory=/boot/efi\ngrub-mkconfig -o /boot/grub/grub.cfg\n\u003c/code\u003e\u003c/pre\u003e\n    \u003ch2\u003efinalize\u003c/h2\u003e\n    \u003cp\u003eback to bootable environment\u003c/p\u003e\n    \u003cp\u003e\u003ccode\u003eexit\u003c/code\u003e\u003c/p\u003e\n    \u003cp\u003eunmount all partitions\u003c/p\u003e\n    \u003cp\u003e\u003ccode\u003eumount -R /mnt\u003c/code\u003e\u003c/p\u003e\n    \u003cp\u003e\u003ccode\u003ereboot\u003c/code\u003e\u003c/p\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"posts/data-engineering-demystified-summary","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/posts/data-engineering-demystified-summary.md","metadata":{"title":"Data Engineering Demistified Summary","description":null,"permalink":null,"priority":0,"tags":[],"categories":[]},"content":{"raw":"\n# Demistified summary\n\nSee http://big-data-demystified.ninja\n\nSpecifically\n- http://big-data-demystified.ninja/2020/07/09/data-lake-architecture-best-practices\n\n## Using Version Control Systems\n\nEnvironment suggestions\n\n- Dev. Has Read only access of Production data.\n- Pre Production. Because some thing can only be tested against production.\n- Production.\n\n\n## Airflow Coding Guidlines\n\n**Keep it simple**. Avoid object oriented abstraction. Very hard to retrospectively understand DAGs, simplicity is king here.\n\n**Avoid using Sensors - Airflow Term?**. Unpredictabilty in production.\n\nIt is prefereable if Jobs are\n\n- Recurrentable - Running the same job again won't change nothing\n- Debugable - can debug the job easily\n- Write after Delete - When deleting data, insert it right after, don't do that in another stage \n**Monitoring is very important!!!**\n\nPros\n- flexibility\n- customizability\n- scale\n- cost\n\nCons\n- learning curve\n- diy\n- time to market\n- open source\n- unclear errors\n\n## Data lake\n\n1. Data quality - MonteCarlo, Great Expectations\n1. Data pipeline stability - Databand and Honey Comb\n1. Data lineage - Apache atlas, amundsen\n1. Data classification\n\n## Cleansing and Preparing data\n\nApproaches\n\n**Python parser**\n\nPros - Simple\nCons - non uniform, hard to maintain\n\n**Dataframe / Pandas**\n\nPros - Simple, Generic, Flexible\nCons - Not scalable, bounded to the RAM\n\n**ELT**\n\nPros - Simple, Generic, Peta-Scale \nCons - Requires Good SQL / Big Data understanding\n\n**Pyspark / Scala**\n\nCons\n- Hard to learn\n- Trivial for simple cases / overkill\n\nPros\n- Good for schema evolution\n\n## 3rd party - APIs, Tips and Tricks\n\n\n## 4 V's\n\n- Volume\n- Veracity\n- Velocity\n- Variety\n\n## Triangle\n\nWhat is the criteria?\n\n```\n       Faster\n\n    /          \\\n\nCheaper  -   Simpler\n```\n","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch1\u003eDemistified summary\u003c/h1\u003e\n    \u003cp\u003eSee \u003ca href=\"http://big-data-demystified.ninja\"\u003ehttp://big-data-demystified.ninja\u003c/a\u003e\u003c/p\u003e\n    \u003cp\u003eSpecifically\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"http://big-data-demystified.ninja/2020/07/09/data-lake-architecture-best-practices\"\u003ehttp://big-data-demystified.ninja/2020/07/09/data-lake-architecture-best-practices\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch2\u003eUsing Version Control Systems\u003c/h2\u003e\n    \u003cp\u003eEnvironment suggestions\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eDev. Has Read only access of Production data.\u003c/li\u003e\n      \u003cli\u003ePre Production. Because some thing can only be tested against production.\u003c/li\u003e\n      \u003cli\u003eProduction.\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch2\u003eAirflow Coding Guidlines\u003c/h2\u003e\n    \u003cp\u003e\u003cstrong\u003eKeep it simple\u003c/strong\u003e. Avoid object oriented abstraction. Very hard to retrospectively understand DAGs, simplicity is king here.\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eAvoid using Sensors - Airflow Term?\u003c/strong\u003e. Unpredictabilty in production.\u003c/p\u003e\n    \u003cp\u003eIt is prefereable if Jobs are\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eRecurrentable - Running the same job again won't change nothing\u003c/li\u003e\n      \u003cli\u003eDebugable - can debug the job easily\u003c/li\u003e\n      \u003cli\u003e\n        Write after Delete - When deleting data, insert it right after, don't do that in another stage\n        \u003cstrong\u003eMonitoring is very important!!!\u003c/strong\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003ePros\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eflexibility\u003c/li\u003e\n      \u003cli\u003ecustomizability\u003c/li\u003e\n      \u003cli\u003escale\u003c/li\u003e\n      \u003cli\u003ecost\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eCons\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003elearning curve\u003c/li\u003e\n      \u003cli\u003ediy\u003c/li\u003e\n      \u003cli\u003etime to market\u003c/li\u003e\n      \u003cli\u003eopen source\u003c/li\u003e\n      \u003cli\u003eunclear errors\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch2\u003eData lake\u003c/h2\u003e\n    \u003col\u003e\n      \u003cli\u003eData quality - MonteCarlo, Great Expectations\u003c/li\u003e\n      \u003cli\u003eData pipeline stability - Databand and Honey Comb\u003c/li\u003e\n      \u003cli\u003eData lineage - Apache atlas, amundsen\u003c/li\u003e\n      \u003cli\u003eData classification\u003c/li\u003e\n    \u003c/ol\u003e\n    \u003ch2\u003eCleansing and Preparing data\u003c/h2\u003e\n    \u003cp\u003eApproaches\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003ePython parser\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003e\n      Pros - Simple\n      Cons - non uniform, hard to maintain\n    \u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eDataframe / Pandas\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003e\n      Pros - Simple, Generic, Flexible\n      Cons - Not scalable, bounded to the RAM\n    \u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eELT\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003e\n      Pros - Simple, Generic, Peta-Scale\n      Cons - Requires Good SQL / Big Data understanding\n    \u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003ePyspark / Scala\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003eCons\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eHard to learn\u003c/li\u003e\n      \u003cli\u003eTrivial for simple cases / overkill\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003ePros\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eGood for schema evolution\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch2\u003e3rd party - APIs, Tips and Tricks\u003c/h2\u003e\n    \u003ch2\u003e4 V's\u003c/h2\u003e\n    \u003cul\u003e\n      \u003cli\u003eVolume\u003c/li\u003e\n      \u003cli\u003eVeracity\u003c/li\u003e\n      \u003cli\u003eVelocity\u003c/li\u003e\n      \u003cli\u003eVariety\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch2\u003eTriangle\u003c/h2\u003e\n    \u003cp\u003eWhat is the criteria?\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-properties\"\u003e       \u003cspan class=\"hljs-attr\"\u003eFaster\u003c/span\u003e\n\n    \u003cspan class=\"hljs-attr\"\u003e/\u003c/span\u003e          \u003cspan class=\"hljs-string\"\u003e\\\n\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003eCheaper\u003c/span\u003e  \u003cspan class=\"hljs-string\"\u003e-   Simpler\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"posts/elasticsearch","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/posts/elasticsearch.md","metadata":{"title":"Elasticsearch","description":null,"permalink":null,"priority":0,"tags":[],"categories":[]},"content":{"raw":"\nConcepts and examples to get reminded of once in a whilei\n\n## What is Elasticsearch \n\n_Elasticsearch_ is a distributed search and analytics engine, providing a _near real-time_ search for a large array of data types, structured or un-structured.\n\nSome of the usecases\n- Backend for search boxes\n- Metrics and Log analysis\n\nElasticsearch uses _Apache Lucene_ under the hood as it's underlying search engine.\n\n## Documents \u0026 Indices\n\nTechnically, and I quote - \"Elasticsearch is a distributed documented store\". It serializes any kind of structure into JSON documents. In an Elasticsearch cluster, the documents are distributed accross multiple nodes and are accessible from any (really?) of the nodes.\n\n__TODO__ Are the documents distributed accross all nodes, or a part of the cluster? It actually seems unreasonable to me that it is a full replication.\n\nElasticsearch maintains an _Inverted Index_ to provide fast search capabilities on all of a document's fields (They say that a document is fully searchable within 1 sec). Practically, the index is the data structure the collects multiple documents and provides fast access to them according to search criteria.\n\nWe can think of the model as such\n```\nField\n    key: String\n    value: Any\n\nDocument\n    fields: Field[]\n\nIndex\n    documents: Document[]\n```\n\nBy default, Elasticsearch index all fields in all documents, and it has different mehcanisms to index different data types.\n\n## Search \u0026 Aggregations\n\nElasticsearch provides a REST API that supports\n- Structured queries -  Queries that are structurely similiar to SQL queries\n- Full text queries - Queries that return all documents that match the query, sorted by relevance\n- Complex queries - A combination of the above\n\nElasticsearch can maintain aggregations of the data enabling us to analyze summaries providing insights regarding metrics, patterns and trends. For example, we can query for\n- Total numbers that match X\n- The average that match Y\n\nFurthermore, aggregations can operate alongside the search requests, meaning that we can gain aggregative information for all search results.\n\n### cat API\n\nThe ```_cat``` endpoint provides \"Compact and Aligned Text\" meaning, a general, __consumed by humans__ information.\n\nGet the list of cat APIs\n\n    /_cat\n\nGet information about indices using [cat indices API](https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-indices.html)\n\n    /_cat/indices\n    /_cat/indices/_all\n    /_cat/indices?format=json\n    /_cat/indices/my-index\n    /_cat/indices/my-index?format=json\n\n### search API\n\nThe ```\u003ctarget\u003e/_search``` endpoint provides searching functionality.\n\nThe ```\u003ctarget\u003e``` parameter is optional and is a comma seperated list indicating data streams, indices and aliases to search. It supports wildcards (*).\n\nThere are many query parameters that affects how the search is being performed and how the results are being returned. We won't list them all here but here is a small list that give a vibe on the kind of possible parameters\n\n- q(string) - The query. Note that it can alternetively be provided through the body\n- explain(boolean) - If true, returns a defailed information about the score computation  \n- timeout(time units) - Sets the timeout for the request  \n- from(integer) - Starting document offset  \n- size(integer) - Number of hits to return  \n\nThe body also contains important information the important one is the \"query\" field.\n\n### shards\n\nShow shards\n\n    /_cat/shards\n    /_cat/shards?h=index,shard\n","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003cp\u003eConcepts and examples to get reminded of once in a whilei\u003c/p\u003e\n    \u003ch2\u003eWhat is Elasticsearch\u003c/h2\u003e\n    \u003cp\u003e\u003cem\u003eElasticsearch\u003c/em\u003e is a distributed search and analytics engine, providing a \u003cem\u003enear real-time\u003c/em\u003e search for a large array of data types, structured or un-structured.\u003c/p\u003e\n    \u003cp\u003eSome of the usecases\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eBackend for search boxes\u003c/li\u003e\n      \u003cli\u003eMetrics and Log analysis\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eElasticsearch uses \u003cem\u003eApache Lucene\u003c/em\u003e under the hood as it's underlying search engine.\u003c/p\u003e\n    \u003ch2\u003eDocuments \u0026#x26; Indices\u003c/h2\u003e\n    \u003cp\u003eTechnically, and I quote - \"Elasticsearch is a distributed documented store\". It serializes any kind of structure into JSON documents. In an Elasticsearch cluster, the documents are distributed accross multiple nodes and are accessible from any (really?) of the nodes.\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eTODO\u003c/strong\u003e Are the documents distributed accross all nodes, or a part of the cluster? It actually seems unreasonable to me that it is a full replication.\u003c/p\u003e\n    \u003cp\u003eElasticsearch maintains an \u003cem\u003eInverted Index\u003c/em\u003e to provide fast search capabilities on all of a document's fields (They say that a document is fully searchable within 1 sec). Practically, the index is the data structure the collects multiple documents and provides fast access to them according to search criteria.\u003c/p\u003e\n    \u003cp\u003eWe can think of the model as such\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-properties\"\u003e\u003cspan class=\"hljs-attr\"\u003eField\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003ekey\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003eString\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003evalue\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003eAny\u003c/span\u003e\n\n\u003cspan class=\"hljs-attr\"\u003eDocument\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003efields\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003eField[]\u003c/span\u003e\n\n\u003cspan class=\"hljs-attr\"\u003eIndex\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003edocuments\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003eDocument[]\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eBy default, Elasticsearch index all fields in all documents, and it has different mehcanisms to index different data types.\u003c/p\u003e\n    \u003ch2\u003eSearch \u0026#x26; Aggregations\u003c/h2\u003e\n    \u003cp\u003eElasticsearch provides a REST API that supports\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eStructured queries - Queries that are structurely similiar to SQL queries\u003c/li\u003e\n      \u003cli\u003eFull text queries - Queries that return all documents that match the query, sorted by relevance\u003c/li\u003e\n      \u003cli\u003eComplex queries - A combination of the above\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eElasticsearch can maintain aggregations of the data enabling us to analyze summaries providing insights regarding metrics, patterns and trends. For example, we can query for\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eTotal numbers that match X\u003c/li\u003e\n      \u003cli\u003eThe average that match Y\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eFurthermore, aggregations can operate alongside the search requests, meaning that we can gain aggregative information for all search results.\u003c/p\u003e\n    \u003ch3\u003ecat API\u003c/h3\u003e\n    \u003cp\u003eThe \u003ccode\u003e_cat\u003c/code\u003e endpoint provides \"Compact and Aligned Text\" meaning, a general, \u003cstrong\u003econsumed by humans\u003c/strong\u003e information.\u003c/p\u003e\n    \u003cp\u003eGet the list of cat APIs\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e/_cat\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eGet information about indices using \u003ca href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-indices.html\"\u003ecat indices API\u003c/a\u003e\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-awk\"\u003e\u003cspan class=\"hljs-regexp\"\u003e/_cat/i\u003c/span\u003endices\n\u003cspan class=\"hljs-regexp\"\u003e/_cat/i\u003c/span\u003endices/_all\n\u003cspan class=\"hljs-regexp\"\u003e/_cat/i\u003c/span\u003endices?format=json\n\u003cspan class=\"hljs-regexp\"\u003e/_cat/i\u003c/span\u003endices/my-index\n\u003cspan class=\"hljs-regexp\"\u003e/_cat/i\u003c/span\u003endices/my-index?format=json\n\u003c/code\u003e\u003c/pre\u003e\n    \u003ch3\u003esearch API\u003c/h3\u003e\n    \u003cp\u003eThe \u003ccode\u003e\u0026#x3C;target\u003e/_search\u003c/code\u003e endpoint provides searching functionality.\u003c/p\u003e\n    \u003cp\u003eThe \u003ccode\u003e\u0026#x3C;target\u003e\u003c/code\u003e parameter is optional and is a comma seperated list indicating data streams, indices and aliases to search. It supports wildcards (*).\u003c/p\u003e\n    \u003cp\u003eThere are many query parameters that affects how the search is being performed and how the results are being returned. We won't list them all here but here is a small list that give a vibe on the kind of possible parameters\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eq(string) - The query. Note that it can alternetively be provided through the body\u003c/li\u003e\n      \u003cli\u003eexplain(boolean) - If true, returns a defailed information about the score computation\u003c/li\u003e\n      \u003cli\u003etimeout(time units) - Sets the timeout for the request\u003c/li\u003e\n      \u003cli\u003efrom(integer) - Starting document offset\u003c/li\u003e\n      \u003cli\u003esize(integer) - Number of hits to return\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eThe body also contains important information the important one is the \"query\" field.\u003c/p\u003e\n    \u003ch3\u003eshards\u003c/h3\u003e\n    \u003cp\u003eShow shards\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-awk\"\u003e\u003cspan class=\"hljs-regexp\"\u003e/_cat/\u003c/span\u003eshards\n\u003cspan class=\"hljs-regexp\"\u003e/_cat/\u003c/span\u003eshards?h=index,shard\n\u003c/code\u003e\u003c/pre\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"posts/emacs-cheetsheet","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/posts/emacs-cheetsheet.md","metadata":{"title":"Emacs Cheatsheet","description":null,"permalink":null,"priority":0,"tags":["Emacs"],"categories":["Cheatsheets"]},"content":{"raw":"\n# Overview \n\nSome tasks or workflows are very mature through emacs and less on other enviornments, for example using clojure repl, editing latex files, scheme work etc...\n\nCreated this file because I use vim in my day to I keep forgetting basic emacs stuff\n\n# emacs\n\n#### open emacs in the terminal emulator\n\n    emacs -nw\n\n#### configuration path\n\n    ~/emacs.d/\n\n# help\n\nIt's important to know how to get help and use the documentations within emacs\n\nEnter tutorial\n\n    C-h t\n\nEnter package documentation\n\n    C-h i d m {package}\n\n# basic keys\n\n## core\n\n    C-x C-x        exit\n    C-g            abort command\n\n## navigation\n\n    C-l     Center text around the cursor\n    \n    C-v     Scroll to next screenful\n    M-v     Scroll to previous screenful\n\n    M-f     Move forward a word\n    M-b     Move backward a word\n    \n    C-n     Move to next line\n    C-p     Move to previous line\n    \n    C-a     Move to beginning of line\n    C-e     Move to end of line\n    \n    M-a     Move back to beginning of sentence\n    M-e     Move forward to end of sentence\n\n    M-\u003c     Move to beginning of the document\n    M-\u003e     Move to end of the document\n\n    C-s     Initiate search mode\n\n## windowing\n\n    C-x 1   Delete all windows except focused\n    C-x 2   Split current window horizontally\n    C-x 3   Split current window verticall\n    C-x o   Move to other window\n\n## editing\n\n    C-k     Delete from cursor to end of line\n    M-k     Delete from cursor to end of sentence\n    C-_     Undo\n    C-w     Cut selected region\n    M-w     Copy selected region\n    C-y     Paste\n\n## files and buffers\n\n    C-s         Save current file\n    C-x k       Kill buffer\n    C-x b       Switch to buffer\n    C-x C-b     Lits buffers","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch1\u003eOverview\u003c/h1\u003e\n    \u003cp\u003eSome tasks or workflows are very mature through emacs and less on other enviornments, for example using clojure repl, editing latex files, scheme work etc...\u003c/p\u003e\n    \u003cp\u003eCreated this file because I use vim in my day to I keep forgetting basic emacs stuff\u003c/p\u003e\n    \u003ch1\u003eemacs\u003c/h1\u003e\n    \u003ch4\u003eopen emacs in the terminal emulator\u003c/h4\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-properties\"\u003e\u003cspan class=\"hljs-attr\"\u003eemacs\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e-nw\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003ch4\u003econfiguration path\u003c/h4\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-arcade\"\u003e~\u003cspan class=\"hljs-regexp\"\u003e/emacs.d/\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003ch1\u003ehelp\u003c/h1\u003e\n    \u003cp\u003eIt's important to know how to get help and use the documentations within emacs\u003c/p\u003e\n    \u003cp\u003eEnter tutorial\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-properties\"\u003e\u003cspan class=\"hljs-attr\"\u003eC-h\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003et\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eEnter package documentation\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-properties\"\u003e\u003cspan class=\"hljs-attr\"\u003eC-h\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003ei d m {package}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003ch1\u003ebasic keys\u003c/h1\u003e\n    \u003ch2\u003ecore\u003c/h2\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-awk\"\u003eC-x C-x        \u003cspan class=\"hljs-keyword\"\u003eexit\u003c/span\u003e\nC-g            abort command\n\u003c/code\u003e\u003c/pre\u003e\n    \u003ch2\u003enavigation\u003c/h2\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-properties\"\u003e\u003cspan class=\"hljs-attr\"\u003eC-l\u003c/span\u003e     \u003cspan class=\"hljs-string\"\u003eCenter text around the cursor\u003c/span\u003e\n\n\u003cspan class=\"hljs-attr\"\u003eC-v\u003c/span\u003e     \u003cspan class=\"hljs-string\"\u003eScroll to next screenful\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003eM-v\u003c/span\u003e     \u003cspan class=\"hljs-string\"\u003eScroll to previous screenful\u003c/span\u003e\n\n\u003cspan class=\"hljs-attr\"\u003eM-f\u003c/span\u003e     \u003cspan class=\"hljs-string\"\u003eMove forward a word\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003eM-b\u003c/span\u003e     \u003cspan class=\"hljs-string\"\u003eMove backward a word\u003c/span\u003e\n\n\u003cspan class=\"hljs-attr\"\u003eC-n\u003c/span\u003e     \u003cspan class=\"hljs-string\"\u003eMove to next line\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003eC-p\u003c/span\u003e     \u003cspan class=\"hljs-string\"\u003eMove to previous line\u003c/span\u003e\n\n\u003cspan class=\"hljs-attr\"\u003eC-a\u003c/span\u003e     \u003cspan class=\"hljs-string\"\u003eMove to beginning of line\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003eC-e\u003c/span\u003e     \u003cspan class=\"hljs-string\"\u003eMove to end of line\u003c/span\u003e\n\n\u003cspan class=\"hljs-attr\"\u003eM-a\u003c/span\u003e     \u003cspan class=\"hljs-string\"\u003eMove back to beginning of sentence\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003eM-e\u003c/span\u003e     \u003cspan class=\"hljs-string\"\u003eMove forward to end of sentence\u003c/span\u003e\n\n\u003cspan class=\"hljs-attr\"\u003eM-\u0026#x3C;\u003c/span\u003e     \u003cspan class=\"hljs-string\"\u003eMove to beginning of the document\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003eM-\u003e\u003c/span\u003e     \u003cspan class=\"hljs-string\"\u003eMove to end of the document\u003c/span\u003e\n\n\u003cspan class=\"hljs-attr\"\u003eC-s\u003c/span\u003e     \u003cspan class=\"hljs-string\"\u003eInitiate search mode\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003ch2\u003ewindowing\u003c/h2\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-pgsql\"\u003eC-x \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e   \u003cspan class=\"hljs-keyword\"\u003eDelete\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eall\u003c/span\u003e windows \u003cspan class=\"hljs-keyword\"\u003eexcept\u003c/span\u003e focused\nC-x \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e   Split \u003cspan class=\"hljs-keyword\"\u003ecurrent\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003ewindow\u003c/span\u003e horizontally\nC-x \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e   Split \u003cspan class=\"hljs-keyword\"\u003ecurrent\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003ewindow\u003c/span\u003e verticall\nC-x o   \u003cspan class=\"hljs-keyword\"\u003eMove\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eto\u003c/span\u003e other \u003cspan class=\"hljs-keyword\"\u003ewindow\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003ch2\u003eediting\u003c/h2\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-pgsql\"\u003eC-k     \u003cspan class=\"hljs-keyword\"\u003eDelete\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003ecursor\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eto\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eend\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eof\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003eline\u003c/span\u003e\nM-k     \u003cspan class=\"hljs-keyword\"\u003eDelete\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003ecursor\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eto\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eend\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eof\u003c/span\u003e sentence\nC-_     Undo\nC-w     Cut selected region\nM-w     \u003cspan class=\"hljs-keyword\"\u003eCopy\u003c/span\u003e selected region\nC-y     Paste\n\u003c/code\u003e\u003c/pre\u003e\n    \u003ch2\u003efiles and buffers\u003c/h2\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-properties\"\u003e\u003cspan class=\"hljs-attr\"\u003eC-s\u003c/span\u003e         \u003cspan class=\"hljs-string\"\u003eSave current file\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003eC-x\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003ek       Kill buffer\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003eC-x\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eb       Switch to buffer\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003eC-x\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eC-b     Lits buffers\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"posts/kafka","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/posts/kafka.md","metadata":{"title":"Kafka","description":null,"permalink":null,"priority":0,"tags":[],"categories":[]},"content":{"raw":"\n# Overview\n\n## What is kafka?\n\n\u003e Kafka is a streamsing platform for ingesting, storing, accessing and processing streams of data\n\n# High level concepts\n\n## Communication model\n\nOn the contrary of a **Directed Communication** where every service is aware of the other process to send messages to, kafka provides a centralized hub in which processes can send and receieve messages without being aware of each other. Such communication model is known as the **Publish/Subscribe** model which enhance the decoupling between different processes.\n\n\u003e Of course those are not termed by kafka\n\n\u003e Recall the different communication formalization according the sync/async and persistent/transient dimensions. Kafka fills conforms to the asynchronous and persisted model. This is unintuitive since this model is usually used for classical **Queues** rather than **Pubsubs**.\n\n## Core components\n\nA **Topic** is a named stream of data (/ channel - CSP?).\n\n**Producer**s are processes that publish data to a Topic.\n\n**Consumer**s are processes that subscribe to data in one or more Topics.\n\nA **Consumer Group** is a set of Consumers that work together as a group.\n\n## Storage\n\n### Commit Log\n\nA **Commit Log** is an append only data structure which contain an *Ordered Sequence* of events (/records)\n\n- Records in the log are immutable\n- Records are ordered and their ordinal is known as their **Offset**\n\n\u003e It is very similiar to existing transaction logs (Redo Log especially) which can be found in classical Relational Databases.\n\n\u003e To gain perspectives we can find similiarities to STM concurrency mode, Redux stores etc...\n\n### Partitions\n\nIn order to provide distribution capabilities, there is no 1 to 1 correlation of Topic to Log since then each Topic would need to be stored on a single machine. Rather, a Topic is broken into smaller units called **Partitions**.\n\nEffectively, every Partition is a single Log. Such model allows kafka to distribute a Topic with N partitions to at most N workers and we can have K Consumers under the same Consumer Group sharing the load of consumption as long as LEQ(K, N). If K\u003eN than it means that some consumers will be idle.\n\nIt makes sense from a technical POV. Kafka simply assigns a Partition to a Consumer.\n\n- If the number of Consumers is less than the number of Partitions, by the pigenhole principle there is at least a single Consumers that is assigned with more than one partition\n- If the number of Consumers is greather than the number of Partitions, there cannot be a surjective mapping from the Partitions the Consumers\n\n### Event\n\nA unit of data in a Topic has many interchangeable names - Event, Message, Record. We favour the term Event.\n\n\u003e Out of the kafka scope, events in the fotware world are *Entities* that desribe something that *happened in the past*. Therefore, event are past tense verbse. Well, it is the same in kafka\n\nAn **Event** is a timestamped key-value pair the records something that happened.\n\nEvent is composed of **Headers**, **Keys**, a **Timestamp** and a **Value**. We will cover some of those at a later time but for a very brief overview\n\n**Headers** contain optional metadata about the Event. One can use them to annotate, monitor and audit and Event. This is very like HTTP headers.\n\n**Keys** are optional values that affect how data is distributed accross partitions (Again, this is known concept, think of Partition Keys and partitioning schemes in different distributed systems). We will explore this concept independently.\n\n**Timestamp** holds information about when the thing that happened hapenned. There are multiple semantics of time we can speak of - When the Event created? When was it ingested? When was it processed? This is a whole topic of it's own which we will explore independantly.\n\n**Value** is the content of the message. Practically this is just a byte array and should be serialized according to the application.\n\n## The Cluster\n\nTODO: cover concepts below\n\n**Broker** TODO\n\n**Replication** TODO\n\n**Leader** TODO\n\n**Follower** TODO\n","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch1\u003eOverview\u003c/h1\u003e\n    \u003ch2\u003eWhat is kafka?\u003c/h2\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003eKafka is a streamsing platform for ingesting, storing, accessing and processing streams of data\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003ch1\u003eHigh level concepts\u003c/h1\u003e\n    \u003ch2\u003eCommunication model\u003c/h2\u003e\n    \u003cp\u003eOn the contrary of a \u003cstrong\u003eDirected Communication\u003c/strong\u003e where every service is aware of the other process to send messages to, kafka provides a centralized hub in which processes can send and receieve messages without being aware of each other. Such communication model is known as the \u003cstrong\u003ePublish/Subscribe\u003c/strong\u003e model which enhance the decoupling between different processes.\u003c/p\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003eOf course those are not termed by kafka\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003eRecall the different communication formalization according the sync/async and persistent/transient dimensions. Kafka fills conforms to the asynchronous and persisted model. This is unintuitive since this model is usually used for classical \u003cstrong\u003eQueues\u003c/strong\u003e rather than \u003cstrong\u003ePubsubs\u003c/strong\u003e.\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003ch2\u003eCore components\u003c/h2\u003e\n    \u003cp\u003eA \u003cstrong\u003eTopic\u003c/strong\u003e is a named stream of data (/ channel - CSP?).\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eProducer\u003c/strong\u003es are processes that publish data to a Topic.\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eConsumer\u003c/strong\u003es are processes that subscribe to data in one or more Topics.\u003c/p\u003e\n    \u003cp\u003eA \u003cstrong\u003eConsumer Group\u003c/strong\u003e is a set of Consumers that work together as a group.\u003c/p\u003e\n    \u003ch2\u003eStorage\u003c/h2\u003e\n    \u003ch3\u003eCommit Log\u003c/h3\u003e\n    \u003cp\u003eA \u003cstrong\u003eCommit Log\u003c/strong\u003e is an append only data structure which contain an \u003cem\u003eOrdered Sequence\u003c/em\u003e of events (/records)\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eRecords in the log are immutable\u003c/li\u003e\n      \u003cli\u003eRecords are ordered and their ordinal is known as their \u003cstrong\u003eOffset\u003c/strong\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003eIt is very similiar to existing transaction logs (Redo Log especially) which can be found in classical Relational Databases.\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003eTo gain perspectives we can find similiarities to STM concurrency mode, Redux stores etc...\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003ch3\u003ePartitions\u003c/h3\u003e\n    \u003cp\u003eIn order to provide distribution capabilities, there is no 1 to 1 correlation of Topic to Log since then each Topic would need to be stored on a single machine. Rather, a Topic is broken into smaller units called \u003cstrong\u003ePartitions\u003c/strong\u003e.\u003c/p\u003e\n    \u003cp\u003eEffectively, every Partition is a single Log. Such model allows kafka to distribute a Topic with N partitions to at most N workers and we can have K Consumers under the same Consumer Group sharing the load of consumption as long as LEQ(K, N). If K\u003eN than it means that some consumers will be idle.\u003c/p\u003e\n    \u003cp\u003eIt makes sense from a technical POV. Kafka simply assigns a Partition to a Consumer.\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eIf the number of Consumers is less than the number of Partitions, by the pigenhole principle there is at least a single Consumers that is assigned with more than one partition\u003c/li\u003e\n      \u003cli\u003eIf the number of Consumers is greather than the number of Partitions, there cannot be a surjective mapping from the Partitions the Consumers\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch3\u003eEvent\u003c/h3\u003e\n    \u003cp\u003eA unit of data in a Topic has many interchangeable names - Event, Message, Record. We favour the term Event.\u003c/p\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003eOut of the kafka scope, events in the fotware world are \u003cem\u003eEntities\u003c/em\u003e that desribe something that \u003cem\u003ehappened in the past\u003c/em\u003e. Therefore, event are past tense verbse. Well, it is the same in kafka\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003cp\u003eAn \u003cstrong\u003eEvent\u003c/strong\u003e is a timestamped key-value pair the records something that happened.\u003c/p\u003e\n    \u003cp\u003eEvent is composed of \u003cstrong\u003eHeaders\u003c/strong\u003e, \u003cstrong\u003eKeys\u003c/strong\u003e, a \u003cstrong\u003eTimestamp\u003c/strong\u003e and a \u003cstrong\u003eValue\u003c/strong\u003e. We will cover some of those at a later time but for a very brief overview\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eHeaders\u003c/strong\u003e contain optional metadata about the Event. One can use them to annotate, monitor and audit and Event. This is very like HTTP headers.\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eKeys\u003c/strong\u003e are optional values that affect how data is distributed accross partitions (Again, this is known concept, think of Partition Keys and partitioning schemes in different distributed systems). We will explore this concept independently.\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eTimestamp\u003c/strong\u003e holds information about when the thing that happened hapenned. There are multiple semantics of time we can speak of - When the Event created? When was it ingested? When was it processed? This is a whole topic of it's own which we will explore independantly.\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eValue\u003c/strong\u003e is the content of the message. Practically this is just a byte array and should be serialized according to the application.\u003c/p\u003e\n    \u003ch2\u003eThe Cluster\u003c/h2\u003e\n    \u003cp\u003eTODO: cover concepts below\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eBroker\u003c/strong\u003e TODO\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eReplication\u003c/strong\u003e TODO\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eLeader\u003c/strong\u003e TODO\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eFollower\u003c/strong\u003e TODO\u003c/p\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"posts/mariadb-notes","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/posts/mariadb-notes.md","metadata":{"title":"MariaDB Notes","description":null,"permalink":null,"priority":0,"tags":["MariaDB"],"categories":["Databases","Notes"]},"content":{"raw":"\nEverything is well documented in the MySQL/MariaDB documentations, but i'll gather some stuff here which I frequently encounter.\n\n## variables inspection\n\nrun ```mysqladmin variables```\n\n## notable variables/configurations\n\n__data_dir__ - the data directory\n\n\u003e usually ```/var/lib/mysql```\n\n__lower_case_table_names__ - as the name suggests, make all tables lower case.\n\n__local_infile__ - enable loading tables from local files.\n\n\u003e for commands such as: ```mysqlimport --local --fields-terminated-by=\"|\" -h localhost some-file.csv```\n\n## configuration files\n\nat least on the machine i am currently at, the root configuration is at ```/etc/my.cnf```. in turn it includes ```/etc/my.cnf.d```.\n\nin there, ill edit configurations in ```/etc/my.cnf.d/server.cnf```.\n\nfor example; to have all tables names with lower case and enable local in files, ill add the following under ```[mysqld]```\n- lower_case_table_names=1\n- local_infile=1\n\n## datadir notes\n\nwe can check where it is configured to be at using the variables inspection above, but usually it is at ```/var/lib/mysql```.\n\nit should be owned by the user/group running the process; those are usually mysql/mysql, but we can check this as well. assuming we start the process using ```systemd```, open the service file, and check what are the ```User``` and ```Group``` configured to be. the service file on my machine is at ```/usr/lib/systemd/system/mariadb.service```\n\n## complete reboot\n\nstop mariadb service - ```systemctl stop mariadb```\n\ndelete contents of datadir -```rm -rf /var/lib/mysql/*```\n\nremake infrastructure files - ```mysql_install_db --user=mysql --ldata=/var/lib/mysql```\n\nstart mariadb service - ```systemctl start mariadb```\n\n\u003e when we need such a thing?\n\u003e for example; there are configurations that only apply on installation, such as ```lower_case_table_names```\n","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003cp\u003eEverything is well documented in the MySQL/MariaDB documentations, but i'll gather some stuff here which I frequently encounter.\u003c/p\u003e\n    \u003ch2\u003evariables inspection\u003c/h2\u003e\n    \u003cp\u003erun \u003ccode\u003emysqladmin variables\u003c/code\u003e\u003c/p\u003e\n    \u003ch2\u003enotable variables/configurations\u003c/h2\u003e\n    \u003cp\u003e\u003cstrong\u003edata_dir\u003c/strong\u003e - the data directory\u003c/p\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003eusually \u003ccode\u003e/var/lib/mysql\u003c/code\u003e\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003cp\u003e\u003cstrong\u003elower_case_table_names\u003c/strong\u003e - as the name suggests, make all tables lower case.\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003elocal_infile\u003c/strong\u003e - enable loading tables from local files.\u003c/p\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003efor commands such as: \u003ccode\u003emysqlimport --local --fields-terminated-by=\"|\" -h localhost some-file.csv\u003c/code\u003e\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003ch2\u003econfiguration files\u003c/h2\u003e\n    \u003cp\u003eat least on the machine i am currently at, the root configuration is at \u003ccode\u003e/etc/my.cnf\u003c/code\u003e. in turn it includes \u003ccode\u003e/etc/my.cnf.d\u003c/code\u003e.\u003c/p\u003e\n    \u003cp\u003ein there, ill edit configurations in \u003ccode\u003e/etc/my.cnf.d/server.cnf\u003c/code\u003e.\u003c/p\u003e\n    \u003cp\u003efor example; to have all tables names with lower case and enable local in files, ill add the following under \u003ccode\u003e[mysqld]\u003c/code\u003e\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003elower_case_table_names=1\u003c/li\u003e\n      \u003cli\u003elocal_infile=1\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch2\u003edatadir notes\u003c/h2\u003e\n    \u003cp\u003ewe can check where it is configured to be at using the variables inspection above, but usually it is at \u003ccode\u003e/var/lib/mysql\u003c/code\u003e.\u003c/p\u003e\n    \u003cp\u003eit should be owned by the user/group running the process; those are usually mysql/mysql, but we can check this as well. assuming we start the process using \u003ccode\u003esystemd\u003c/code\u003e, open the service file, and check what are the \u003ccode\u003eUser\u003c/code\u003e and \u003ccode\u003eGroup\u003c/code\u003e configured to be. the service file on my machine is at \u003ccode\u003e/usr/lib/systemd/system/mariadb.service\u003c/code\u003e\u003c/p\u003e\n    \u003ch2\u003ecomplete reboot\u003c/h2\u003e\n    \u003cp\u003estop mariadb service - \u003ccode\u003esystemctl stop mariadb\u003c/code\u003e\u003c/p\u003e\n    \u003cp\u003edelete contents of datadir -\u003ccode\u003erm -rf /var/lib/mysql/*\u003c/code\u003e\u003c/p\u003e\n    \u003cp\u003eremake infrastructure files - \u003ccode\u003emysql_install_db --user=mysql --ldata=/var/lib/mysql\u003c/code\u003e\u003c/p\u003e\n    \u003cp\u003estart mariadb service - \u003ccode\u003esystemctl start mariadb\u003c/code\u003e\u003c/p\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003e\n        when we need such a thing?\n        for example; there are configurations that only apply on installation, such as \u003ccode\u003elower_case_table_names\u003c/code\u003e\n      \u003c/p\u003e\n    \u003c/blockquote\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"posts/pacman","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/posts/pacman.md","metadata":{"title":"Pacman","description":null,"permalink":null,"priority":0,"tags":[],"categories":[]},"content":{"raw":"\n# Pacman\n\nFocues on esoteric usages I sometimes do, instead of re-searching\n\n\u003e Perhaps do a single packages for all (apk, pacman, apt)?\n\n__Purge Package__\n\n    pacman -Rns {package}\n\nThe most common use case is to just delete a package {package}, removing all it's configurations and dependencies.\n\n- R is the _Remove_ operation\n- n flag indicates to remove the package's configuration\n- s flag indicates to remove unnecessary dependencies\n\n\n__List Orphans__\n\n    pacman -Qdt\n\n- Q is the _Query_ action\n- d filters only depnedencies\n- t filters only those who are unrequired\n\n__Delete orphans__\n\n    pacman -Rns $(pacman -Qdtq)\n\nNotice that we added a _q_ option to the _List Orphans_ command, it prints less information. Specifically, it makes it so the _Query_ operation will only return the package name. That way, we can easily pass it as an argument to the _Remove_ operation\n","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch1\u003ePacman\u003c/h1\u003e\n    \u003cp\u003eFocues on esoteric usages I sometimes do, instead of re-searching\u003c/p\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003ePerhaps do a single packages for all (apk, pacman, apt)?\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003cp\u003e\u003cstrong\u003ePurge Package\u003c/strong\u003e\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-properties\"\u003e\u003cspan class=\"hljs-attr\"\u003epacman\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e-Rns {package}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eThe most common use case is to just delete a package {package}, removing all it's configurations and dependencies.\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eR is the \u003cem\u003eRemove\u003c/em\u003e operation\u003c/li\u003e\n      \u003cli\u003en flag indicates to remove the package's configuration\u003c/li\u003e\n      \u003cli\u003es flag indicates to remove unnecessary dependencies\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003e\u003cstrong\u003eList Orphans\u003c/strong\u003e\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-properties\"\u003e\u003cspan class=\"hljs-attr\"\u003epacman\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e-Qdt\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cul\u003e\n      \u003cli\u003eQ is the \u003cem\u003eQuery\u003c/em\u003e action\u003c/li\u003e\n      \u003cli\u003ed filters only depnedencies\u003c/li\u003e\n      \u003cli\u003et filters only those who are unrequired\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003e\u003cstrong\u003eDelete orphans\u003c/strong\u003e\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-properties\"\u003e\u003cspan class=\"hljs-attr\"\u003epacman\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e-Rns $(pacman -Qdtq)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eNotice that we added a \u003cem\u003eq\u003c/em\u003e option to the \u003cem\u003eList Orphans\u003c/em\u003e command, it prints less information. Specifically, it makes it so the \u003cem\u003eQuery\u003c/em\u003e operation will only return the package name. That way, we can easily pass it as an argument to the \u003cem\u003eRemove\u003c/em\u003e operation\u003c/p\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"posts/postgres-notes","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/posts/postgres-notes.md","metadata":{"title":"Postgres Notes","description":null,"permalink":null,"priority":0,"tags":["PostgreSQL"],"categories":["Notes","Databases"]},"content":{"raw":"\nWithin the REPL, all buit-in commands are prefixed with ```\\```. For example;\n\n```\n\\?              Displays help page\n\\q              Quits the REPL\n\\l              Lists all databases\n\\c {database}   Connect to a different database\n```\n\n### Roles\n\nA Role is the single entity for authentication/authorization. According to how it is set up, it can act as a user, a group or both.\n\nCreate a role\n\n```sql\nCREATE ROLE \"somerole\"\n```\n\nGrant login to the role\n\n```sql\nALTER ROLE \"somerole\" WITH LOGIN;\n```\n\n### Serial (Auto Increment)\n\n```sql\nCREATE TABLE some_table (\n  id SERIAL PRIMARY KEY,\n  other varchar(20)\n)\n```\n","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003cp\u003eWithin the REPL, all buit-in commands are prefixed with \u003ccode\u003e\\\u003c/code\u003e. For example;\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-properties\"\u003e\u003cspan class=\"hljs-attr\"\u003e\\?\u003c/span\u003e              \u003cspan class=\"hljs-string\"\u003eDisplays help page\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003e\\q\u003c/span\u003e              \u003cspan class=\"hljs-string\"\u003eQuits the REPL\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003e\\l\u003c/span\u003e              \u003cspan class=\"hljs-string\"\u003eLists all databases\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003e\\c\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e{database}   Connect to a different database\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003ch3\u003eRoles\u003c/h3\u003e\n    \u003cp\u003eA Role is the single entity for authentication/authorization. According to how it is set up, it can act as a user, a group or both.\u003c/p\u003e\n    \u003cp\u003eCreate a role\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-sql\"\u003e\u003cspan class=\"hljs-keyword\"\u003eCREATE\u003c/span\u003e ROLE \"somerole\"\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eGrant login to the role\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-sql\"\u003e\u003cspan class=\"hljs-keyword\"\u003eALTER\u003c/span\u003e ROLE \"somerole\" \u003cspan class=\"hljs-keyword\"\u003eWITH\u003c/span\u003e LOGIN;\n\u003c/code\u003e\u003c/pre\u003e\n    \u003ch3\u003eSerial (Auto Increment)\u003c/h3\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-sql\"\u003e\u003cspan class=\"hljs-keyword\"\u003eCREATE\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eTABLE\u003c/span\u003e some_table (\n  id SERIAL \u003cspan class=\"hljs-keyword\"\u003ePRIMARY\u003c/span\u003e KEY,\n  other \u003cspan class=\"hljs-type\"\u003evarchar\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e)\n)\n\u003c/code\u003e\u003c/pre\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"posts/system","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/posts/system.md","metadata":{"title":"Scattered stuff to bring together","description":null,"permalink":null,"priority":0,"tags":[],"categories":[]},"content":{"raw":"\n# Cache\n\n**read-through** strategy is when the server checks if X is in the cache, if it does it returns it, if it does not, invoke the appropriate database query, cache the result as X, and return back.\n\n# Framework\n\n## 1. Undestand the problem and establish a scope for the design\n\n- Take your time\n- Ask clarifications\n- Write down assumptions\n\n## 2. High level proposition\n\n## 3. Design deep dive\n\n## 4. Deep dive\n","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch1\u003eCache\u003c/h1\u003e\n    \u003cp\u003e\u003cstrong\u003eread-through\u003c/strong\u003e strategy is when the server checks if X is in the cache, if it does it returns it, if it does not, invoke the appropriate database query, cache the result as X, and return back.\u003c/p\u003e\n    \u003ch1\u003eFramework\u003c/h1\u003e\n    \u003ch2\u003e1. Undestand the problem and establish a scope for the design\u003c/h2\u003e\n    \u003cul\u003e\n      \u003cli\u003eTake your time\u003c/li\u003e\n      \u003cli\u003eAsk clarifications\u003c/li\u003e\n      \u003cli\u003eWrite down assumptions\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch2\u003e2. High level proposition\u003c/h2\u003e\n    \u003ch2\u003e3. Design deep dive\u003c/h2\u003e\n    \u003ch2\u003e4. Deep dive\u003c/h2\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},{"id":"posts/vagrant-over-libvirt-arch","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/posts/vagrant-over-libvirt-arch.md","metadata":{"title":"Vagrant over libvirt on Archlinux","description":null,"permalink":null,"priority":0,"tags":[],"categories":[]},"content":{"raw":"\n# The tools\n\n[Vagrant](https://www.vagrantup.com/) is an open source software to virtualize development environments.\n\n[libvirt](https://libvit.org/) is an open source virtualization API. I think the [archwiki page](https://wiki.archlinux.org/title/Libvit) actually describes it better, as a collection of software that provides a way to manage virtualization functionality.\n\n[kvm](https://www.linux-kvm.org/page/Main_Page/) a short for **K**ernel-based **V**irtual **M**achine, is a virtualization infrastructure provided by the kernel.\n\n[QEMU](https://www.qemu.org/) is an open source machine emulator and virtualizer.\n\n# The stack\n\nVagrant will use the libvirt APIs which will be drived by QEMU utilizing kvm.\n\n```\n+-----------+\n|  Vagrant  |\n+-----------+\n|  libvirt  |\n+-----------+\n|  QEMU     |\n+-----------+\n|  KVM      |\n+-----------\n```\n\nTo install the stack we will need the following packages  \n- [vagrant](https://archlinux.org/packages/?name=vagrant)\n- [libvirt](https://archlinux.org/packages/?name=libvirt)\n- [iptables-nft](https://archlinux.org/packages/?name=iptables-nft)\n- [dnsmasq](https://archlinux.org/packages/?name=dnsmasq)\n- [qemu-headless](https://archlinux.org/packages/?name=qemu-headless)\n\nRun the command\n\n    pacman -Suy vagrant libvirt iptables-nft dnsmasq qemu-headless\n\n# Startup\n\nIn order to run the stack we need the following services running\n- libvirtd.service \n- virtlogd.service\n\nYou can either start them by running\n\n    systemctl start libvirtd virtlogd\n\nOr you can enable them by running\n\n    systemctl enable libvirtd virtlogd\n\nYou can check that everything is running by running\n\n    virsh -c qemu:///system\n\n\u003e [virsh](https://linux.die.net/man/1/virsh) is a cli to interact with guest domains (virtual machines).\n\n# Starting out first Vagrant Box\n\nIn Vagrant's domain, a Box is a package format, a bit like an ISO or a docker image. We will start a ```debian/bullseye64``` Boxed OS.\n\nInside your working directory, there should be a configuration file that Vagrant can read and function by - It is the ```Vagrantfile```. The Vagrantfile contains configuration about what Box to run, how many resources to allocate and more.\n\nIn a new environment, we can either create a file manually or use Vagrants ```init``` command to do this for us. \n\n    vagrant init --minimal\n\nNow, lets edit the newly created Vagrantfile and set the parameter ```config.vm.box``` to \"debian/bullseye64\". Lets also add the following parameters  \n\nWe can always validate the Vagrantfile by running\n\n    vagrant validate\n\nAt this point, during validation, you might get an error like ```No usable default provider could be found for your system```. Thats fine, see next.\n\nVagrant uses a notion of Providers. A Provider is Vagrant's abstraction over the underlying hypervisor that vagrant will operate. In our case, as illustrated above, we use libvirt as the hypervisor so we will need to install the relevant Provider. To install the Provider we will use Vagrant's Plugins mechanism and run\n\n    vagrant plugin install vagrant-libvirt\n\nNow we are finally ready to run the Box by running the command\n\n    vagrant up --provider=libvirt\n\nSome errors you might encounter\n\n**Some error about polkit indicates permission issues**. Read about it for best scenario solution, as a quick and easy solution, add your user to the libvirt group\n\n**Some error about your machine not supporting NFS.** Just install nfs-utils by running\n\n    pacman -Syu nfs-utils\n\n**Forevr waiting on IP acquisition.** I still havn't completely figured this out. It looks somethong about DNS and DHCP. What I did find out that some Boxes do not get this issue, try changing.\n\nTo ssh into the OS run\n\n    vagrant ssh\n\nTo clean up run\n\n    vagrant destroy\n\n# Other tools\n\n__virt-manager__ is a graphical tool to list and manage the guest domains.\n","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch1\u003eThe tools\u003c/h1\u003e\n    \u003cp\u003e\u003ca href=\"https://www.vagrantup.com/\"\u003eVagrant\u003c/a\u003e is an open source software to virtualize development environments.\u003c/p\u003e\n    \u003cp\u003e\u003ca href=\"https://libvit.org/\"\u003elibvirt\u003c/a\u003e is an open source virtualization API. I think the \u003ca href=\"https://wiki.archlinux.org/title/Libvit\"\u003earchwiki page\u003c/a\u003e actually describes it better, as a collection of software that provides a way to manage virtualization functionality.\u003c/p\u003e\n    \u003cp\u003e\u003ca href=\"https://www.linux-kvm.org/page/Main_Page/\"\u003ekvm\u003c/a\u003e a short for \u003cstrong\u003eK\u003c/strong\u003eernel-based \u003cstrong\u003eV\u003c/strong\u003eirtual \u003cstrong\u003eM\u003c/strong\u003eachine, is a virtualization infrastructure provided by the kernel.\u003c/p\u003e\n    \u003cp\u003e\u003ca href=\"https://www.qemu.org/\"\u003eQEMU\u003c/a\u003e is an open source machine emulator and virtualizer.\u003c/p\u003e\n    \u003ch1\u003eThe stack\u003c/h1\u003e\n    \u003cp\u003eVagrant will use the libvirt APIs which will be drived by QEMU utilizing kvm.\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-asciidoc\"\u003e\u003cspan class=\"hljs-code\"\u003e+-----------+\u003c/span\u003e\n\u003cspan class=\"hljs-section\"\u003e|  Vagrant  |\n+-----------+\u003c/span\u003e\n\u003cspan class=\"hljs-section\"\u003e|  libvirt  |\n+-----------+\u003c/span\u003e\n\u003cspan class=\"hljs-section\"\u003e|  QEMU     |\n+-----------+\u003c/span\u003e\n\u003cspan class=\"hljs-section\"\u003e|  KVM      |\n+-----------\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eTo install the stack we will need the following packages\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"https://archlinux.org/packages/?name=vagrant\"\u003evagrant\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"https://archlinux.org/packages/?name=libvirt\"\u003elibvirt\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"https://archlinux.org/packages/?name=iptables-nft\"\u003eiptables-nft\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"https://archlinux.org/packages/?name=dnsmasq\"\u003ednsmasq\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"https://archlinux.org/packages/?name=qemu-headless\"\u003eqemu-headless\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eRun the command\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-properties\"\u003e\u003cspan class=\"hljs-attr\"\u003epacman\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e-Suy vagrant libvirt iptables-nft dnsmasq qemu-headless\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003ch1\u003eStartup\u003c/h1\u003e\n    \u003cp\u003eIn order to run the stack we need the following services running\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003elibvirtd.service\u003c/li\u003e\n      \u003cli\u003evirtlogd.service\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eYou can either start them by running\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-properties\"\u003e\u003cspan class=\"hljs-attr\"\u003esystemctl\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003estart libvirtd virtlogd\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eOr you can enable them by running\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-properties\"\u003e\u003cspan class=\"hljs-attr\"\u003esystemctl\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eenable libvirtd virtlogd\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eYou can check that everything is running by running\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-perl\"\u003evirsh -c qemu:\u003cspan class=\"hljs-regexp\"\u003e//\u003c/span\u003e/\u003cspan class=\"hljs-keyword\"\u003esystem\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003e\u003ca href=\"https://linux.die.net/man/1/virsh\"\u003evirsh\u003c/a\u003e is a cli to interact with guest domains (virtual machines).\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003ch1\u003eStarting out first Vagrant Box\u003c/h1\u003e\n    \u003cp\u003eIn Vagrant's domain, a Box is a package format, a bit like an ISO or a docker image. We will start a \u003ccode\u003edebian/bullseye64\u003c/code\u003e Boxed OS.\u003c/p\u003e\n    \u003cp\u003eInside your working directory, there should be a configuration file that Vagrant can read and function by - It is the \u003ccode\u003eVagrantfile\u003c/code\u003e. The Vagrantfile contains configuration about what Box to run, how many resources to allocate and more.\u003c/p\u003e\n    \u003cp\u003eIn a new environment, we can either create a file manually or use Vagrants \u003ccode\u003einit\u003c/code\u003e command to do this for us.\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-properties\"\u003e\u003cspan class=\"hljs-attr\"\u003evagrant\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003einit --minimal\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eNow, lets edit the newly created Vagrantfile and set the parameter \u003ccode\u003econfig.vm.box\u003c/code\u003e to \"debian/bullseye64\". Lets also add the following parameters\u003c/p\u003e\n    \u003cp\u003eWe can always validate the Vagrantfile by running\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-properties\"\u003e\u003cspan class=\"hljs-attr\"\u003evagrant\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003evalidate\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eAt this point, during validation, you might get an error like \u003ccode\u003eNo usable default provider could be found for your system\u003c/code\u003e. Thats fine, see next.\u003c/p\u003e\n    \u003cp\u003eVagrant uses a notion of Providers. A Provider is Vagrant's abstraction over the underlying hypervisor that vagrant will operate. In our case, as illustrated above, we use libvirt as the hypervisor so we will need to install the relevant Provider. To install the Provider we will use Vagrant's Plugins mechanism and run\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-properties\"\u003e\u003cspan class=\"hljs-attr\"\u003evagrant\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eplugin install vagrant-libvirt\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eNow we are finally ready to run the Box by running the command\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-ini\"\u003evagrant up \u003cspan class=\"hljs-attr\"\u003e--provider\u003c/span\u003e=libvirt\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eSome errors you might encounter\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eSome error about polkit indicates permission issues\u003c/strong\u003e. Read about it for best scenario solution, as a quick and easy solution, add your user to the libvirt group\u003c/p\u003e\n    \u003cp\u003e\u003cstrong\u003eSome error about your machine not supporting NFS.\u003c/strong\u003e Just install nfs-utils by running\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-properties\"\u003e\u003cspan class=\"hljs-attr\"\u003epacman\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e-Syu nfs-utils\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003e\u003cstrong\u003eForevr waiting on IP acquisition.\u003c/strong\u003e I still havn't completely figured this out. It looks somethong about DNS and DHCP. What I did find out that some Boxes do not get this issue, try changing.\u003c/p\u003e\n    \u003cp\u003eTo ssh into the OS run\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-properties\"\u003e\u003cspan class=\"hljs-attr\"\u003evagrant\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003essh\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003eTo clean up run\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-properties\"\u003e\u003cspan class=\"hljs-attr\"\u003evagrant\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003edestroy\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n    \u003ch1\u003eOther tools\u003c/h1\u003e\n    \u003cp\u003e\u003cstrong\u003evirt-manager\u003c/strong\u003e is a graphical tool to list and manage the guest domains.\u003c/p\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}}],"metadataAggregation":{"categories":{"Algorithms":{"count":4},"Aws":{"count":5},"Domain Driven Design":{"count":4},"Kubernetes":{"count":6},"Guides":{"count":1},"Cheatsheets":{"count":1},"Databases":{"count":2},"Notes":{"count":2}},"tags":{"Dynamic Programming":{"count":3},"Palindrom":{"count":1},"Kubernetes":{"count":6},"Archlinux":{"count":1},"Linux":{"count":1},"Emacs":{"count":1},"MariaDB":{"count":1},"PostgreSQL":{"count":1}}}}},"__N_SSG":true},"page":"/debug","query":{},"buildId":"yGlP4Z_vvfL6c7wzC5t24","assetPrefix":".","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>