<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/fc3ec0f12aa27086.css" as="style"/><link rel="stylesheet" href="/_next/static/css/fc3ec0f12aa27086.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a7fabc11ce9fdee5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a7fabc11ce9fdee5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-20cc5a3cc3d233fe.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-97927b8cd08b2c1f.js" defer=""></script><script src="/_next/static/chunks/pages/_app-0fb546fbb661cca9.js" defer=""></script><script src="/_next/static/chunks/175675d1-7de8b3bfdcedb0f1.js" defer=""></script><script src="/_next/static/chunks/113-d38bb2befbc77dc7.js" defer=""></script><script src="/_next/static/chunks/pages/_articles/%5B...articleId%5D-14c11b9e88d91c56.js" defer=""></script><script src="/_next/static/JQBmH912bFvvEYPKe2kRJ/_buildManifest.js" defer=""></script><script src="/_next/static/JQBmH912bFvvEYPKe2kRJ/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="Layout_container__K9hpm"><nav class="p-6 border border-r-2"><div class="flex flex-col items-center"><img src="/logo.png" width="100" height="100"/><span class="text-3xl font-semibold">Tal Glanzman</span><span class="text-lg">Software Engineer</span></div><div class="mt-6"><ul><li><a href="/">Home</a></li><li><a href="/_index/">Index</a></li><details><summary>Computer Science</summary><ul class="ml-4"><li><a href="/_articles/computer-science/linear-programming/">Linear Programming</a></li></ul></details><li><a href="/_articles/about/">About</a></li></ul></div><div class="mt-6"><div><form action="/_search" autoComplete="off" class="flex flex-col justify-center items-center"><input class="text-input" type="text" name="query" placeholder="Search"/><input class="button" type="submit" hidden="" value="Search"/></form></div></div></nav><header><div class=" p-2 flex flex-col items-center"><span class="text-4xl font-semibold">The Probabilistic Method</span><span class="text-lg"></span></div></header><main><div class="text-sm m-4"><div><span>Categories<!-- -->: </span><span><span class="inline-block"><a href="/_categories/Computer%20Science/">Computer Science</a></span><span class="inline-block">, <a href="/_categories/Probabilistic%20Method/">Probabilistic Method</a></span></span></div><div><span>Tags<!-- -->: </span><span><span class="inline-block"><a href="/_tags/Probabilistic%20Method/">Probabilistic Method</a></span><span class="inline-block">, <a href="/_tags/Probability/">Probability</a></span><span class="inline-block">, <a href="/_tags/Algorithms/">Algorithms</a></span></span></div></div><div class="ArticleContent_md__52PoF"><!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>
  <body>
    <nav class="toc">
      <ol class="toc-level toc-level-1">
        <li class="toc-item toc-item-h1"><a class="toc-link toc-link-h1" href="#introduction">Introduction</a></li>
        <li class="toc-item toc-item-h1"><a class="toc-link toc-link-h1" href="#probability-background">Probability Background</a></li>
      </ol>
    </nav>
    <h1 id="introduction">Introduction</h1>
    <p>In this page we will discuss the probabilistic method which is a powerful tool to prove the existence of a combinatorial object. For a long time we have used conventional approaches for this purpose - Either we provided a proof by contruction or we provided a non-constructive proof. The probabilistic method is a non-constructive method first introduced by <a href="https://en.wikipedia.org/wiki/Paul_Erd%C5%91s">Paul Erdos</a> while he was working on the development of the <a href="https://en.wikipedia.org/wiki/Ramsey_theory">Ramsey Theory</a>.</p>
    <p>In essence, the method shows that the probability that some object with the desired property exist is greater than 0 and therefore one such instance surely exist, otherwise the probability was strictly 0.</p>
    <p>We will use many practical examples and uses of this method which we will link to throughout this page.</p>
    <h1 id="probability-background">Probability Background</h1>
    <p>Before we continue we need to review some basic probability concepts.</p>
    <h3 id="definition">Definition</h3>
    <p>A <strong>probability space</strong> is a triplet $(\Omega, \Sigma, Pr)$ where</p>
    <ol>
      <li>
        <p>$\Omega$ is a non-empty set known as the <strong>sample space</strong>. It represents all possible outcomes of some experiment.</p>
        <ul>
          <li>We will limit ourselves to a finite sample space.</li>
        </ul>
      </li>
      <li>
        <p>$\Sigma$ is the collection of subsets of $\Omega$ which is closed under the <strong>complement ($x'$)</strong>, <strong>union ($x \cup y$)</strong> and <strong>intersection ($x \cap y$)</strong> operations.</p>
        <ul>
          <li>An element $A \in \Sigma$ is called an <strong>event</strong>.</li>
        </ul>
      </li>
      <li>
        <p>$Pr : \Sigma \rightarrow [0, 1]$ is a function with the following properties:</p>
        <ul>
          <li>$Pr(\Omega) = 1$</li>
          <li>$\forall A, B \in \Sigma ; A \cap B = \phi \implies Pr(A) + Pr(B) = Pr(A \cup B)$</li>
          <li>$\forall \omega \in \Omega ; $ we will denote $Pr(\omega) = Pr(\{\omega\})$</li>
        </ul>
      </li>
    </ol>
    <blockquote>
      <p>Methematically, a probability space is a measure space with the measure function over $\Omega$ where $\Sigma$ is the $\sigma$-algebra over $\Omega$ and $Pr$ is the measure of the space such that $Pr(\Omega) = 1$.</p>
    </blockquote>
    <h3 id="union-bound">Union Bound</h3>
    <p>The measue function is sub-additive with respect to the union operation. Meaning, every two events $A, B$ satisfies $Pr(A \cup B) \leq Pr(A) + Pr(B)$.</p>
    <p>It makes sense, if the two events are not disjointed, the intersection part must be calculated once, not twice as is calculated in $Pr(A) + $Pr(B)$.</p>
    <p>More generally, the <strong>Union Bound</strong> theorem states that for any set of events $\{ A_i \}$ it holds that:</p>
    <p>
      $$
      Pr(\bigcup_i A_i) \leq \sum_i Pr(A_i)
      $$
    </p>
    <h3 id="conditional-probability">Conditional Probability</h3>
    <p>The probability of some event $A$, given that we know that the event $B$ happened is called the <strong>conditional probability of $A$ given $B$</strong> and is notated and given by</p>
    <p>
      $$
      Pr(A | B) = \frac{Pr(A \cap B)}{Pr(B)}
      $$
    </p>
    <p>This definition is intuitive. The probability $A$ happens given $B$ happens is firstly at the intersection between them. Then, we scale this probability of the intersection to limit ourselves to the "world" of $B$ only.</p>
    <p>Another variant of this definition is known as the <strong>product rule</strong>:</p>
    <p>
      $$
      Pr(A \cap B) = Pr(A | B) Pr(B)
      $$
    </p>
    <p>Let's see an example - Consider the experiment of rolling a six-sided die and let $A$ be the event that the number $2$ was thrown and $B$ the event that an even number was thrown. Formally, $A = \{ 2 \}, B = \{2, 4, 6 \}$. It is obvious that $Pr(A) = \frac{1}{6}$ and $Pr(B) = \frac{3}{6} = \frac{1}{2}$. Intuitively, the probability that $A$ given $B$ is $\frac{1}{3}$ since if we know an even number was rolled it means that one of three numbers 2, 4 and 6 were rolled - from those, the probability that 2 was rolled is $\frac{1}{3}$. We can verify the intuition using the formula: $Pr(A | B) = \frac{Pr(A \cap B)}{Pr(B)} = \frac{\frac{1}{6}}{\frac{1}{2}} = \frac{1}{3}$.</p>
    <p>Now, what happens when what we know about $B$ doesn't affect $A$? For example, consider $A$ to be the result of a die roll and $B$ the event that it is Sunday - the result of the roll is not affected by the day of week at all. Such events are called <strong>independent</strong>, let's define it:</p>
    <p>Two events $A$ and $B$ such that $Pr(A | B) = Pr(A)$ are said to be <strong>independent</strong>.</p>
    <p>For two independent events $A$ and $B$ we can see that:</p>
    <p>
      $$
      Pr(A) = Pr(A | B) = \frac{Pr(A \cap B)}{Pr(B)} \implies Pr(A)Pr(B) = Pr(A \cap B)
      $$
    </p>
    <p>Using a bit of algebra, we can get the <strong>very</strong> important rule, <strong>Bayes Rule</strong> which states that if we can always invert the conditional causality:</p>
    <p>
      $$
      Pr(A | B) = \frac{Pr(B | A) Pr(A)}{Pr(B)}
      $$
    </p>
  </body>
</html>
</div></main><footer><div class=" border-t-2 p-4 flex flex-row justify-center items-center space-x-4"><a href="https://github.com/tglanz"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://il.linkedin.com/in/tal-glanzman"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"article":{"id":"computer-science/probabalistic-method","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/computer-science/probabalistic-method.md","metadata":{"title":"The Probabilistic Method","description":null,"priority":0,"tags":["Probabilistic Method","Probability","Algorithms"],"categories":["Computer Science","Probabilistic Method"],"toc":true},"content":{"raw":"\n# Introduction\n\nIn this page we will discuss the probabilistic method which is a powerful tool to prove the existence of a combinatorial object. For a long time we have used conventional approaches for this purpose - Either we provided a proof by contruction or we provided a non-constructive proof. The probabilistic method is a non-constructive method first introduced by [Paul Erdos](https://en.wikipedia.org/wiki/Paul_Erd%C5%91s) while he was working on the development of the [Ramsey Theory](https://en.wikipedia.org/wiki/Ramsey_theory).\n\nIn essence, the method shows that the probability that some object with the desired property exist is greater than 0 and therefore one such instance surely exist, otherwise the probability was strictly 0. \n\nWe will use many practical examples and uses of this method which we will link to throughout this page.\n\n# Probability Background\n\nBefore we continue we need to review some basic probability concepts.\n\n### Definition\n\nA **probability space** is a triplet $(\\Omega, \\Sigma, Pr)$ where\n\n1. $\\Omega$ is a non-empty set known as the **sample space**. It represents all possible outcomes of some experiment.\n    - We will limit ourselves to a finite sample space.\n\n2. $\\Sigma$ is the collection of subsets of $\\Omega$ which is closed under the **complement ($x'$)**, **union ($x \\cup y$)** and **intersection ($x \\cap y$)** operations.\n    - An element $A \\in \\Sigma$ is called an **event**.\n\n3. $Pr : \\Sigma \\rightarrow [0, 1]$ is a function with the following properties:\n    - $Pr(\\Omega) = 1$\n    - $\\forall A, B \\in \\Sigma ; A \\cap B = \\phi \\implies Pr(A) + Pr(B) = Pr(A \\cup B)$\n    - $\\forall \\omega \\in \\Omega ; $  we will denote $Pr(\\omega) = Pr(\\\\{\\omega\\\\})$\n\n\u003e Methematically, a probability space is a measure space with the measure function over $\\Omega$ where $\\Sigma$ is the $\\sigma$-algebra over $\\Omega$ and $Pr$ is the measure of the space such that $Pr(\\Omega) = 1$.\n\n### Union Bound\n\nThe measue function is sub-additive with respect to the union operation. Meaning, every two events $A, B$ satisfies $Pr(A \\cup B) \\leq Pr(A) + Pr(B)$.\n\nIt makes sense, if the two events are not disjointed, the intersection part must be calculated once, not twice as is calculated in $Pr(A) + $Pr(B)$.\n\nMore generally, the **Union Bound** theorem states that for any set of events $\\\\{ A_i \\\\}$ it holds that:\n\n$$\n  Pr(\\bigcup_i A_i) \\leq \\sum_i Pr(A_i)\n$$\n\n### Conditional Probability\n\nThe probability of some event $A$, given that we know that the event $B$ happened is called the **conditional probability of $A$ given $B$** and is notated and given by\n\n$$\n  Pr(A | B) = \\frac{Pr(A \\cap B)}{Pr(B)}\n$$\n\nThis definition is intuitive. The probability $A$ happens given $B$ happens is firstly at the intersection between them. Then, we scale this probability of the intersection to limit ourselves to the \"world\" of $B$ only.\n\nAnother variant of this definition is known as the **product rule**:\n\n$$\n  Pr(A \\cap B) = Pr(A | B) Pr(B)\n$$\n\nLet's see an example - Consider the experiment of rolling a six-sided die and let $A$ be the event that the number $2$ was thrown and $B$ the event that an even number was thrown. Formally, $A = \\\\{ 2 \\\\}, B = \\\\{2, 4, 6 \\\\}$. It is obvious that $Pr(A) = \\frac{1}{6}$ and $Pr(B) = \\frac{3}{6} = \\frac{1}{2}$. Intuitively, the probability that $A$ given $B$ is $\\frac{1}{3}$ since if we know an even number was rolled it means that one of three numbers 2, 4 and 6 were rolled - from those, the probability that 2 was rolled is $\\frac{1}{3}$. We can verify the intuition using the formula: $Pr(A | B) = \\frac{Pr(A \\cap B)}{Pr(B)} = \\frac{\\frac{1}{6}}{\\frac{1}{2}} = \\frac{1}{3}$.\n\nNow, what happens when what we know about $B$ doesn't affect $A$? For example, consider $A$ to be the result of a die roll and $B$ the event that it is Sunday - the result of the roll is not affected by the day of week at all. Such events are called **independent**, let's define it:\n\nTwo events $A$ and $B$ such that $Pr(A | B) = Pr(A)$ are said to be **independent**.\n\nFor two independent events $A$ and $B$ we can see that:\n\n$$\n    Pr(A) = Pr(A | B) = \\frac{Pr(A \\cap B)}{Pr(B)} \\implies Pr(A)Pr(B) = Pr(A \\cap B)\n$$\n\nUsing a bit of algebra, we can get the **very** important rule, **Bayes Rule** which states that if we can always invert the conditional causality:\n\n$$\n    Pr(A | B) = \\frac{Pr(B | A) Pr(A)}{Pr(B)}\n$$\n","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003cnav class=\"toc\"\u003e\n      \u003col class=\"toc-level toc-level-1\"\u003e\n        \u003cli class=\"toc-item toc-item-h1\"\u003e\u003ca class=\"toc-link toc-link-h1\" href=\"#introduction\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n        \u003cli class=\"toc-item toc-item-h1\"\u003e\u003ca class=\"toc-link toc-link-h1\" href=\"#probability-background\"\u003eProbability Background\u003c/a\u003e\u003c/li\u003e\n      \u003c/ol\u003e\n    \u003c/nav\u003e\n    \u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n    \u003cp\u003eIn this page we will discuss the probabilistic method which is a powerful tool to prove the existence of a combinatorial object. For a long time we have used conventional approaches for this purpose - Either we provided a proof by contruction or we provided a non-constructive proof. The probabilistic method is a non-constructive method first introduced by \u003ca href=\"https://en.wikipedia.org/wiki/Paul_Erd%C5%91s\"\u003ePaul Erdos\u003c/a\u003e while he was working on the development of the \u003ca href=\"https://en.wikipedia.org/wiki/Ramsey_theory\"\u003eRamsey Theory\u003c/a\u003e.\u003c/p\u003e\n    \u003cp\u003eIn essence, the method shows that the probability that some object with the desired property exist is greater than 0 and therefore one such instance surely exist, otherwise the probability was strictly 0.\u003c/p\u003e\n    \u003cp\u003eWe will use many practical examples and uses of this method which we will link to throughout this page.\u003c/p\u003e\n    \u003ch1 id=\"probability-background\"\u003eProbability Background\u003c/h1\u003e\n    \u003cp\u003eBefore we continue we need to review some basic probability concepts.\u003c/p\u003e\n    \u003ch3 id=\"definition\"\u003eDefinition\u003c/h3\u003e\n    \u003cp\u003eA \u003cstrong\u003eprobability space\u003c/strong\u003e is a triplet $(\\Omega, \\Sigma, Pr)$ where\u003c/p\u003e\n    \u003col\u003e\n      \u003cli\u003e\n        \u003cp\u003e$\\Omega$ is a non-empty set known as the \u003cstrong\u003esample space\u003c/strong\u003e. It represents all possible outcomes of some experiment.\u003c/p\u003e\n        \u003cul\u003e\n          \u003cli\u003eWe will limit ourselves to a finite sample space.\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\n        \u003cp\u003e$\\Sigma$ is the collection of subsets of $\\Omega$ which is closed under the \u003cstrong\u003ecomplement ($x'$)\u003c/strong\u003e, \u003cstrong\u003eunion ($x \\cup y$)\u003c/strong\u003e and \u003cstrong\u003eintersection ($x \\cap y$)\u003c/strong\u003e operations.\u003c/p\u003e\n        \u003cul\u003e\n          \u003cli\u003eAn element $A \\in \\Sigma$ is called an \u003cstrong\u003eevent\u003c/strong\u003e.\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\n        \u003cp\u003e$Pr : \\Sigma \\rightarrow [0, 1]$ is a function with the following properties:\u003c/p\u003e\n        \u003cul\u003e\n          \u003cli\u003e$Pr(\\Omega) = 1$\u003c/li\u003e\n          \u003cli\u003e$\\forall A, B \\in \\Sigma ; A \\cap B = \\phi \\implies Pr(A) + Pr(B) = Pr(A \\cup B)$\u003c/li\u003e\n          \u003cli\u003e$\\forall \\omega \\in \\Omega ; $ we will denote $Pr(\\omega) = Pr(\\{\\omega\\})$\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ol\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003eMethematically, a probability space is a measure space with the measure function over $\\Omega$ where $\\Sigma$ is the $\\sigma$-algebra over $\\Omega$ and $Pr$ is the measure of the space such that $Pr(\\Omega) = 1$.\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003ch3 id=\"union-bound\"\u003eUnion Bound\u003c/h3\u003e\n    \u003cp\u003eThe measue function is sub-additive with respect to the union operation. Meaning, every two events $A, B$ satisfies $Pr(A \\cup B) \\leq Pr(A) + Pr(B)$.\u003c/p\u003e\n    \u003cp\u003eIt makes sense, if the two events are not disjointed, the intersection part must be calculated once, not twice as is calculated in $Pr(A) + $Pr(B)$.\u003c/p\u003e\n    \u003cp\u003eMore generally, the \u003cstrong\u003eUnion Bound\u003c/strong\u003e theorem states that for any set of events $\\{ A_i \\}$ it holds that:\u003c/p\u003e\n    \u003cp\u003e\n      $$\n      Pr(\\bigcup_i A_i) \\leq \\sum_i Pr(A_i)\n      $$\n    \u003c/p\u003e\n    \u003ch3 id=\"conditional-probability\"\u003eConditional Probability\u003c/h3\u003e\n    \u003cp\u003eThe probability of some event $A$, given that we know that the event $B$ happened is called the \u003cstrong\u003econditional probability of $A$ given $B$\u003c/strong\u003e and is notated and given by\u003c/p\u003e\n    \u003cp\u003e\n      $$\n      Pr(A | B) = \\frac{Pr(A \\cap B)}{Pr(B)}\n      $$\n    \u003c/p\u003e\n    \u003cp\u003eThis definition is intuitive. The probability $A$ happens given $B$ happens is firstly at the intersection between them. Then, we scale this probability of the intersection to limit ourselves to the \"world\" of $B$ only.\u003c/p\u003e\n    \u003cp\u003eAnother variant of this definition is known as the \u003cstrong\u003eproduct rule\u003c/strong\u003e:\u003c/p\u003e\n    \u003cp\u003e\n      $$\n      Pr(A \\cap B) = Pr(A | B) Pr(B)\n      $$\n    \u003c/p\u003e\n    \u003cp\u003eLet's see an example - Consider the experiment of rolling a six-sided die and let $A$ be the event that the number $2$ was thrown and $B$ the event that an even number was thrown. Formally, $A = \\{ 2 \\}, B = \\{2, 4, 6 \\}$. It is obvious that $Pr(A) = \\frac{1}{6}$ and $Pr(B) = \\frac{3}{6} = \\frac{1}{2}$. Intuitively, the probability that $A$ given $B$ is $\\frac{1}{3}$ since if we know an even number was rolled it means that one of three numbers 2, 4 and 6 were rolled - from those, the probability that 2 was rolled is $\\frac{1}{3}$. We can verify the intuition using the formula: $Pr(A | B) = \\frac{Pr(A \\cap B)}{Pr(B)} = \\frac{\\frac{1}{6}}{\\frac{1}{2}} = \\frac{1}{3}$.\u003c/p\u003e\n    \u003cp\u003eNow, what happens when what we know about $B$ doesn't affect $A$? For example, consider $A$ to be the result of a die roll and $B$ the event that it is Sunday - the result of the roll is not affected by the day of week at all. Such events are called \u003cstrong\u003eindependent\u003c/strong\u003e, let's define it:\u003c/p\u003e\n    \u003cp\u003eTwo events $A$ and $B$ such that $Pr(A | B) = Pr(A)$ are said to be \u003cstrong\u003eindependent\u003c/strong\u003e.\u003c/p\u003e\n    \u003cp\u003eFor two independent events $A$ and $B$ we can see that:\u003c/p\u003e\n    \u003cp\u003e\n      $$\n      Pr(A) = Pr(A | B) = \\frac{Pr(A \\cap B)}{Pr(B)} \\implies Pr(A)Pr(B) = Pr(A \\cap B)\n      $$\n    \u003c/p\u003e\n    \u003cp\u003eUsing a bit of algebra, we can get the \u003cstrong\u003every\u003c/strong\u003e important rule, \u003cstrong\u003eBayes Rule\u003c/strong\u003e which states that if we can always invert the conditional causality:\u003c/p\u003e\n    \u003cp\u003e\n      $$\n      Pr(A | B) = \\frac{Pr(B | A) Pr(A)}{Pr(B)}\n      $$\n    \u003c/p\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}}},"__N_SSG":true},"page":"/_articles/[...articleId]","query":{"articleId":["computer-science","probabalistic-method"]},"buildId":"JQBmH912bFvvEYPKe2kRJ","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>