{"pageProps":{"article":{"id":"posts/kafka","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/posts/kafka.md","metadata":{"title":"Kafka","description":null,"permalink":null,"priority":0,"tags":[],"categories":[]},"content":{"raw":"\n# Overview\n\n## What is kafka?\n\n> Kafka is a streamsing platform for ingesting, storing, accessing and processing streams of data\n\n# High level concepts\n\n## Communication model\n\nOn the contrary of a **Directed Communication** where every service is aware of the other process to send messages to, kafka provides a centralized hub in which processes can send and receieve messages without being aware of each other. Such communication model is known as the **Publish/Subscribe** model which enhance the decoupling between different processes.\n\n> Of course those are not termed by kafka\n\n> Recall the different communication formalization according the sync/async and persistent/transient dimensions. Kafka fills conforms to the asynchronous and persisted model. This is unintuitive since this model is usually used for classical **Queues** rather than **Pubsubs**.\n\n## Core components\n\nA **Topic** is a named stream of data (/ channel - CSP?).\n\n**Producer**s are processes that publish data to a Topic.\n\n**Consumer**s are processes that subscribe to data in one or more Topics.\n\nA **Consumer Group** is a set of Consumers that work together as a group.\n\n## Storage\n\n### Commit Log\n\nA **Commit Log** is an append only data structure which contain an *Ordered Sequence* of events (/records)\n\n- Records in the log are immutable\n- Records are ordered and their ordinal is known as their **Offset**\n\n> It is very similiar to existing transaction logs (Redo Log especially) which can be found in classical Relational Databases.\n\n> To gain perspectives we can find similiarities to STM concurrency mode, Redux stores etc...\n\n### Partitions\n\nIn order to provide distribution capabilities, there is no 1 to 1 correlation of Topic to Log since then each Topic would need to be stored on a single machine. Rather, a Topic is broken into smaller units called **Partitions**.\n\nEffectively, every Partition is a single Log. Such model allows kafka to distribute a Topic with N partitions to at most N workers and we can have K Consumers under the same Consumer Group sharing the load of consumption as long as LEQ(K, N). If K>N than it means that some consumers will be idle.\n\nIt makes sense from a technical POV. Kafka simply assigns a Partition to a Consumer.\n\n- If the number of Consumers is less than the number of Partitions, by the pigenhole principle there is at least a single Consumers that is assigned with more than one partition\n- If the number of Consumers is greather than the number of Partitions, there cannot be a surjective mapping from the Partitions the Consumers\n\n### Event\n\nA unit of data in a Topic has many interchangeable names - Event, Message, Record. We favour the term Event.\n\n> Out of the kafka scope, events in the fotware world are *Entities* that desribe something that *happened in the past*. Therefore, event are past tense verbse. Well, it is the same in kafka\n\nAn **Event** is a timestamped key-value pair the records something that happened.\n\nEvent is composed of **Headers**, **Keys**, a **Timestamp** and a **Value**. We will cover some of those at a later time but for a very brief overview\n\n**Headers** contain optional metadata about the Event. One can use them to annotate, monitor and audit and Event. This is very like HTTP headers.\n\n**Keys** are optional values that affect how data is distributed accross partitions (Again, this is known concept, think of Partition Keys and partitioning schemes in different distributed systems). We will explore this concept independently.\n\n**Timestamp** holds information about when the thing that happened hapenned. There are multiple semantics of time we can speak of - When the Event created? When was it ingested? When was it processed? This is a whole topic of it's own which we will explore independantly.\n\n**Value** is the content of the message. Practically this is just a byte array and should be serialized according to the application.\n\n## The Cluster\n\nTODO: cover concepts below\n\n**Broker** TODO\n\n**Replication** TODO\n\n**Leader** TODO\n\n**Follower** TODO\n","html":"<!doctype html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"utf-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n  </head>\n  <body>\n    <h1>Overview</h1>\n    <h2>What is kafka?</h2>\n    <blockquote>\n      <p>Kafka is a streamsing platform for ingesting, storing, accessing and processing streams of data</p>\n    </blockquote>\n    <h1>High level concepts</h1>\n    <h2>Communication model</h2>\n    <p>On the contrary of a <strong>Directed Communication</strong> where every service is aware of the other process to send messages to, kafka provides a centralized hub in which processes can send and receieve messages without being aware of each other. Such communication model is known as the <strong>Publish/Subscribe</strong> model which enhance the decoupling between different processes.</p>\n    <blockquote>\n      <p>Of course those are not termed by kafka</p>\n    </blockquote>\n    <blockquote>\n      <p>Recall the different communication formalization according the sync/async and persistent/transient dimensions. Kafka fills conforms to the asynchronous and persisted model. This is unintuitive since this model is usually used for classical <strong>Queues</strong> rather than <strong>Pubsubs</strong>.</p>\n    </blockquote>\n    <h2>Core components</h2>\n    <p>A <strong>Topic</strong> is a named stream of data (/ channel - CSP?).</p>\n    <p><strong>Producer</strong>s are processes that publish data to a Topic.</p>\n    <p><strong>Consumer</strong>s are processes that subscribe to data in one or more Topics.</p>\n    <p>A <strong>Consumer Group</strong> is a set of Consumers that work together as a group.</p>\n    <h2>Storage</h2>\n    <h3>Commit Log</h3>\n    <p>A <strong>Commit Log</strong> is an append only data structure which contain an <em>Ordered Sequence</em> of events (/records)</p>\n    <ul>\n      <li>Records in the log are immutable</li>\n      <li>Records are ordered and their ordinal is known as their <strong>Offset</strong></li>\n    </ul>\n    <blockquote>\n      <p>It is very similiar to existing transaction logs (Redo Log especially) which can be found in classical Relational Databases.</p>\n    </blockquote>\n    <blockquote>\n      <p>To gain perspectives we can find similiarities to STM concurrency mode, Redux stores etc...</p>\n    </blockquote>\n    <h3>Partitions</h3>\n    <p>In order to provide distribution capabilities, there is no 1 to 1 correlation of Topic to Log since then each Topic would need to be stored on a single machine. Rather, a Topic is broken into smaller units called <strong>Partitions</strong>.</p>\n    <p>Effectively, every Partition is a single Log. Such model allows kafka to distribute a Topic with N partitions to at most N workers and we can have K Consumers under the same Consumer Group sharing the load of consumption as long as LEQ(K, N). If K>N than it means that some consumers will be idle.</p>\n    <p>It makes sense from a technical POV. Kafka simply assigns a Partition to a Consumer.</p>\n    <ul>\n      <li>If the number of Consumers is less than the number of Partitions, by the pigenhole principle there is at least a single Consumers that is assigned with more than one partition</li>\n      <li>If the number of Consumers is greather than the number of Partitions, there cannot be a surjective mapping from the Partitions the Consumers</li>\n    </ul>\n    <h3>Event</h3>\n    <p>A unit of data in a Topic has many interchangeable names - Event, Message, Record. We favour the term Event.</p>\n    <blockquote>\n      <p>Out of the kafka scope, events in the fotware world are <em>Entities</em> that desribe something that <em>happened in the past</em>. Therefore, event are past tense verbse. Well, it is the same in kafka</p>\n    </blockquote>\n    <p>An <strong>Event</strong> is a timestamped key-value pair the records something that happened.</p>\n    <p>Event is composed of <strong>Headers</strong>, <strong>Keys</strong>, a <strong>Timestamp</strong> and a <strong>Value</strong>. We will cover some of those at a later time but for a very brief overview</p>\n    <p><strong>Headers</strong> contain optional metadata about the Event. One can use them to annotate, monitor and audit and Event. This is very like HTTP headers.</p>\n    <p><strong>Keys</strong> are optional values that affect how data is distributed accross partitions (Again, this is known concept, think of Partition Keys and partitioning schemes in different distributed systems). We will explore this concept independently.</p>\n    <p><strong>Timestamp</strong> holds information about when the thing that happened hapenned. There are multiple semantics of time we can speak of - When the Event created? When was it ingested? When was it processed? This is a whole topic of it's own which we will explore independantly.</p>\n    <p><strong>Value</strong> is the content of the message. Practically this is just a byte array and should be serialized according to the application.</p>\n    <h2>The Cluster</h2>\n    <p>TODO: cover concepts below</p>\n    <p><strong>Broker</strong> TODO</p>\n    <p><strong>Replication</strong> TODO</p>\n    <p><strong>Leader</strong> TODO</p>\n    <p><strong>Follower</strong> TODO</p>\n  </body>\n</html>\n"}}},"__N_SSG":true}