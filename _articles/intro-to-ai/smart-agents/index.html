<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/fc3ec0f12aa27086.css" as="style"/><link rel="stylesheet" href="/_next/static/css/fc3ec0f12aa27086.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a7fabc11ce9fdee5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a7fabc11ce9fdee5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-20cc5a3cc3d233fe.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-97927b8cd08b2c1f.js" defer=""></script><script src="/_next/static/chunks/pages/_app-2b3ff0d0be8e812a.js" defer=""></script><script src="/_next/static/chunks/175675d1-7de8b3bfdcedb0f1.js" defer=""></script><script src="/_next/static/chunks/113-d38bb2befbc77dc7.js" defer=""></script><script src="/_next/static/chunks/pages/_articles/%5B...articleId%5D-14c11b9e88d91c56.js" defer=""></script><script src="/_next/static/2UtthYsk4Izfh9PIWGXxM/_buildManifest.js" defer=""></script><script src="/_next/static/2UtthYsk4Izfh9PIWGXxM/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="Layout_container__K9hpm"><nav class="p-6 border border-r-2"><div class="flex flex-col items-center"><img src="/logo.png" width="100" height="100"/><span class="text-3xl font-semibold">Tal Glanzman</span><span class="text-lg">Software Engineer</span></div><div class="mt-6"><ul><li><a href="/">Home</a></li><li><a href="/_index/">Index</a></li><details><summary>Computer Science</summary><ul class="ml-4"><li><a href="/_articles/computer-science/linear-programming/">Linear Programming</a></li><li><a href="/_articles/probabilistic-method/probabilistic-method/">Probabilistic Method</a></li></ul></details><li><a href="/_articles/about/">About</a></li></ul></div><div class="mt-6"><div><form action="/_search" autoComplete="off" class="flex flex-col justify-center items-center"><input class="text-input" type="text" name="query" placeholder="Search"/><input class="button" type="submit" hidden="" value="Search"/></form></div></div></nav><header><div class=" p-2 flex flex-col items-center"><span class="text-4xl font-semibold">Smart Agents</span><span class="text-lg"></span></div></header><main><div class="text-sm m-4"><div><span>Categories<!-- -->: </span><span><span class="inline-block"><a href="/_categories/Intro%20to%20AI/">Intro to AI</a></span></span></div></div><div class="ArticleContent_md__52PoF"><!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>
  <body>
    <h1 id="smart-agents">Smart Agents</h1>
    <p>At a (very) high level there are multiple approaches to AI - Two main questions arise:</p>
    <ol>
      <li>Do we focus on <em>thought</em> or <em>behavior</em>?</li>
      <li>Do we want to imitate a human behavior or implement an ideal model for a problem?</li>
    </ol>
    <p>We will take the stand where a <strong>smart agent</strong> is the one who performs the best action given the scenario.</p>
    <p>An <strong>Agent</strong> is any entity that examines it's surrounding through <strong>sensors</strong> and acts in it's environment using <strong>actuators</strong>.</p>
    <p>Formally, the <strong>behavior</strong> of an agent is a mathematical function from the percepts (sensory inputs) to actions. The implementation of this function is part of agent's program.</p>
    <p>An agent's behavior can be implemented in many ways - But which one is better? What makes an agent "smart" or "dumb"? To answer those question we must add <strong>performance measurments</strong> to the agent's definition.</p>
    <p>There are many properties of an enviornment that affects the agents within it:</p>
    <ul>
      <li>
        <p><strong>Fully vs Partially Observable</strong>. An environment is said to be fully observable if it's entirety is visible to the agent (think of chess for example). An environment that is not fully observable is said to be partially visible (think of poker for example).</p>
      </li>
      <li>
        <p><strong>Single vs Multi Agent</strong>. Whether the environment contains a single agent (solitare) or multiple agents which can either compete (chess) or cooperate.</p>
      </li>
      <li>
        <p><strong>Deterministic vs Stochastic</strong>. In a deterministic environment, the agent has complete control over the environment through it's actions (chess), where in a stochastic environment the agent has only partial control over it's environment (weather forecast).</p>
      </li>
      <li>
        <p><strong>Episodic vs Sequential</strong>. In an episodic environment the agent's tasks are broken into independent episodes (product defect recognition) while a sequencial environment is where the agent task is single throughout (chess).</p>
      </li>
      <li>
        <p><strong>Static vs Dynamic</strong>. An environment is said to be static if it doesn't change while the agent plans out is next move (chess), otherwise it is said to be dynamic (best traffic route finder).</p>
      </li>
      <li>
        <p><strong>Discrete vs Continuous</strong>. A discrete environment is when we can clearly limit and disinct each action (chess) - Otherwise it is continuous (taxi driver).</p>
      </li>
      <li>
        <p><strong>Known vs Unknown</strong>. A known environment is an environment such that the agent knows the implications for each of it's actions. Otherwise it is unknown.</p>
      </li>
    </ul>
    <p>Our world is a partially observable, multi agent, stochastic, sequential, dynamic, continuous and unknown environment.</p>
    <p>The goal of AI is the to plan the <strong>agent's program</strong> that will manifest the function which map the percepts to actions.</p>
    <p>There are many kinds of agents:</p>
    <ul>
      <li>Simple reflex agents. Agents are affected only from the current percepts.</li>
      <li>Model reflex agents. The agent contains an inner state that depends on the history of percepts. The inner state is updated according to it's <strong>model</strong>.</li>
      <li>Goal based agents. A <em>goal</em> is some claim regarding the environment given to an agent which it attempt to fulfill.</li>
      <li>Utility based agents. Some states to the goal can be better than others - The agent tries to maximize the overall quality of it's steps.</li>
      <li>Learning agents. Agents that learn over time. Such agents are usually composed of a <em>learning element</em> that accumulates knowledge, a <em>critic element</em> that gives feedback, a <em>performance element</em> that performs actions and a <em>problem generator</em> element that might suggest new exploration routes.</li>
    </ul>
  </body>
</html>
</div></main><footer><div class=" border-t-2 p-4 flex flex-row justify-center items-center space-x-4"><a href="https://github.com/tglanz"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://il.linkedin.com/in/tal-glanzman"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"article":{"id":"intro-to-ai/smart-agents","filePath":"/home/runner/work/tglanz.github.io/tglanz.github.io/content/intro-to-ai/smart-agents.md","metadata":{"title":"Smart Agents","description":null,"priority":1,"tags":[],"categories":["Intro to AI"],"toc":false},"content":{"raw":"\n# Smart Agents \n\nAt a (very) high level there are multiple approaches to AI - Two main questions arise:\n\n1. Do we focus on _thought_ or _behavior_?\n2. Do we want to imitate a human behavior or implement an ideal model for a problem?\n\nWe will take the stand where a **smart agent** is the one who performs the best action given the scenario.\n\nAn **Agent** is any entity that examines it's surrounding through **sensors** and acts in it's environment using **actuators**.\n\nFormally, the **behavior** of an agent is a mathematical function from the percepts (sensory inputs) to actions. The implementation of this function is part of agent's program.\n\nAn agent's behavior can be implemented in many ways - But which one is better? What makes an agent \"smart\" or \"dumb\"? To answer those question we must add **performance measurments** to the agent's definition.\n\nThere are many properties of an enviornment that affects the agents within it:\n\n- **Fully vs Partially Observable**. An environment is said to be fully observable if it's entirety is visible to the agent (think of chess for example). An environment that is not fully observable is said to be partially visible (think of poker for example).\n\n- **Single vs Multi Agent**. Whether the environment contains a single agent (solitare) or multiple agents which can either compete (chess) or cooperate.\n\n- **Deterministic vs Stochastic**. In a deterministic environment, the agent has complete control over the environment through it's actions (chess), where in a stochastic environment the agent has only partial control over it's environment (weather forecast).\n\n- **Episodic vs Sequential**. In an episodic environment the agent's tasks are broken into independent episodes (product defect recognition) while a sequencial environment is where the agent task is single throughout (chess).\n\n- **Static vs Dynamic**. An environment is said to be static if it doesn't change while the agent plans out is next move (chess), otherwise it is said to be dynamic (best traffic route finder).\n\n- **Discrete vs Continuous**. A discrete environment is when we can clearly limit and disinct each action (chess) - Otherwise it is continuous (taxi driver).\n\n- **Known vs Unknown**. A known environment is an environment such that the agent knows the implications for each of it's actions. Otherwise it is unknown.\n\nOur world is a partially observable, multi agent, stochastic, sequential, dynamic, continuous and unknown environment. \n\nThe goal of AI is the to plan the **agent's program** that will manifest the function which map the percepts to actions. \n\nThere are many kinds of agents:\n\n- Simple reflex agents. Agents are affected only from the current percepts.\n- Model reflex agents. The agent contains an inner state that depends on the history of percepts. The inner state is updated according to it's **model**.\n- Goal based agents. A *goal* is some claim regarding the environment given to an agent which it attempt to fulfill.\n- Utility based agents. Some states to the goal can be better than others - The agent tries to maximize the overall quality of it's steps.\n- Learning agents. Agents that learn over time. Such agents are usually composed of a _learning element_ that accumulates knowledge, a _critic element_ that gives feedback, a _performance element_ that performs actions and a _problem generator_ element that might suggest new exploration routes.\n\n\n","html":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch1 id=\"smart-agents\"\u003eSmart Agents\u003c/h1\u003e\n    \u003cp\u003eAt a (very) high level there are multiple approaches to AI - Two main questions arise:\u003c/p\u003e\n    \u003col\u003e\n      \u003cli\u003eDo we focus on \u003cem\u003ethought\u003c/em\u003e or \u003cem\u003ebehavior\u003c/em\u003e?\u003c/li\u003e\n      \u003cli\u003eDo we want to imitate a human behavior or implement an ideal model for a problem?\u003c/li\u003e\n    \u003c/ol\u003e\n    \u003cp\u003eWe will take the stand where a \u003cstrong\u003esmart agent\u003c/strong\u003e is the one who performs the best action given the scenario.\u003c/p\u003e\n    \u003cp\u003eAn \u003cstrong\u003eAgent\u003c/strong\u003e is any entity that examines it's surrounding through \u003cstrong\u003esensors\u003c/strong\u003e and acts in it's environment using \u003cstrong\u003eactuators\u003c/strong\u003e.\u003c/p\u003e\n    \u003cp\u003eFormally, the \u003cstrong\u003ebehavior\u003c/strong\u003e of an agent is a mathematical function from the percepts (sensory inputs) to actions. The implementation of this function is part of agent's program.\u003c/p\u003e\n    \u003cp\u003eAn agent's behavior can be implemented in many ways - But which one is better? What makes an agent \"smart\" or \"dumb\"? To answer those question we must add \u003cstrong\u003eperformance measurments\u003c/strong\u003e to the agent's definition.\u003c/p\u003e\n    \u003cp\u003eThere are many properties of an enviornment that affects the agents within it:\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\n        \u003cp\u003e\u003cstrong\u003eFully vs Partially Observable\u003c/strong\u003e. An environment is said to be fully observable if it's entirety is visible to the agent (think of chess for example). An environment that is not fully observable is said to be partially visible (think of poker for example).\u003c/p\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\n        \u003cp\u003e\u003cstrong\u003eSingle vs Multi Agent\u003c/strong\u003e. Whether the environment contains a single agent (solitare) or multiple agents which can either compete (chess) or cooperate.\u003c/p\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\n        \u003cp\u003e\u003cstrong\u003eDeterministic vs Stochastic\u003c/strong\u003e. In a deterministic environment, the agent has complete control over the environment through it's actions (chess), where in a stochastic environment the agent has only partial control over it's environment (weather forecast).\u003c/p\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\n        \u003cp\u003e\u003cstrong\u003eEpisodic vs Sequential\u003c/strong\u003e. In an episodic environment the agent's tasks are broken into independent episodes (product defect recognition) while a sequencial environment is where the agent task is single throughout (chess).\u003c/p\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\n        \u003cp\u003e\u003cstrong\u003eStatic vs Dynamic\u003c/strong\u003e. An environment is said to be static if it doesn't change while the agent plans out is next move (chess), otherwise it is said to be dynamic (best traffic route finder).\u003c/p\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\n        \u003cp\u003e\u003cstrong\u003eDiscrete vs Continuous\u003c/strong\u003e. A discrete environment is when we can clearly limit and disinct each action (chess) - Otherwise it is continuous (taxi driver).\u003c/p\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\n        \u003cp\u003e\u003cstrong\u003eKnown vs Unknown\u003c/strong\u003e. A known environment is an environment such that the agent knows the implications for each of it's actions. Otherwise it is unknown.\u003c/p\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eOur world is a partially observable, multi agent, stochastic, sequential, dynamic, continuous and unknown environment.\u003c/p\u003e\n    \u003cp\u003eThe goal of AI is the to plan the \u003cstrong\u003eagent's program\u003c/strong\u003e that will manifest the function which map the percepts to actions.\u003c/p\u003e\n    \u003cp\u003eThere are many kinds of agents:\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eSimple reflex agents. Agents are affected only from the current percepts.\u003c/li\u003e\n      \u003cli\u003eModel reflex agents. The agent contains an inner state that depends on the history of percepts. The inner state is updated according to it's \u003cstrong\u003emodel\u003c/strong\u003e.\u003c/li\u003e\n      \u003cli\u003eGoal based agents. A \u003cem\u003egoal\u003c/em\u003e is some claim regarding the environment given to an agent which it attempt to fulfill.\u003c/li\u003e\n      \u003cli\u003eUtility based agents. Some states to the goal can be better than others - The agent tries to maximize the overall quality of it's steps.\u003c/li\u003e\n      \u003cli\u003eLearning agents. Agents that learn over time. Such agents are usually composed of a \u003cem\u003elearning element\u003c/em\u003e that accumulates knowledge, a \u003cem\u003ecritic element\u003c/em\u003e that gives feedback, a \u003cem\u003eperformance element\u003c/em\u003e that performs actions and a \u003cem\u003eproblem generator\u003c/em\u003e element that might suggest new exploration routes.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}}},"__N_SSG":true},"page":"/_articles/[...articleId]","query":{"articleId":["intro-to-ai","smart-agents"]},"buildId":"2UtthYsk4Izfh9PIWGXxM","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>