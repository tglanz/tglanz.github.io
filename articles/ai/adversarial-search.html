<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/a34f9d1faa5f3315-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/379a2867f23cfb09.css" crossorigin="" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/1cb3813e458884a4.css" crossorigin="" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-cbb25c2103158eec.js" crossorigin=""/><script src="/_next/static/chunks/fd9d1056-672a444f700a8e5a.js" async="" crossorigin=""></script><script src="/_next/static/chunks/472-7e5eafffd19dca44.js" async="" crossorigin=""></script><script src="/_next/static/chunks/main-app-d355f2bc91b5bf25.js" async="" crossorigin=""></script><script src="/_next/static/chunks/326-dcee1ff54fa4f70c.js" async=""></script><script src="/_next/static/chunks/app/layout-0926e968f56c67e9.js" async=""></script><script src="/_next/static/chunks/d3ac728e-1e5d8b71e3d43fec.js" async=""></script><script src="/_next/static/chunks/app/articles/%5B...slug%5D/page-44f6caefb92fad34.js" async=""></script><title>Article</title><meta name="description" content="Personal site"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="256x256"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js" crossorigin="" noModule=""></script></head><body class="__className_36bd41 Layout_body__oXsmr"><header class="Layout_header__XC_Gv"><div class="NavBar_container__pKdC7"><a class="link" href="/">Home</a><a class="link" href="/academic">Academic Work</a><a class="link" href="/categories">Categories</a><a class="link" href="/articles">All</a><a class="link" href="/articles/about">About</a></div></header><main class="Layout_main__luTTh"><div class="article-page_container__5yaZl"><h1>Adversarial Search</h1><div class="ArticleContent_md__SxPPI"><!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>
  <body>
    <p>A <strong>competitive environment</strong> is a multi-agent environment in which two or more adversarial agents have coflicting goals. There are multiple ways we can view a such environments, Here we will model adversarial agents with the techniques of adversarial game-tree search.</p>
    <p>To introduce the topic we will restrict ourselves to <strong>Two-player zero-sum games</strong>.</p>
    <h2 id="two-player-zero-sum-games">Two-player zero-sum games</h2>
    <p>A game is said to be</p>
    <ul>
      <li><em>Deterministic</em> if every action leads to a deterministic state.</li>
      <li>With <em>Perfect Information</em> if it's a <em>fully observable</em> environment. Meaning, every player can see the entire state.</li>
      <li><em>Zero-Sum</em> when there are zero points alloted to the game at initialization and players gain points by taking them from other players. There is no "win-win" situation.</li>
      <li><em>Turn Making</em> if only a single player can make a move in each state, alternatively.</li>
    </ul>
    <p>A <strong>two-player zero-sum game</strong> is a deterministic, two-player, turn-making, zero-sum game with perfect information.</p>
    <p>We will name the two adversaries <strong>MAX</strong> and <strong>MIN</strong>.</p>
    <p>Formally we define the game with the following elements:</p>
    <ul>
      <li>$S_0$ is the <strong>initial state</strong>.</li>
      <li><em>ToMove(s)</em> is the player to make the move in state s.</li>
      <li><em>Actions(s)</em> is the set of legal moves in state s.</li>
      <li><em>Result(s, a)</em> is the <strong>transition model</strong> which defines the resulting state of state s after action a.</li>
      <li><em>IsTerminal(s)</em> is the <strong>terminal test</strong> indicating whether s is a terminal state.</li>
      <li><em>Utility(s, p)</em> is the <strong>utility function</strong> which defines the value assigned to player p if the game terminated in the terminal state s. This function is also known as <strong>objective function</strong> or <strong>payoff function</strong></li>
    </ul>
    <p>Like many other search problems, $S_0$, <em>Actions</em> and <em>Result</em> define a <strong>state space graph</strong> - A graph where the vertices are states and the edges are the moves. We can impose a <strong>search tree</strong> in order to determine what action to take by searching the graph. We define the complete <strong>game tree</strong> as the tree as the search tree that investigate every possible action for every possible state up to terminals.</p>
    <p>Take a look at the example search tree below. Note that by convention we will symbolize the MAX player with a triangle and the MIN player with an inverted triangle.</p>
    <img src="https://www.plantuml.com/plantuml/png/RT1B2eCm58NXiLb5DX1Aqs6ZKD_F5OY3s0OLH4JRJahtNYAjrPix2lnyvC0bpIJ7baRqHQmBBUTMZig-JvqScbvLn79_hHkUc-pXsSTpW7mS8Yr2Meoq16afqZAavKWhpAQlmHXrQTdBMlMsa69-rCvH7FJZLESly03W4FW0dFv0NvSn6goYDZOM40B08OW0s6ruMiIhk1eR2qW0u12a0EmsV2soLToCZGNa07082W0s6s_o0G00">
    <h2 id="the-minimax-search-algorithm">The Minimax search algorithm</h2>
    <p>Both MAX and MIN playes aim to optimize their decisions throughout the game. Every action MAX makes MIN also makes an action! MAX cannot hope for MIN to blunder - He needs to find a sequence of actions that will maximize the score assuming MIN tries to minimize the score.</p>
    <p>The search approach we take is to keep track of the score after each decision in the tree. Decisions made by MAX aim to increase the score while decisions made by MIN aim to decrease the score - Practically we will run a DFS variant, tracking and modifying the score accordingly. This algorithm is called <strong>minimax search</strong>.</p>
    <p>Formally:</p>
    <p>
      \begin{align*}
      Minmax(s) =&#x26; &#x26; \\
      &#x26; Utility(s, MAX) &#x26; if ~ IsTerminal(s) \\
      &#x26; \max_{a \in Actions(s)}{Minimax(s, a)} &#x26; if ~ ToMove(s) = MAX \\
      &#x26; \min_{a \in Actions(s)}{Minimax(s, a)} &#x26; if ~ ToMove(s) = MAX \\
      \end{align*}
    </p>
    <p>Just computing the Minimax is not enought. Remember, we needed a search algorithm to tell MAX what is his optimal action to take. For this purpose, we need to keep track of the optimal move when computing the Minimax.</p>
    <pre><code class="hljs language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">MinimaxSearch</span>(<span class="hljs-params">game, state</span>) -> action \\\\
  player = game.ToMove(state)
  value, move = MaxValue(game, state, player)
  <span class="hljs-keyword">return</span> move

<span class="hljs-keyword">def</span> <span class="hljs-title function_">MaxValue</span>(<span class="hljs-params">game, state, player</span>) -> (utility, action):
  <span class="hljs-keyword">if</span> game.IsTerminal(state):
    <span class="hljs-keyword">return</span> game.Utility(state, player), null

  <span class="hljs-keyword">return</span> (utility, action) that maximizes
    MinValue(game, game.Result(state, a), player) foreach a <span class="hljs-keyword">in</span> Actions(state)

<span class="hljs-keyword">def</span> <span class="hljs-title function_">MinValue</span>(<span class="hljs-params">game, state, player</span>) -> (utility, action):
  <span class="hljs-keyword">if</span> game.IsTerminal(state):
    <span class="hljs-keyword">return</span> game.Utility(state, player), null

  <span class="hljs-keyword">return</span> (utility, action) that minimizes 
    MaxValue(game, game.Result(state, a), player) foreach a <span class="hljs-keyword">in</span> Actions(state)

</code></pre>
    <h2 id="more-than-2-agents">More than 2 agents</h2>
    <p>TODO</p>
    <h2 id="alpha-beta-pruning">Alpha-Beta pruning</h2>
    <p>TODO</p>
  </body>
</html>
</div></div></main><footer><div class="Footer_container__Z8cUU"><a href="https://il.linkedin.com/in/tal-glanzman"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="25" width="25" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a><a href="https://github.com/tglanz"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="25" width="25" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div></footer><script src="/_next/static/chunks/webpack-cbb25c2103158eec.js" crossorigin="" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/a34f9d1faa5f3315-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/379a2867f23cfb09.css\",\"style\",{\"crossOrigin\":\"\"}]\n0:\"$L3\"\n"])</script><script>self.__next_f.push([1,"4:HL[\"/_next/static/css/1cb3813e458884a4.css\",\"style\",{\"crossOrigin\":\"\"}]\n"])</script><script>self.__next_f.push([1,"5:I[3728,[],\"\"]\n7:I[9928,[],\"\"]\n8:I[5420,[\"326\",\"static/chunks/326-dcee1ff54fa4f70c.js\",\"185\",\"static/chunks/app/layout-0926e968f56c67e9.js\"],\"\"]\n9:I[6954,[],\"\"]\na:I[7264,[],\"\"]\nd:I[8326,[\"954\",\"static/chunks/d3ac728e-1e5d8b71e3d43fec.js\",\"326\",\"static/chunks/326-dcee1ff54fa4f70c.js\",\"496\",\"static/chunks/app/articles/%5B...slug%5D/page-44f6caefb92fad34.js\"],\"\"]\ne:T518,"])</script><script>self.__next_f.push([1,"M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"])</script><script>self.__next_f.push([1,"3:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/379a2867f23cfb09.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],[\"$\",\"$L5\",null,{\"buildId\":\"nB7Ffkk80k0MX8_fd4v_t\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/articles/ai/adversarial-search\",\"initialTree\":[\"\",{\"children\":[\"articles\",{\"children\":[[\"slug\",\"ai/adversarial-search\",\"c\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":[\\\"ai\\\",\\\"adversarial-search\\\"]}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialHead\":[false,\"$L6\"],\"globalErrorComponent\":\"$7\",\"children\":[null,[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_36bd41 Layout_body__oXsmr\",\"children\":[[\"$\",\"header\",null,{\"className\":\"Layout_header__XC_Gv\",\"children\":[\"$\",\"$L8\",null,{}]}],[\"$\",\"main\",null,{\"className\":\"Layout_main__luTTh\",\"children\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"initialChildNode\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"articles\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"initialChildNode\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"articles\",\"children\",[\"slug\",\"ai/adversarial-search\",\"c\"],\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"initialChildNode\":[\"$Lb\",\"$Lc\",null],\"childPropSegment\":\"__PAGE__?{\\\"slug\\\":[\\\"ai\\\",\\\"adversarial-search\\\"]}\",\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/1cb3813e458884a4.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]]}],\"childPropSegment\":[\"slug\",\"ai/adversarial-search\",\"c\"],\"styles\":null}],\"childPropSegment\":\"articles\",\"styles\":null}]}],[\"$\",\"footer\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"Footer_container__Z8cUU\",\"children\":[[\"$\",\"$Ld\",null,{\"href\":\"https://il.linkedin.com/in/tal-glanzman\",\"children\":[\"$\",\"svg\",null,{\"stroke\":\"currentColor\",\"fill\":\"currentColor\",\"strokeWidth\":\"0\",\"viewBox\":\"0 0 448 512\",\"children\":[\"$undefined\",[[\"$\",\"path\",\"0\",{\"d\":\"M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z\",\"children\":\"$undefined\"}]]],\"className\":\"$undefined\",\"style\":{\"color\":\"$undefined\"},\"height\":25,\"width\":25,\"xmlns\":\"http://www.w3.org/2000/svg\"}]}],[\"$\",\"$Ld\",null,{\"href\":\"https://github.com/tglanz\",\"children\":[\"$\",\"svg\",null,{\"stroke\":\"currentColor\",\"fill\":\"currentColor\",\"strokeWidth\":\"0\",\"viewBox\":\"0 0 496 512\",\"children\":[\"$undefined\",[[\"$\",\"path\",\"0\",{\"d\":\"$e\",\"children\":\"$undefined\"}]]],\"className\":\"$undefined\",\"style\":{\"color\":\"$undefined\"},\"height\":25,\"width\":25,\"xmlns\":\"http://www.w3.org/2000/svg\"}]}]]}]}]]}]}],null]}]]\n"])</script><script>self.__next_f.push([1,"6:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Article\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Personal site\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"256x256\"}],[\"$\",\"meta\",\"5\",{\"name\":\"next-size-adjust\"}]]\nb:null\n"])</script><script>self.__next_f.push([1,"f:I[6222,[\"954\",\"static/chunks/d3ac728e-1e5d8b71e3d43fec.js\",\"326\",\"static/chunks/326-dcee1ff54fa4f70c.js\",\"496\",\"static/chunks/app/articles/%5B...slug%5D/page-44f6caefb92fad34.js\"],\"ArticleContent\"]\n10:T1276,"])</script><script>self.__next_f.push([1,"\nA **competitive environment** is a multi-agent environment in which two or more adversarial agents have coflicting goals. There are multiple ways we can view a such environments, Here we will model adversarial agents with the techniques of adversarial game-tree search. \n\nTo introduce the topic we will restrict ourselves to **Two-player zero-sum games**.\n\n## Two-player zero-sum games\n\nA game is said to be \n\n- _Deterministic_ if every action leads to a deterministic state.\n- With _Perfect Information_ if it's a _fully observable_ environment. Meaning, every player can see the entire state.\n- _Zero-Sum_ when there are zero points alloted to the game at initialization and players gain points by taking them from other players. There is no \"win-win\" situation.\n- _Turn Making_ if only a single player can make a move in each state, alternatively.\n\nA **two-player zero-sum game** is a deterministic, two-player, turn-making, zero-sum game with perfect information.\n\nWe will name the two adversaries **MAX** and **MIN**.\n\nFormally we define the game with the following elements:\n\n- $S_0$ is the **initial state**.\n- _ToMove(s)_ is the player to make the move in state s.\n- _Actions(s)_ is the set of legal moves in state s.\n- _Result(s, a)_ is the **transition model** which defines the resulting state of state s after action a.\n- _IsTerminal(s)_ is the **terminal test** indicating whether s is a terminal state.\n- _Utility(s, p)_ is the **utility function** which defines the value assigned to player p if the game terminated in the terminal state s. This function is also known as **objective function** or **payoff function**\n\nLike many other search problems, $S_0$, _Actions_ and _Result_ define a **state space graph** - A graph where the vertices are states and the edges are the moves. We can impose a **search tree** in order to determine what action to take by searching the graph. We define the complete **game tree** as the tree as the search tree that investigate every possible action for every possible state up to terminals.\n\nTake a look at the example search tree below. Note that by convention we will symbolize the MAX player with a triangle and the MIN player with an inverted triangle.\n\n```plantuml\n\ngraph G {\n\tA [shape=triangle]\n\tB [shape=invtriangle]\n\tC [shape=invtriangle]\n\tD [shape=invtriangle]\n\tE [shape=triangle]\n\tF [shape=triangle]\n\tG [shape=triangle]\n\tH [shape=triangle]\n\tI [shape=triangle]\n\tJ [shape=triangle]\n\tK [shape=triangle]\n\tL [shape=triangle]\n\tM [shape=triangle]\n\n\tA -- B [label=\u003ca\u003csub\u003e1\u003c/sub\u003e\u003e];\n\tA -- C [label=\u003ca\u003csub\u003e2\u003c/sub\u003e\u003e];\n\tA -- D [label=\u003ca\u003csub\u003e3\u003c/sub\u003e\u003e];\n\n\tB -- E [label=\u003cb\u003csub\u003e1\u003c/sub\u003e\u003e];\n\tB -- F [label=\u003cb\u003csub\u003e2\u003c/sub\u003e\u003e];\n\tB -- G [label=\u003cb\u003csub\u003e3\u003c/sub\u003e\u003e];\n\n\tC -- H [label=\u003cc\u003csub\u003e1\u003c/sub\u003e\u003e];\n\tC -- I [label=\u003cc\u003csub\u003e2\u003c/sub\u003e\u003e];\n\tC -- J [label=\u003cc\u003csub\u003e3\u003c/sub\u003e\u003e];\n\n\tD -- K [label=\u003cd\u003csub\u003e1\u003c/sub\u003e\u003e];\n\tD -- L [label=\u003cd\u003csub\u003e2\u003c/sub\u003e\u003e];\n\tD -- M [label=\u003cd\u003csub\u003e3\u003c/sub\u003e\u003e];\n}\n\n```\n\n## The Minimax search algorithm\n\nBoth MAX and MIN playes aim to optimize their decisions throughout the game. Every action MAX makes MIN also makes an action! MAX cannot hope for MIN to blunder - He needs to find a sequence of actions that will maximize the score assuming MIN tries to minimize the score.\n\nThe search approach we take is to keep track of the score after each decision in the tree. Decisions made by MAX aim to increase the score while decisions made by MIN aim to decrease the score - Practically we will run a DFS variant, tracking and modifying the score accordingly. This algorithm is called **minimax search**.\n\nFormally:\n\n\\begin{align*}\nMinmax(s) =\u0026 \u0026 \\\\\\\\\n  \u0026 Utility(s, MAX) \u0026 if ~ IsTerminal(s) \\\\\\\\\n  \u0026 \\max_{a \\in Actions(s)}{Minimax(s, a)} \u0026 if ~ ToMove(s) = MAX \\\\\\\\\n  \u0026 \\min_{a \\in Actions(s)}{Minimax(s, a)} \u0026 if ~ ToMove(s) = MAX \\\\\\\\\n\\end{align*}\n\nJust computing the Minimax is not enought. Remember, we needed a search algorithm to tell MAX what is his optimal action to take. For this purpose, we need to keep track of the optimal move when computing the Minimax.\n\n```python\ndef MinimaxSearch(game, state) -\u003e action \\\\\\\\\n  player = game.ToMove(state)\n  value, move = MaxValue(game, state, player)\n  return move\n\ndef MaxValue(game, state, player) -\u003e (utility, action):\n  if game.IsTerminal(state):\n    return game.Utility(state, player), null\n\n  return (utility, action) that maximizes\n    MinValue(game, game.Result(state, a), player) foreach a in Actions(state)\n\ndef MinValue(game, state, player) -\u003e (utility, action):\n  if game.IsTerminal(state):\n    return game.Utility(state, player), null\n\n  return (utility, action) that minimizes \n    MaxValue(game, game.Result(state, a), player) foreach a in Actions(state)\n\n```\n\n## More than 2 agents\n\nTODO\n\n## Alpha-Beta pruning\n\nTODO\n"])</script><script>self.__next_f.push([1,"11:T1766,"])</script><script>self.__next_f.push([1,"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003cp\u003eA \u003cstrong\u003ecompetitive environment\u003c/strong\u003e is a multi-agent environment in which two or more adversarial agents have coflicting goals. There are multiple ways we can view a such environments, Here we will model adversarial agents with the techniques of adversarial game-tree search.\u003c/p\u003e\n    \u003cp\u003eTo introduce the topic we will restrict ourselves to \u003cstrong\u003eTwo-player zero-sum games\u003c/strong\u003e.\u003c/p\u003e\n    \u003ch2 id=\"two-player-zero-sum-games\"\u003eTwo-player zero-sum games\u003c/h2\u003e\n    \u003cp\u003eA game is said to be\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\u003cem\u003eDeterministic\u003c/em\u003e if every action leads to a deterministic state.\u003c/li\u003e\n      \u003cli\u003eWith \u003cem\u003ePerfect Information\u003c/em\u003e if it's a \u003cem\u003efully observable\u003c/em\u003e environment. Meaning, every player can see the entire state.\u003c/li\u003e\n      \u003cli\u003e\u003cem\u003eZero-Sum\u003c/em\u003e when there are zero points alloted to the game at initialization and players gain points by taking them from other players. There is no \"win-win\" situation.\u003c/li\u003e\n      \u003cli\u003e\u003cem\u003eTurn Making\u003c/em\u003e if only a single player can make a move in each state, alternatively.\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eA \u003cstrong\u003etwo-player zero-sum game\u003c/strong\u003e is a deterministic, two-player, turn-making, zero-sum game with perfect information.\u003c/p\u003e\n    \u003cp\u003eWe will name the two adversaries \u003cstrong\u003eMAX\u003c/strong\u003e and \u003cstrong\u003eMIN\u003c/strong\u003e.\u003c/p\u003e\n    \u003cp\u003eFormally we define the game with the following elements:\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e$S_0$ is the \u003cstrong\u003einitial state\u003c/strong\u003e.\u003c/li\u003e\n      \u003cli\u003e\u003cem\u003eToMove(s)\u003c/em\u003e is the player to make the move in state s.\u003c/li\u003e\n      \u003cli\u003e\u003cem\u003eActions(s)\u003c/em\u003e is the set of legal moves in state s.\u003c/li\u003e\n      \u003cli\u003e\u003cem\u003eResult(s, a)\u003c/em\u003e is the \u003cstrong\u003etransition model\u003c/strong\u003e which defines the resulting state of state s after action a.\u003c/li\u003e\n      \u003cli\u003e\u003cem\u003eIsTerminal(s)\u003c/em\u003e is the \u003cstrong\u003eterminal test\u003c/strong\u003e indicating whether s is a terminal state.\u003c/li\u003e\n      \u003cli\u003e\u003cem\u003eUtility(s, p)\u003c/em\u003e is the \u003cstrong\u003eutility function\u003c/strong\u003e which defines the value assigned to player p if the game terminated in the terminal state s. This function is also known as \u003cstrong\u003eobjective function\u003c/strong\u003e or \u003cstrong\u003epayoff function\u003c/strong\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eLike many other search problems, $S_0$, \u003cem\u003eActions\u003c/em\u003e and \u003cem\u003eResult\u003c/em\u003e define a \u003cstrong\u003estate space graph\u003c/strong\u003e - A graph where the vertices are states and the edges are the moves. We can impose a \u003cstrong\u003esearch tree\u003c/strong\u003e in order to determine what action to take by searching the graph. We define the complete \u003cstrong\u003egame tree\u003c/strong\u003e as the tree as the search tree that investigate every possible action for every possible state up to terminals.\u003c/p\u003e\n    \u003cp\u003eTake a look at the example search tree below. Note that by convention we will symbolize the MAX player with a triangle and the MIN player with an inverted triangle.\u003c/p\u003e\n    \u003cimg src=\"https://www.plantuml.com/plantuml/png/RT1B2eCm58NXiLb5DX1Aqs6ZKD_F5OY3s0OLH4JRJahtNYAjrPix2lnyvC0bpIJ7baRqHQmBBUTMZig-JvqScbvLn79_hHkUc-pXsSTpW7mS8Yr2Meoq16afqZAavKWhpAQlmHXrQTdBMlMsa69-rCvH7FJZLESly03W4FW0dFv0NvSn6goYDZOM40B08OW0s6ruMiIhk1eR2qW0u12a0EmsV2soLToCZGNa07082W0s6s_o0G00\"\u003e\n    \u003ch2 id=\"the-minimax-search-algorithm\"\u003eThe Minimax search algorithm\u003c/h2\u003e\n    \u003cp\u003eBoth MAX and MIN playes aim to optimize their decisions throughout the game. Every action MAX makes MIN also makes an action! MAX cannot hope for MIN to blunder - He needs to find a sequence of actions that will maximize the score assuming MIN tries to minimize the score.\u003c/p\u003e\n    \u003cp\u003eThe search approach we take is to keep track of the score after each decision in the tree. Decisions made by MAX aim to increase the score while decisions made by MIN aim to decrease the score - Practically we will run a DFS variant, tracking and modifying the score accordingly. This algorithm is called \u003cstrong\u003eminimax search\u003c/strong\u003e.\u003c/p\u003e\n    \u003cp\u003eFormally:\u003c/p\u003e\n    \u003cp\u003e\n      \\begin{align*}\n      Minmax(s) =\u0026#x26; \u0026#x26; \\\\\n      \u0026#x26; Utility(s, MAX) \u0026#x26; if ~ IsTerminal(s) \\\\\n      \u0026#x26; \\max_{a \\in Actions(s)}{Minimax(s, a)} \u0026#x26; if ~ ToMove(s) = MAX \\\\\n      \u0026#x26; \\min_{a \\in Actions(s)}{Minimax(s, a)} \u0026#x26; if ~ ToMove(s) = MAX \\\\\n      \\end{align*}\n    \u003c/p\u003e\n    \u003cp\u003eJust computing the Minimax is not enought. Remember, we needed a search algorithm to tell MAX what is his optimal action to take. For this purpose, we need to keep track of the optimal move when computing the Minimax.\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eMinimaxSearch\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003egame, state\u003c/span\u003e) -\u003e action \\\\\\\\\n  player = game.ToMove(state)\n  value, move = MaxValue(game, state, player)\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e move\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eMaxValue\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003egame, state, player\u003c/span\u003e) -\u003e (utility, action):\n  \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e game.IsTerminal(state):\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e game.Utility(state, player), null\n\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e (utility, action) that maximizes\n    MinValue(game, game.Result(state, a), player) foreach a \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e Actions(state)\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eMinValue\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003egame, state, player\u003c/span\u003e) -\u003e (utility, action):\n  \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e game.IsTerminal(state):\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e game.Utility(state, player), null\n\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e (utility, action) that minimizes \n    MaxValue(game, game.Result(state, a), player) foreach a \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e Actions(state)\n\n\u003c/code\u003e\u003c/pre\u003e\n    \u003ch2 id=\"more-than-2-agents\"\u003eMore than 2 agents\u003c/h2\u003e\n    \u003cp\u003eTODO\u003c/p\u003e\n    \u003ch2 id=\"alpha-beta-pruning\"\u003eAlpha-Beta pruning\u003c/h2\u003e\n    \u003cp\u003eTODO\u003c/p\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"])</script><script>self.__next_f.push([1,"c:[\"$\",\"div\",null,{\"className\":\"article-page_container__5yaZl\",\"children\":[[\"$\",\"h1\",null,{\"children\":\"Adversarial Search\"}],\"$undefined\",[\"$\",\"$Lf\",null,{\"content\":{\"raw\":\"$10\",\"html\":\"$11\"}}]]}]\n"])</script><script>self.__next_f.push([1,""])</script></body></html>